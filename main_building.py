# ========================================
# BEAR å»ºç­‘ç¯å¢ƒä¼˜åŒ–è®­ç»ƒä¸»ç¨‹åº
# ========================================
# åŸºäº DROPT æ¡†æ¶ï¼Œåº”ç”¨æ‰©æ•£æ¨¡å‹+å¼ºåŒ–å­¦ä¹ åˆ°å»ºç­‘HVACæ§åˆ¶

import argparse
import os
import pprint
import torch
import numpy as np
from datetime import datetime
from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer
from torch.utils.tensorboard import SummaryWriter
from tianshou.utils import TensorboardLogger
from dropt_utils.tianshou_compat import offpolicy_trainer
import warnings

# å¯¼å…¥å»ºç­‘ç¯å¢ƒ
from env.building_env_wrapper import make_building_env
# å¯¼å…¥æ—¥å¿—æ ¼å¼åŒ–å·¥å…·
from dropt_utils.logger_formatter import EnhancedTensorboardLogger
# å¯¼å…¥é…ç½®å¸¸é‡ï¼ˆä¿®å¤ï¼šç»Ÿä¸€ç®¡ç†é…ç½®å‚æ•°ï¼Œé¿å…ç¡¬ç¼–ç ï¼‰
from env.building_config import (
    DEFAULT_REWARD_SCALE,
    DEFAULT_ENERGY_WEIGHT,
    DEFAULT_TEMP_WEIGHT,
    DEFAULT_VIOLATION_PENALTY,
    DEFAULT_TARGET_TEMP,
    DEFAULT_TEMP_TOLERANCE,
    DEFAULT_MAX_POWER,
    DEFAULT_TIME_RESOLUTION,
    DEFAULT_EPISODE_LENGTH,
    DEFAULT_TRAINING_NUM,
    DEFAULT_TEST_NUM,
    DEFAULT_BUFFER_SIZE,
    DEFAULT_BATCH_SIZE,
    DEFAULT_GAMMA,
    DEFAULT_N_STEP,
    DEFAULT_STEP_PER_EPOCH,
    DEFAULT_STEP_PER_COLLECT,
    DEFAULT_EPISODE_PER_TEST,
    DEFAULT_DIFFUSION_STEPS,
    DEFAULT_BETA_SCHEDULE,
    DEFAULT_HIDDEN_DIM,
    DEFAULT_ACTOR_LR,
    DEFAULT_CRITIC_LR,
    DEFAULT_EXPLORATION_NOISE,
    DEFAULT_LOG_DIR,
    DEFAULT_SAVE_INTERVAL,
    DEFAULT_MPC_PLANNING_STEPS,
)

# å¯¼å…¥ DROPT æ ¸å¿ƒç»„ä»¶
from policy import DiffusionOPT
from diffusion import Diffusion
from diffusion.model import MLP, DoubleCritic

warnings.filterwarnings('ignore')


def get_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆé’ˆå¯¹å»ºç­‘ç¯å¢ƒè°ƒæ•´ï¼‰
    """
    parser = argparse.ArgumentParser(description='BEAR å»ºç­‘ç¯å¢ƒ HVAC ä¼˜åŒ–è®­ç»ƒç¨‹åº')
    
    # ========== ç¯å¢ƒå‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--building-type', type=str, default='OfficeSmall',
                        help='å»ºç­‘ç±»å‹ (OfficeSmall, Hospital, SchoolPrimaryç­‰)')
    parser.add_argument('--weather-type', type=str, default='Hot_Dry',
                        help='æ°”å€™ç±»å‹ (Hot_Dry, Hot_Humid, Cold_Humidç­‰)')
    parser.add_argument('--location', type=str, default='Tucson',
                        help='åœ°ç†ä½ç½® (Tucson, Tampa, Rochesterç­‰)')
    parser.add_argument('--target-temp', type=float, default=DEFAULT_TARGET_TEMP,
                        help=f'ç›®æ ‡æ¸©åº¦ (Â°Cï¼Œé»˜è®¤{DEFAULT_TARGET_TEMP})')
    parser.add_argument('--temp-tolerance', type=float, default=DEFAULT_TEMP_TOLERANCE,
                        help=f'æ¸©åº¦å®¹å·® (Â°Cï¼Œé»˜è®¤{DEFAULT_TEMP_TOLERANCE})')
    parser.add_argument('--max-power', type=int, default=DEFAULT_MAX_POWER,
                        help=f'HVAC æœ€å¤§åŠŸç‡ (Wï¼Œé»˜è®¤{DEFAULT_MAX_POWER})')
    parser.add_argument('--time-resolution', type=int, default=DEFAULT_TIME_RESOLUTION,
                        help=f'æ—¶é—´åˆ†è¾¨ç‡ (ç§’ï¼Œé»˜è®¤{DEFAULT_TIME_RESOLUTION}=1å°æ—¶)')
    parser.add_argument('--episode-length', type=int, default=DEFAULT_EPISODE_LENGTH,
                        help='Episode horizon in steps (default ~1 week)')
    parser.add_argument('--full-episode', action='store_true', default=False,
                        help='Override to use the full-year trajectory (sets episode-length=None)')
    parser.add_argument('--energy-weight', type=float, default=DEFAULT_ENERGY_WEIGHT,
                        help=f'èƒ½è€—æƒé‡ Î± (é»˜è®¤{DEFAULT_ENERGY_WEIGHT})')
    parser.add_argument('--temp-weight', type=float, default=DEFAULT_TEMP_WEIGHT,
                        help=f'æ¸©åº¦åå·®æƒé‡ Î² (é»˜è®¤{DEFAULT_TEMP_WEIGHT})')
    parser.add_argument('--add-violation-penalty', dest='add_violation_penalty',
                        action='store_true', default=True,
                        help='å¯ç”¨æ¸©åº¦è¶Šç•Œæƒ©ç½šï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-add-violation-penalty', dest='add_violation_penalty',
                        action='store_false',
                        help='å…³é—­æ¸©åº¦è¶Šç•Œæƒ©ç½š')
    parser.add_argument('--violation-penalty', type=float, default=DEFAULT_VIOLATION_PENALTY,
                        help=f'æ¸©åº¦è¶Šç•Œæƒ©ç½šç³»æ•° Î³ (é»˜è®¤{DEFAULT_VIOLATION_PENALTY})')

    # ========== ä¸“å®¶æ§åˆ¶å™¨å‚æ•° ==========
    parser.add_argument('--expert-type', type=str, default=None,
                        choices=['mpc', 'pid', 'rule', 'bangbang', None],
                        help='ä¸“å®¶æ§åˆ¶å™¨ç±»å‹ï¼ˆç”¨äºè¡Œä¸ºå…‹éš†ï¼‰')
    parser.add_argument('--mpc-planning-steps', type=int, default=DEFAULT_MPC_PLANNING_STEPS,
                        help=f'MPC ä¸“å®¶è§„åˆ’æ­¥æ•° (é»˜è®¤{DEFAULT_MPC_PLANNING_STEPS})')
    parser.add_argument('--bc-coef', action='store_true', default=False,
                        help='æ˜¯å¦ä½¿ç”¨è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰æŸå¤±')
    parser.add_argument('--bc-weight', type=float, default=0.8,
                        help='è¡Œä¸ºå…‹éš†æŸå¤±åˆå§‹æƒé‡ï¼ˆé»˜è®¤0.8ï¼‰')
    parser.add_argument('--bc-weight-final', type=float, default=0.1,
                        help='BCæƒé‡æœ€ç»ˆå€¼ï¼ˆé»˜è®¤0.1ï¼Œé€æ¸è¿‡æ¸¡åˆ°ç­–ç•¥æ¢¯åº¦ï¼‰')
    parser.add_argument('--bc-weight-decay-steps', type=int, default=150000,
                        help='BCæƒé‡çº¿æ€§è¡°å‡æ­¥æ•°ï¼ˆé»˜è®¤15ä¸‡æ­¥ï¼Œå»¶é•¿ä¸“å®¶å¼•å¯¼ï¼‰')
    
    # ========== åŸºç¡€è®­ç»ƒå‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--exploration-noise', type=float, default=DEFAULT_EXPLORATION_NOISE,
                        help=f'æ¢ç´¢å™ªå£°æ ‡å‡†å·® (é»˜è®¤{DEFAULT_EXPLORATION_NOISE}ï¼Œé™ä½éšæœºæ‰°åŠ¨)')
    parser.add_argument('--algorithm', type=str, default='diffusion_opt',
                        help='ç®—æ³•åç§°')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­')
    parser.add_argument('--buffer-size', type=int, default=DEFAULT_BUFFER_SIZE,
                        help=f'ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å° (é»˜è®¤{DEFAULT_BUFFER_SIZE:,})')
    parser.add_argument('-e', '--epoch', type=int, default=20000,
                        help='æ€»è®­ç»ƒè½®æ¬¡ (é»˜è®¤ 20000ï¼Œä¾¿äºé˜¶æ®µæ€§è¯„ä¼°)')
    parser.add_argument('--step-per-epoch', type=int, default=16384,
                        help='æ¯ä¸ªepoché‡‡é›†çš„ç¯å¢ƒæ­¥æ•° (é»˜è®¤16384)')
    parser.add_argument('--step-per-collect', type=int, default=4096,
                        help='æ¯æ¬¡é‡‡é›†çš„æ­¥æ•° (é»˜è®¤4096ï¼Œæå‡æ ·æœ¬è´¨é‡)')
    parser.add_argument('-b', '--batch-size', type=int, default=256,
                        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤256)')
    parser.add_argument('--wd', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡ç³»æ•°')
    parser.add_argument('--gamma', type=float, default=DEFAULT_GAMMA,
                        help=f'æŠ˜æ‰£å› å­ (é»˜è®¤{DEFAULT_GAMMA})')
    parser.add_argument('--n-step', type=int, default=DEFAULT_N_STEP,
                        help=f'Næ­¥TDå­¦ä¹  (é»˜è®¤{DEFAULT_N_STEP})')
    parser.add_argument('--training-num', type=int, default=DEFAULT_TRAINING_NUM,
                        help=f'å¹¶è¡Œè®­ç»ƒç¯å¢ƒæ•°é‡ (é»˜è®¤{DEFAULT_TRAINING_NUM})')
    parser.add_argument('--test-num', type=int, default=DEFAULT_TEST_NUM,
                        help=f'å¹¶è¡Œæµ‹è¯•ç¯å¢ƒæ•°é‡ (é»˜è®¤{DEFAULT_TEST_NUM})')
    parser.add_argument('--episode-per-test', type=int, default=DEFAULT_EPISODE_PER_TEST,
                        help='æ¯æ¬¡è¯„ä¼°æ‰€è·‘çš„episodeæ•°é‡ï¼ˆå‡å°‘å¯ç¼©çŸ­æ¯è½®è¯„ä¼°æ—¶é—´ï¼‰')
    parser.add_argument('--prioritized-replay', action='store_true', default=False,
                        help='æ˜¯å¦å¯ç”¨ä¼˜å…ˆç»éªŒå›æ”¾ï¼ˆPERï¼‰')
    parser.add_argument('--prior-alpha', type=float, default=0.6,
                        help='PER é‡‡æ ·åˆ†å¸ƒå¹³æ»‘å› å­ alpha')
    parser.add_argument('--prior-beta', type=float, default=0.4,
                        help='PER é‡è¦æ€§é‡‡æ ·ä¿®æ­£ beta')

    # ========== ç½‘ç»œæ¶æ„å‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--hidden-dim', type=int, default=DEFAULT_HIDDEN_DIM,
                        help=f'MLPéšè—å±‚ç»´åº¦ (é»˜è®¤{DEFAULT_HIDDEN_DIM})')
    parser.add_argument('--actor-lr', type=float, default=1e-4,
                        help='Actorå­¦ä¹ ç‡ (é»˜è®¤1e-4ï¼Œé™ä½æ¢¯åº¦éœ‡è¡)')
    parser.add_argument('--critic-lr', type=float, default=DEFAULT_CRITIC_LR,
                        help=f'Criticå­¦ä¹ ç‡ (é»˜è®¤{DEFAULT_CRITIC_LR})')
    parser.add_argument('--reward-scale', type=float, default=DEFAULT_REWARD_SCALE,
                        help='Scale raw rewards to stabilize critic/actor updates')
    parser.add_argument('--reward-normalization', dest='reward_normalization', action='store_true',
                        help='Enable Tianshou reward normalization (default)')
    parser.add_argument('--no-reward-normalization', dest='reward_normalization', action='store_false',
                        help='Disable reward normalization for legacy behavior')
    parser.set_defaults(reward_normalization=True)

    # ========== æ‰©æ•£æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--diffusion-steps', type=int, default=DEFAULT_DIFFUSION_STEPS,
                        help=f'æ‰©æ•£æ­¥æ•° (é»˜è®¤{DEFAULT_DIFFUSION_STEPS})')
    parser.add_argument('--beta-schedule', type=str, default=DEFAULT_BETA_SCHEDULE,
                        help=f'å™ªå£°è°ƒåº¦ç±»å‹ (é»˜è®¤{DEFAULT_BETA_SCHEDULE})')

    # ========== æ—¥å¿—å’Œè®¾å¤‡å‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--logdir', type=str, default=DEFAULT_LOG_DIR,
                        help=f'æ—¥å¿—ä¿å­˜ç›®å½• (é»˜è®¤{DEFAULT_LOG_DIR})')
    parser.add_argument('--log-prefix', type=str, default='default',
                        help='æ—¥å¿—æ–‡ä»¶å‰ç¼€')
    parser.add_argument('--device', type=str, default='cuda:0',
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--resume-path', type=str, default=None,
                        help='æ¢å¤è®­ç»ƒçš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--watch', action='store_true', default=False,
                        help='è§‚å¯Ÿæ¨¡å¼ï¼ˆä¸è®­ç»ƒï¼‰')
    parser.add_argument('--lr-decay', action='store_true', default=False,
                        help='æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡')
    parser.add_argument('--save-interval', type=int, default=DEFAULT_SAVE_INTERVAL,
                        help=f'æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆè½®æ¬¡ï¼Œé»˜è®¤{DEFAULT_SAVE_INTERVAL}ï¼‰')
    parser.add_argument('--vector-env-type', type=str, default='dummy',
                        choices=['dummy', 'subproc'],
                        help='å‘é‡ç¯å¢ƒå®ç° (dummy=å•è¿›ç¨‹, subproc=å¤šè¿›ç¨‹å¹¶è¡Œ)')
    parser.add_argument('--log-update-interval', type=int, default=50,
                        help='è®°å½•æ¢¯åº¦/ä¼˜åŒ–æŒ‡æ ‡åˆ° TensorBoard çš„é—´éš”ï¼ˆæ¢¯åº¦æ­¥ï¼‰')
    parser.add_argument('--update-per-step', type=float, default=0.5,
                        help='æ¯ä¸ªç¯å¢ƒæ­¥æ‰§è¡Œçš„å‚æ•°æ›´æ–°æ¬¡æ•° (é»˜è®¤0.5)')
    args = parser.parse_args()

    if args.full_episode:
        args.episode_length = None

    if args.reward_normalization and args.n_step > 1:
        print("âš ï¸  æç¤º: n_step>1 ä¸å¥–åŠ±å½’ä¸€åŒ–ä¸å…¼å®¹ï¼Œå·²è‡ªåŠ¨å…³é—­ reward_normalization")
        args.reward_normalization = False

    if args.bc_weight_final is None:
        args.bc_weight_final = args.bc_weight

    return args


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # ========== è·å–å‚æ•° ==========
    args = get_args()
    
    # è®¾ç½®è®¾å¤‡
    args.device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # ========== åˆ›å»ºæ—¥å¿—ç›®å½• ==========
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_name = f"{args.log_prefix}_{args.building_type}_{args.weather_type}_{timestamp}"
    log_path = os.path.join(args.logdir, log_name)
    os.makedirs(log_path, exist_ok=True)
    
    # åˆ›å»º TensorBoard writer å’Œå¢å¼ºçš„æ—¥å¿—è®°å½•å™¨
    writer = SummaryWriter(log_path)
    # logger will be created after envs to inject metrics_getter
    metrics_getter = None
    logger = EnhancedTensorboardLogger(
        writer=writer,
        total_epochs=args.epoch,
        reward_scale=args.reward_scale,
        log_interval=1,  # æ¯ä¸ªepochéƒ½è¾“å‡ºï¼ˆå¯æ”¹ä¸º10è¡¨ç¤ºæ¯10ä¸ªepochè¾“å‡ºä¸€æ¬¡ï¼‰
        verbose=True,  # True=è¯¦ç»†æ ¼å¼ï¼ŒFalse=ç´§å‡‘æ ¼å¼
        diffusion_steps=args.diffusion_steps,  # æ‰©æ•£æ¨¡å‹æ­¥æ•°
        update_log_interval=args.log_update_interval,
        step_per_epoch=args.step_per_epoch,
        metrics_getter=None,  # placeholder, will be replaced later
        png_interval=5,
    )
    
    # æ‰“å°é…ç½®
    print("\n" + "=" * 60)
    print("  BEAR å»ºç­‘ç¯å¢ƒ HVAC ä¼˜åŒ–è®­ç»ƒ")
    print("=" * 60)
    print(f"\né…ç½®å‚æ•°:")
    pprint.pprint(vars(args))
    print()
    
    # ========== åˆ›å»ºç¯å¢ƒ ==========
    print("æ­£åœ¨åˆ›å»ºç¯å¢ƒ...")
    expert_kwargs = None
    if args.expert_type:
        expert_kwargs = {}
        if args.expert_type == 'mpc':
            expert_kwargs['planning_steps'] = args.mpc_planning_steps

    env, train_envs, test_envs = make_building_env(
        building_type=args.building_type,
        weather_type=args.weather_type,
        location=args.location,
        target_temp=args.target_temp,
        temp_tolerance=args.temp_tolerance,
        max_power=args.max_power,
        time_resolution=args.time_resolution,
        energy_weight=args.energy_weight,
        temp_weight=args.temp_weight,
        episode_length=args.episode_length,
        add_violation_penalty=args.add_violation_penalty,
        violation_penalty=args.violation_penalty,
        reward_scale=args.reward_scale,  # å¥–åŠ±ç¼©æ”¾ï¼Œé™ä½Qå€¼å’ŒæŸå¤±çš„å°ºåº¦
        expert_type=args.expert_type if args.bc_coef else None,
        expert_kwargs=expert_kwargs,
        training_num=args.training_num,
        test_num=args.test_num,
        vector_env_type=args.vector_env_type
    )
    
    print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"  å»ºç­‘ç±»å‹: {args.building_type}")
    print(f"  æ°”å€™ç±»å‹: {args.weather_type}")
    print(f"  åœ°ç†ä½ç½®: {args.location}")
    print(f"  æˆ¿é—´æ•°é‡: {env.roomnum}")
    print(f"  çŠ¶æ€ç»´åº¦: {env.state_dim}")
    print(f"  åŠ¨ä½œç»´åº¦: {env.action_dim}")
    print(f"  å¥–åŠ±ç¼©æ”¾ç³»æ•°: {env.reward_scale}")
    if args.expert_type:
        print(f"  ä¸“å®¶æ§åˆ¶å™¨: {args.expert_type}")
        if args.expert_type == 'mpc':
            print(f"    - MPC è§„åˆ’æ­¥æ•°: {args.mpc_planning_steps}")

    def _aggregate_metrics(vector_env):
        if vector_env is None:
            return None
        env_list = getattr(vector_env, "_env_list", None)
        if not env_list:
            return None
        values = [env_inst.consume_metrics() for env_inst in env_list]
        values = [m for m in values if m]
        if not values:
            return None
        result = {}
        for key in ('avg_energy', 'avg_comfort_mean', 'avg_violations', 'avg_pue'):
            nums = [m[key] for m in values if m.get(key) is not None]
            if nums:
                result[key] = float(np.mean(nums))
        return result if result else None

    def metrics_getter(mode: str):
        target_env = train_envs if mode == 'train' else test_envs
        return _aggregate_metrics(target_env)

    logger.training_logger.metrics_getter = metrics_getter

    # ========== åˆ›å»ºç½‘ç»œ ==========
    print("\næ­£åœ¨åˆ›å»ºç¥ç»ç½‘ç»œ...")
    state_dim = env.state_dim
    action_dim = env.action_dim
    max_action = 1.0
    
    # Actor (æ‰©æ•£æ¨¡å‹)
    actor = MLP(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim,
        t_dim=16
    ).to(args.device)

    actor_optim = torch.optim.Adam(
        actor.parameters(),
        lr=args.actor_lr,
        weight_decay=args.wd
    )

    # Critic (åŒQç½‘ç»œ)
    critic = DoubleCritic(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim
    ).to(args.device)
    
    critic_optim = torch.optim.Adam(
        critic.parameters(),
        lr=args.critic_lr,
        weight_decay=args.wd
    )
    
    print(f"âœ“ ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    print(f"  Actor å‚æ•°é‡: {sum(p.numel() for p in actor.parameters()):,}")
    print(f"  Critic å‚æ•°é‡: {sum(p.numel() for p in critic.parameters()):,}")
    
    # ========== åˆ›å»ºæ‰©æ•£æ¨¡å‹ ==========
    diffusion = Diffusion(
        state_dim=state_dim,
        action_dim=action_dim,
        model=actor,
        max_action=max_action,
        beta_schedule=args.beta_schedule,
        n_timesteps=args.diffusion_steps,
    ).to(args.device)
    
    # ========== åˆ›å»ºç­–ç•¥ ==========
    print("\næ­£åœ¨åˆ›å»ºç­–ç•¥...")
    policy = DiffusionOPT(
        state_dim=state_dim,
        actor=diffusion,  # ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸º actor
        actor_optim=actor_optim,
        action_dim=action_dim,
        critic=critic,
        critic_optim=critic_optim,
        device=args.device,
        tau=0.005,
        gamma=args.gamma,
        exploration_noise=args.exploration_noise,
        bc_coef=args.bc_coef,
        bc_weight=args.bc_weight,
        bc_weight_final=args.bc_weight_final,
        bc_weight_decay_steps=args.bc_weight_decay_steps,
        action_space=env.action_space,
        estimation_step=args.n_step,
        lr_decay=args.lr_decay,
        lr_maxt=args.epoch,
        reward_normalization=args.reward_normalization,
    )
    
    print(f"âœ“ ç­–ç•¥åˆ›å»ºæˆåŠŸ")
    print(f"  ç®—æ³•: {args.algorithm}")
    print(f"  æ‰©æ•£æ­¥æ•°: {args.diffusion_steps}")
    if args.bc_coef:
        print(f"  è¡Œä¸ºå…‹éš†æƒé‡: {args.bc_weight}")
    
    # ========== åˆ›å»ºæ”¶é›†å™¨ ==========
    print("\næ­£åœ¨åˆ›å»ºæ•°æ®æ”¶é›†å™¨...")
    buffer_num = max(1, args.training_num)
    if args.prioritized_replay:
        replay_buffer = PrioritizedVectorReplayBuffer(
            args.buffer_size,
            buffer_num=buffer_num,
            alpha=args.prior_alpha,
            beta=args.prior_beta,
        )
    else:
        replay_buffer = VectorReplayBuffer(args.buffer_size, buffer_num)

    train_collector = Collector(
        policy,
        train_envs,
        replay_buffer,
        exploration_noise=True
    )

    test_collector = Collector(policy, test_envs)

    # ä»…åœ¨å‰ warmup_steps å…³é—­é‡‡é›†å™ªå£°ï¼Œä¹‹åæ¢å¤é»˜è®¤å™ªå£°
    warmup_noise_steps = 250_000

    def train_fn(epoch: int, env_step: int):
        """åŠ¨æ€åˆ‡æ¢é‡‡é›†å™ªå£°ï¼šå‰ warmup_steps å…³é—­ï¼Œä¹‹åå¼€å¯ã€‚"""
        if not hasattr(train_collector, "exploration_noise"):
            return
        enable_noise = env_step >= warmup_noise_steps
        # ä»…åœ¨çŠ¶æ€å˜åŒ–æ—¶æ›´æ–°ï¼Œé¿å…åå¤èµ‹å€¼
        if train_collector.exploration_noise != enable_noise:
            train_collector.exploration_noise = enable_noise
            status = "å¼€å¯" if enable_noise else "å…³é—­"
            print(f"[train_fn] env_step={env_step}: å·²{status}é‡‡é›†å™ªå£°")
    
    print(f"âœ“ æ”¶é›†å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  è®­ç»ƒç¯å¢ƒæ•°: {args.training_num}")
    print(f"  æµ‹è¯•ç¯å¢ƒæ•°: {args.test_num}")
    buffer_type = "Prioritized" if args.prioritized_replay else "Uniform"
    print(f"  ç¼“å†²åŒºå¤§å°: {args.buffer_size:,} ({buffer_type})")
    
    # ========== å¼€å§‹è®­ç»ƒ ==========
    print("\n" + "=" * 60)
    print("  å¼€å§‹è®­ç»ƒ")
    print("=" * 60)
    print(f"\nâš ï¸ æ³¨æ„: å¥–åŠ±å·²ç¼©æ”¾ {args.reward_scale}x")
    print(f"\nğŸ’¡ æç¤º: æ—¥å¿—è¾“å‡ºå·²ä¼˜åŒ–ï¼Œå…³é”®æŒ‡æ ‡å°†æ¸…æ™°æ˜¾ç¤º")
    print(f"  - æ¯ä¸ªepochéƒ½ä¼šæ˜¾ç¤ºè¯¦ç»†çš„è®­ç»ƒæŒ‡æ ‡")
    print(f"  - å¼‚å¸¸å€¼ä¼šç”¨ âš  ç¬¦å·æ ‡è®°")
    print(f"  - æ—¶é—´ç»Ÿè®¡ä¼šè‡ªåŠ¨ä¼°ç®—å‰©ä½™è®­ç»ƒæ—¶é—´\n")

    result = offpolicy_trainer(
        policy=policy,
        train_collector=train_collector,
        test_collector=test_collector,
        max_epoch=args.epoch,
        step_per_epoch=args.step_per_epoch,
        step_per_collect=args.step_per_collect,
        episode_per_test=args.episode_per_test,
        batch_size=args.batch_size,
        update_per_step=args.update_per_step,
        test_in_train=False,
        logger=logger,
        save_best_fn=lambda policy: torch.save(
            policy.state_dict(),
            os.path.join(log_path, 'policy_best.pth')
        ),
        save_checkpoint_fn=lambda epoch, env_step, gradient_step: torch.save(
            {
                'model': policy.state_dict(),
                'optim_actor': actor_optim.state_dict(),
                'optim_critic': critic_optim.state_dict(),
            },
            os.path.join(log_path, f'checkpoint_{epoch}.pth')
        ) if epoch % args.save_interval == 0 else None,
        train_fn=train_fn,
    )
    
    # ========== è®­ç»ƒå®Œæˆ ==========
    print("\n" + "=" * 60)
    print("  è®­ç»ƒå®Œæˆ")
    print("=" * 60)
    pprint.pprint(result)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(policy.state_dict(), os.path.join(log_path, 'policy_final.pth'))
    print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {log_path}")


if __name__ == '__main__':
    main()
