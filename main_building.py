# ========================================
# BEAR Âª∫Á≠ëÁéØÂ¢É‰ºòÂåñËÆ≠ÁªÉ‰∏ªÁ®ãÂ∫è
# ========================================
# Âü∫‰∫é DROPT Ê°ÜÊû∂ÔºåÂ∫îÁî®Êâ©Êï£Ê®°Âûã+Âº∫ÂåñÂ≠¶‰π†Âà∞Âª∫Á≠ëHVACÊéßÂà∂

import argparse
import math
import os
import pprint
import sys
import torch
import numpy as np
from datetime import datetime
from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer
from torch.utils.tensorboard import SummaryWriter
from tianshou.utils import TensorboardLogger
from dropt_utils.tianshou_compat import offpolicy_trainer
from dropt_utils.paper_logging import add_paper_logging_args, run_paper_logging
import warnings

# ÂØºÂÖ•Âª∫Á≠ëÁéØÂ¢É
from env.building_env_wrapper import make_building_env
# ÂØºÂÖ•Êó•ÂøóÊ†ºÂºèÂåñÂ∑•ÂÖ∑
from dropt_utils.logger_formatter import EnhancedTensorboardLogger
# ÂØºÂÖ•ÈÖçÁΩÆÂ∏∏ÈáèÔºà‰øÆÂ§çÔºöÁªü‰∏ÄÁÆ°ÁêÜÈÖçÁΩÆÂèÇÊï∞ÔºåÈÅøÂÖçÁ°¨ÁºñÁ†ÅÔºâ
from env.building_config import (
    DEFAULT_REWARD_SCALE,
    DEFAULT_ENERGY_WEIGHT,
    DEFAULT_TEMP_WEIGHT,
    DEFAULT_VIOLATION_PENALTY,
    DEFAULT_TARGET_TEMP,
    DEFAULT_TEMP_TOLERANCE,
    DEFAULT_MAX_POWER,
    DEFAULT_TIME_RESOLUTION,
    DEFAULT_EPISODE_LENGTH,
    DEFAULT_TRAINING_NUM,
    DEFAULT_TEST_NUM,
    DEFAULT_BUFFER_SIZE,
    DEFAULT_BATCH_SIZE,
    DEFAULT_GAMMA,
    DEFAULT_N_STEP,
    DEFAULT_STEP_PER_EPOCH,
    DEFAULT_STEP_PER_COLLECT,
    DEFAULT_EPISODE_PER_TEST,
    DEFAULT_DIFFUSION_STEPS,
    DEFAULT_BETA_SCHEDULE,
    DEFAULT_HIDDEN_DIM,
    DEFAULT_ACTOR_LR,
    DEFAULT_CRITIC_LR,
    DEFAULT_EXPLORATION_NOISE,
    DEFAULT_LOG_DIR,
    DEFAULT_SAVE_INTERVAL,
    DEFAULT_MPC_PLANNING_STEPS,
)

# ÂØºÂÖ• DROPT Ê†∏ÂøÉÁªÑ‰ª∂
from policy import DiffusionOPT
from diffusion import Diffusion
from diffusion.model import MLP, DoubleCritic

warnings.filterwarnings('ignore')


def get_args():
    """
    Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞ÔºàÈíàÂØπÂª∫Á≠ëÁéØÂ¢ÉË∞ÉÊï¥Ôºâ
    """
    parser = argparse.ArgumentParser(description='BEAR Âª∫Á≠ëÁéØÂ¢É HVAC ‰ºòÂåñËÆ≠ÁªÉÁ®ãÂ∫è')
    
    # ========== ÁéØÂ¢ÉÂèÇÊï∞Ôºà‰ΩøÁî®ÈÖçÁΩÆÂ∏∏Èáè‰Ωú‰∏∫ÈªòËÆ§ÂÄºÔºâ ==========
    parser.add_argument('--building-type', type=str, default='OfficeSmall',
                        help='Âª∫Á≠ëÁ±ªÂûã (OfficeSmall, Hospital, SchoolPrimaryÁ≠â)')
    parser.add_argument('--weather-type', type=str, default='Hot_Dry',
                        help='Ê∞îÂÄôÁ±ªÂûã (Hot_Dry, Hot_Humid, Cold_HumidÁ≠â)')
    parser.add_argument('--location', type=str, default='Tucson',
                        help='Âú∞ÁêÜ‰ΩçÁΩÆ (Tucson, Tampa, RochesterÁ≠â)')
    parser.add_argument('--target-temp', type=float, default=DEFAULT_TARGET_TEMP,
                        help=f'ÁõÆÊ†áÊ∏©Â∫¶ (¬∞CÔºåÈªòËÆ§{DEFAULT_TARGET_TEMP})')
    parser.add_argument('--temp-tolerance', type=float, default=DEFAULT_TEMP_TOLERANCE,
                        help=f'Ê∏©Â∫¶ÂÆπÂ∑Æ (¬∞CÔºåÈªòËÆ§{DEFAULT_TEMP_TOLERANCE})')
    parser.add_argument('--max-power', type=int, default=DEFAULT_MAX_POWER,
                        help=f'HVAC ÊúÄÂ§ßÂäüÁéá (WÔºåÈªòËÆ§{DEFAULT_MAX_POWER})')
    parser.add_argument('--time-resolution', type=int, default=DEFAULT_TIME_RESOLUTION,
                        help=f'Êó∂Èó¥ÂàÜËæ®Áéá (ÁßíÔºåÈªòËÆ§{DEFAULT_TIME_RESOLUTION}=1Â∞èÊó∂)')
    parser.add_argument('--episode-length', type=int, default=DEFAULT_EPISODE_LENGTH,
                        help='Episode horizon in steps (default ~1 week)')
    parser.add_argument('--full-episode', action='store_true', default=False,
                        help='Override to use the full-year trajectory (sets episode-length=None)')
    parser.add_argument('--energy-weight', type=float, default=DEFAULT_ENERGY_WEIGHT,
                        help=f'ËÉΩËÄóÊùÉÈáç Œ± (ÈªòËÆ§{DEFAULT_ENERGY_WEIGHT})')
    parser.add_argument('--temp-weight', type=float, default=DEFAULT_TEMP_WEIGHT,
                        help=f'Ê∏©Â∫¶ÂÅèÂ∑ÆÊùÉÈáç Œ≤ (ÈªòËÆ§{DEFAULT_TEMP_WEIGHT})')
    parser.add_argument('--add-violation-penalty', dest='add_violation_penalty',
                        action='store_true', default=True,
                        help='ÂêØÁî®Ê∏©Â∫¶Ë∂äÁïåÊÉ©ÁΩöÔºàÈªòËÆ§ÂºÄÂêØÔºâ')
    parser.add_argument('--no-add-violation-penalty', dest='add_violation_penalty',
                        action='store_false',
                        help='ÂÖ≥Èó≠Ê∏©Â∫¶Ë∂äÁïåÊÉ©ÁΩö')
    parser.add_argument('--violation-penalty', type=float, default=DEFAULT_VIOLATION_PENALTY,
                        help=f'Ê∏©Â∫¶Ë∂äÁïåÊÉ©ÁΩöÁ≥ªÊï∞ Œ≥ (ÈªòËÆ§{DEFAULT_VIOLATION_PENALTY})')

    # ========== ‰∏ìÂÆ∂ÊéßÂà∂Âô®ÂèÇÊï∞ ==========
    parser.add_argument('--expert-type', type=str, default=None,
                        choices=['mpc', 'pid', 'rule', 'bangbang', None],
                        help='‰∏ìÂÆ∂ÊéßÂà∂Âô®Á±ªÂûãÔºàÁî®‰∫éË°å‰∏∫ÂÖãÈöÜÔºâ')
    parser.add_argument('--mpc-planning-steps', type=int, default=DEFAULT_MPC_PLANNING_STEPS,
                        help=f'MPC ‰∏ìÂÆ∂ËßÑÂàíÊ≠•Êï∞ (ÈªòËÆ§{DEFAULT_MPC_PLANNING_STEPS})')
    parser.add_argument('--bc-coef', action='store_true', default=False,
                        help='ÊòØÂê¶‰ΩøÁî®Ë°å‰∏∫ÂÖãÈöÜÔºàBCÔºâÊçüÂ§±')
    parser.add_argument('--bc-weight', type=float, default=0.8,
                        help='Ë°å‰∏∫ÂÖãÈöÜÊçüÂ§±ÂàùÂßãÊùÉÈáçÔºàÈªòËÆ§0.8Ôºâ')
    parser.add_argument('--bc-weight-final', type=float, default=0.1,
                        help='BCÊùÉÈáçÊúÄÁªàÂÄºÔºàÈªòËÆ§0.1ÔºåÈÄêÊ∏êËøáÊ∏°Âà∞Á≠ñÁï•Ê¢ØÂ∫¶Ôºâ')
    parser.add_argument('--bc-weight-decay-steps', type=int, default=150000,
                        help='BCÊùÉÈáçÁ∫øÊÄßË°∞ÂáèÊ≠•Êï∞ÔºàÈªòËÆ§15‰∏áÊ≠•ÔºåÂª∂Èïø‰∏ìÂÆ∂ÂºïÂØºÔºâ')
    
    # ========== Âü∫Á°ÄËÆ≠ÁªÉÂèÇÊï∞Ôºà‰ΩøÁî®ÈÖçÁΩÆÂ∏∏Èáè‰Ωú‰∏∫ÈªòËÆ§ÂÄºÔºâ ==========
    parser.add_argument('--exploration-noise', type=float, default=DEFAULT_EXPLORATION_NOISE,
                        help=f'Êé¢Á¥¢Âô™Â£∞Ê†áÂáÜÂ∑Æ (ÈªòËÆ§{DEFAULT_EXPLORATION_NOISE}ÔºåÈôç‰ΩéÈöèÊú∫Êâ∞Âä®)')
    parser.add_argument('--algorithm', type=str, default='diffusion_opt',
                        help='ÁÆóÊ≥ïÂêçÁß∞')
    parser.add_argument('--seed', type=int, default=42,
                        help='ÈöèÊú∫ÁßçÂ≠ê')
    parser.add_argument('--buffer-size', type=int, default=DEFAULT_BUFFER_SIZE,
                        help=f'ÁªèÈ™åÂõûÊîæÁºìÂÜ≤Âå∫Â§ßÂ∞è (ÈªòËÆ§{DEFAULT_BUFFER_SIZE:,})')
    parser.add_argument('-e', '--epoch', type=int, default=20000,
                        help='ÊÄªËÆ≠ÁªÉËΩÆÊ¨° (ÈªòËÆ§ 20000Ôºå‰æø‰∫éÈò∂ÊÆµÊÄßËØÑ‰º∞)')
    parser.add_argument('--step-per-epoch', type=int, default=16384,
                        help='ÊØè‰∏™epochÈááÈõÜÁöÑÁéØÂ¢ÉÊ≠•Êï∞ (ÈªòËÆ§16384)')
    parser.add_argument('--step-per-collect', type=int, default=4096,
                        help='ÊØèÊ¨°ÈááÈõÜÁöÑÊ≠•Êï∞ (ÈªòËÆ§4096ÔºåÊèêÂçáÊ†∑Êú¨Ë¥®Èáè)')
    parser.add_argument('--total-steps', type=int, default=None,
                        help='Total environment steps budget (overrides epoch if set)')
    parser.add_argument('-b', '--batch-size', type=int, default=256,
                        help='ÊâπÊ¨°Â§ßÂ∞è (ÈªòËÆ§256)')
    parser.add_argument('--wd', type=float, default=1e-4,
                        help='ÊùÉÈáçË°∞ÂáèÁ≥ªÊï∞')
    parser.add_argument('--gamma', type=float, default=DEFAULT_GAMMA,
                        help=f'ÊäòÊâ£Âõ†Â≠ê (ÈªòËÆ§{DEFAULT_GAMMA})')
    parser.add_argument('--n-step', type=int, default=DEFAULT_N_STEP,
                        help=f'NÊ≠•TDÂ≠¶‰π† (ÈªòËÆ§{DEFAULT_N_STEP})')
    parser.add_argument('--training-num', type=int, default=DEFAULT_TRAINING_NUM,
                        help=f'Âπ∂Ë°åËÆ≠ÁªÉÁéØÂ¢ÉÊï∞Èáè (ÈªòËÆ§{DEFAULT_TRAINING_NUM})')
    parser.add_argument('--test-num', type=int, default=DEFAULT_TEST_NUM,
                        help=f'Âπ∂Ë°åÊµãËØïÁéØÂ¢ÉÊï∞Èáè (ÈªòËÆ§{DEFAULT_TEST_NUM})')
    parser.add_argument('--episode-per-test', type=int, default=DEFAULT_EPISODE_PER_TEST,
                        help='ÊØèÊ¨°ËØÑ‰º∞ÊâÄË∑ëÁöÑepisodeÊï∞ÈáèÔºàÂáèÂ∞ëÂèØÁº©Áü≠ÊØèËΩÆËØÑ‰º∞Êó∂Èó¥Ôºâ')
    parser.add_argument('--prioritized-replay', action='store_true', default=False,
                        help='ÊòØÂê¶ÂêØÁî®‰ºòÂÖàÁªèÈ™åÂõûÊîæÔºàPERÔºâ')
    parser.add_argument('--prior-alpha', type=float, default=0.6,
                        help='PER ÈááÊ†∑ÂàÜÂ∏ÉÂπ≥ÊªëÂõ†Â≠ê alpha')
    parser.add_argument('--prior-beta', type=float, default=0.4,
                        help='PER ÈáçË¶ÅÊÄßÈááÊ†∑‰øÆÊ≠£ beta')

    # ========== ÁΩëÁªúÊû∂ÊûÑÂèÇÊï∞Ôºà‰ΩøÁî®ÈÖçÁΩÆÂ∏∏Èáè‰Ωú‰∏∫ÈªòËÆ§ÂÄºÔºâ ==========
    parser.add_argument('--hidden-dim', type=int, default=DEFAULT_HIDDEN_DIM,
                        help=f'MLPÈöêËóèÂ±ÇÁª¥Â∫¶ (ÈªòËÆ§{DEFAULT_HIDDEN_DIM})')
    parser.add_argument('--actor-lr', type=float, default=1e-4,
                        help='ActorÂ≠¶‰π†Áéá (ÈªòËÆ§1e-4ÔºåÈôç‰ΩéÊ¢ØÂ∫¶ÈúáËç°)')
    parser.add_argument('--critic-lr', type=float, default=DEFAULT_CRITIC_LR,
                        help=f'CriticÂ≠¶‰π†Áéá (ÈªòËÆ§{DEFAULT_CRITIC_LR})')
    parser.add_argument('--reward-scale', type=float, default=DEFAULT_REWARD_SCALE,
                        help='Scale raw rewards to stabilize critic/actor updates')
    parser.add_argument('--reward-normalization', dest='reward_normalization', action='store_true',
                        help='Enable Tianshou reward normalization (default)')
    parser.add_argument('--no-reward-normalization', dest='reward_normalization', action='store_false',
                        help='Disable reward normalization for legacy behavior')
    parser.set_defaults(reward_normalization=True)

    # ========== Êâ©Êï£Ê®°ÂûãÂèÇÊï∞Ôºà‰ΩøÁî®ÈÖçÁΩÆÂ∏∏Èáè‰Ωú‰∏∫ÈªòËÆ§ÂÄºÔºâ ==========
    parser.add_argument('--diffusion-steps', type=int, default=DEFAULT_DIFFUSION_STEPS,
                        help=f'Êâ©Êï£Ê≠•Êï∞ (ÈªòËÆ§{DEFAULT_DIFFUSION_STEPS})')
    parser.add_argument('--beta-schedule', type=str, default=DEFAULT_BETA_SCHEDULE,
                        help=f'Âô™Â£∞Ë∞ÉÂ∫¶Á±ªÂûã (ÈªòËÆ§{DEFAULT_BETA_SCHEDULE})')

    # ========== Êó•ÂøóÂíåËÆæÂ§áÂèÇÊï∞Ôºà‰ΩøÁî®ÈÖçÁΩÆÂ∏∏Èáè‰Ωú‰∏∫ÈªòËÆ§ÂÄºÔºâ ==========
    parser.add_argument('--logdir', type=str, default=DEFAULT_LOG_DIR,
                        help=f'Êó•Âøó‰øùÂ≠òÁõÆÂΩï (ÈªòËÆ§{DEFAULT_LOG_DIR})')
    parser.add_argument('--log-prefix', type=str, default='default',
                        help='Êó•ÂøóÊñá‰ª∂ÂâçÁºÄ')
    parser.add_argument('--device', type=str, default='cuda:0',
                        help='ËÆ°ÁÆóËÆæÂ§á')
    parser.add_argument('--resume-path', type=str, default=None,
                        help='ÊÅ¢Â§çËÆ≠ÁªÉÁöÑÊ®°ÂûãË∑ØÂæÑ')
    parser.add_argument('--watch', action='store_true', default=False,
                        help='ËßÇÂØüÊ®°ÂºèÔºà‰∏çËÆ≠ÁªÉÔºâ')
    parser.add_argument('--lr-decay', action='store_true', default=False,
                        help='ÊòØÂê¶‰ΩøÁî®Â≠¶‰π†ÁéáË°∞Âáè')
    parser.add_argument('--save-interval', type=int, default=DEFAULT_SAVE_INTERVAL,
                        help=f'Ê®°Âûã‰øùÂ≠òÈó¥ÈöîÔºàËΩÆÊ¨°ÔºåÈªòËÆ§{DEFAULT_SAVE_INTERVAL}Ôºâ')
    parser.add_argument('--vector-env-type', type=str, default='dummy',
                        choices=['dummy', 'subproc'],
                        help='ÂêëÈáèÁéØÂ¢ÉÂÆûÁé∞ (dummy=ÂçïËøõÁ®ã, subproc=Â§öËøõÁ®ãÂπ∂Ë°å)')
    parser.add_argument('--log-update-interval', type=int, default=50,
                        help='ËÆ∞ÂΩïÊ¢ØÂ∫¶/‰ºòÂåñÊåáÊ†áÂà∞ TensorBoard ÁöÑÈó¥ÈöîÔºàÊ¢ØÂ∫¶Ê≠•Ôºâ')
    parser.add_argument('--update-per-step', type=float, default=0.5,
                        help='ÊØè‰∏™ÁéØÂ¢ÉÊ≠•ÊâßË°åÁöÑÂèÇÊï∞Êõ¥Êñ∞Ê¨°Êï∞ (ÈªòËÆ§0.5)')
    add_paper_logging_args(parser)
    args = parser.parse_args()

    if args.full_episode:
        args.episode_length = None
    argv = sys.argv[1:]
    has_epoch_flag = any(arg in ('--epoch', '-e') for arg in argv)
    has_total_steps_flag = '--total-steps' in argv
    if not has_epoch_flag and not has_total_steps_flag:
        args.total_steps = 1_000_000
    if args.total_steps is not None and args.total_steps > 0:
        args.epoch = max(1, math.ceil(args.total_steps / args.step_per_epoch))


    if args.reward_normalization and args.n_step > 1:
        print("‚ö†Ô∏è  ÊèêÁ§∫: n_step>1 ‰∏éÂ•ñÂä±ÂΩí‰∏ÄÂåñ‰∏çÂÖºÂÆπÔºåÂ∑≤Ëá™Âä®ÂÖ≥Èó≠ reward_normalization")
        args.reward_normalization = False

    if args.bc_weight_final is None:
        args.bc_weight_final = args.bc_weight

    return args


def main():
    """‰∏ªËÆ≠ÁªÉÂáΩÊï∞"""
    # ========== Ëé∑ÂèñÂèÇÊï∞ ==========
    args = get_args()
    
    # ËÆæÁΩÆËÆæÂ§á
    args.device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # ========== ÂàõÂª∫Êó•ÂøóÁõÆÂΩï ==========
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_name = f"{args.log_prefix}_{args.building_type}_{args.weather_type}_{timestamp}"
    log_path = os.path.join(args.logdir, log_name)
    os.makedirs(log_path, exist_ok=True)
    
    # ÂàõÂª∫ TensorBoard writer ÂíåÂ¢ûÂº∫ÁöÑÊó•ÂøóËÆ∞ÂΩïÂô®
    writer = SummaryWriter(log_path)
    # logger will be created after envs to inject metrics_getter
    metrics_getter = None
    logger = EnhancedTensorboardLogger(
        writer=writer,
        total_epochs=args.epoch,
        reward_scale=args.reward_scale,
        log_interval=1,  # ÊØè‰∏™epochÈÉΩËæìÂá∫ÔºàÂèØÊîπ‰∏∫10Ë°®Á§∫ÊØè10‰∏™epochËæìÂá∫‰∏ÄÊ¨°Ôºâ
        verbose=True,  # True=ËØ¶ÁªÜÊ†ºÂºèÔºåFalse=Á¥ßÂáëÊ†ºÂºè
        diffusion_steps=args.diffusion_steps,  # Êâ©Êï£Ê®°ÂûãÊ≠•Êï∞
        update_log_interval=args.log_update_interval,
        step_per_epoch=args.step_per_epoch,
        metrics_getter=None,  # placeholder, will be replaced later
        png_interval=5,
    )
    
    # ÊâìÂç∞ÈÖçÁΩÆ
    print("\n" + "=" * 60)
    print("  BEAR Âª∫Á≠ëÁéØÂ¢É HVAC ‰ºòÂåñËÆ≠ÁªÉ")
    print("=" * 60)
    print(f"\nÈÖçÁΩÆÂèÇÊï∞:")
    pprint.pprint(vars(args))
    print()
    
    # ========== ÂàõÂª∫ÁéØÂ¢É ==========
    print("Ê≠£Âú®ÂàõÂª∫ÁéØÂ¢É...")
    expert_kwargs = None
    if args.expert_type:
        expert_kwargs = {}
        if args.expert_type == 'mpc':
            expert_kwargs['planning_steps'] = args.mpc_planning_steps

    env, train_envs, test_envs = make_building_env(
        building_type=args.building_type,
        weather_type=args.weather_type,
        location=args.location,
        target_temp=args.target_temp,
        temp_tolerance=args.temp_tolerance,
        max_power=args.max_power,
        time_resolution=args.time_resolution,
        energy_weight=args.energy_weight,
        temp_weight=args.temp_weight,
        episode_length=args.episode_length,
        add_violation_penalty=args.add_violation_penalty,
        violation_penalty=args.violation_penalty,
        reward_scale=args.reward_scale,  # Â•ñÂä±Áº©ÊîæÔºåÈôç‰ΩéQÂÄºÂíåÊçüÂ§±ÁöÑÂ∞∫Â∫¶
        expert_type=args.expert_type if args.bc_coef else None,
        expert_kwargs=expert_kwargs,
        training_num=args.training_num,
        test_num=args.test_num,
        vector_env_type=args.vector_env_type
    )
    
    print(f"‚úì ÁéØÂ¢ÉÂàõÂª∫ÊàêÂäü")
    print(f"  Âª∫Á≠ëÁ±ªÂûã: {args.building_type}")
    print(f"  Ê∞îÂÄôÁ±ªÂûã: {args.weather_type}")
    print(f"  Âú∞ÁêÜ‰ΩçÁΩÆ: {args.location}")
    print(f"  ÊàøÈó¥Êï∞Èáè: {env.roomnum}")
    print(f"  Áä∂ÊÄÅÁª¥Â∫¶: {env.state_dim}")
    print(f"  Âä®‰ΩúÁª¥Â∫¶: {env.action_dim}")
    print(f"  Â•ñÂä±Áº©ÊîæÁ≥ªÊï∞: {env.reward_scale}")
    if args.expert_type:
        print(f"  ‰∏ìÂÆ∂ÊéßÂà∂Âô®: {args.expert_type}")
        if args.expert_type == 'mpc':
            print(f"    - MPC ËßÑÂàíÊ≠•Êï∞: {args.mpc_planning_steps}")

    def _aggregate_metrics(vector_env):
        if vector_env is None:
            return None
        env_list = getattr(vector_env, "_env_list", None)
        if not env_list:
            return None
        values = [env_inst.consume_metrics() for env_inst in env_list]
        values = [m for m in values if m]
        if not values:
            return None
        result = {}
        for key in ('avg_energy', 'avg_comfort_mean', 'avg_violations', 'avg_pue'):
            nums = [m[key] for m in values if m.get(key) is not None]
            if nums:
                result[key] = float(np.mean(nums))
        return result if result else None

    def metrics_getter(mode: str):
        target_env = train_envs if mode == 'train' else test_envs
        return _aggregate_metrics(target_env)

    logger.training_logger.metrics_getter = metrics_getter

    # ========== ÂàõÂª∫ÁΩëÁªú ==========
    print("\nÊ≠£Âú®ÂàõÂª∫Á•ûÁªèÁΩëÁªú...")
    state_dim = env.state_dim
    action_dim = env.action_dim
    max_action = 1.0
    
    # Actor (Êâ©Êï£Ê®°Âûã)
    actor = MLP(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim,
        t_dim=16
    ).to(args.device)

    actor_optim = torch.optim.Adam(
        actor.parameters(),
        lr=args.actor_lr,
        weight_decay=args.wd
    )

    # Critic (ÂèåQÁΩëÁªú)
    critic = DoubleCritic(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim
    ).to(args.device)
    
    critic_optim = torch.optim.Adam(
        critic.parameters(),
        lr=args.critic_lr,
        weight_decay=args.wd
    )
    
    print(f"‚úì ÁΩëÁªúÂàõÂª∫ÊàêÂäü")
    print(f"  Actor ÂèÇÊï∞Èáè: {sum(p.numel() for p in actor.parameters()):,}")
    print(f"  Critic ÂèÇÊï∞Èáè: {sum(p.numel() for p in critic.parameters()):,}")
    
    # ========== ÂàõÂª∫Êâ©Êï£Ê®°Âûã ==========
    diffusion = Diffusion(
        state_dim=state_dim,
        action_dim=action_dim,
        model=actor,
        max_action=max_action,
        beta_schedule=args.beta_schedule,
        n_timesteps=args.diffusion_steps,
    ).to(args.device)
    
    # ========== ÂàõÂª∫Á≠ñÁï• ==========
    print("\nÊ≠£Âú®ÂàõÂª∫Á≠ñÁï•...")
    policy = DiffusionOPT(
        state_dim=state_dim,
        actor=diffusion,  # ‰ΩøÁî®Êâ©Êï£Ê®°Âûã‰Ωú‰∏∫ actor
        actor_optim=actor_optim,
        action_dim=action_dim,
        critic=critic,
        critic_optim=critic_optim,
        device=args.device,
        tau=0.005,
        gamma=args.gamma,
        exploration_noise=args.exploration_noise,
        bc_coef=args.bc_coef,
        bc_weight=args.bc_weight,
        bc_weight_final=args.bc_weight_final,
        bc_weight_decay_steps=args.bc_weight_decay_steps,
        action_space=env.action_space,
        estimation_step=args.n_step,
        lr_decay=args.lr_decay,
        lr_maxt=args.epoch,
        reward_normalization=args.reward_normalization,
    )
    
    print(f"‚úì Á≠ñÁï•ÂàõÂª∫ÊàêÂäü")
    print(f"  ÁÆóÊ≥ï: {args.algorithm}")
    print(f"  Êâ©Êï£Ê≠•Êï∞: {args.diffusion_steps}")
    if args.bc_coef:
        print(f"  Ë°å‰∏∫ÂÖãÈöÜÊùÉÈáç: {args.bc_weight}")
    
    # ========== ÂàõÂª∫Êî∂ÈõÜÂô® ==========
    print("\nÊ≠£Âú®ÂàõÂª∫Êï∞ÊçÆÊî∂ÈõÜÂô®...")
    buffer_num = max(1, args.training_num)
    if args.prioritized_replay:
        replay_buffer = PrioritizedVectorReplayBuffer(
            args.buffer_size,
            buffer_num=buffer_num,
            alpha=args.prior_alpha,
            beta=args.prior_beta,
        )
    else:
        replay_buffer = VectorReplayBuffer(args.buffer_size, buffer_num)

    train_collector = Collector(
        policy,
        train_envs,
        replay_buffer,
        exploration_noise=True
    )

    test_collector = Collector(policy, test_envs)

    # ‰ªÖÂú®Ââç warmup_steps ÂÖ≥Èó≠ÈááÈõÜÂô™Â£∞Ôºå‰πãÂêéÊÅ¢Â§çÈªòËÆ§Âô™Â£∞
    warmup_noise_steps = 250_000

    def train_fn(epoch: int, env_step: int):
        """Âä®ÊÄÅÂàáÊç¢ÈááÈõÜÂô™Â£∞ÔºöÂâç warmup_steps ÂÖ≥Èó≠Ôºå‰πãÂêéÂºÄÂêØ„ÄÇ"""
        if not hasattr(train_collector, "exploration_noise"):
            return
        enable_noise = env_step >= warmup_noise_steps
        # ‰ªÖÂú®Áä∂ÊÄÅÂèòÂåñÊó∂Êõ¥Êñ∞ÔºåÈÅøÂÖçÂèçÂ§çËµãÂÄº
        if train_collector.exploration_noise != enable_noise:
            train_collector.exploration_noise = enable_noise
            status = "ÂºÄÂêØ" if enable_noise else "ÂÖ≥Èó≠"
            print(f"[train_fn] env_step={env_step}: Â∑≤{status}ÈááÈõÜÂô™Â£∞")
    
    print(f"‚úì Êî∂ÈõÜÂô®ÂàõÂª∫ÊàêÂäü")
    print(f"  ËÆ≠ÁªÉÁéØÂ¢ÉÊï∞: {args.training_num}")
    print(f"  ÊµãËØïÁéØÂ¢ÉÊï∞: {args.test_num}")
    buffer_type = "Prioritized" if args.prioritized_replay else "Uniform"
    print(f"  ÁºìÂÜ≤Âå∫Â§ßÂ∞è: {args.buffer_size:,} ({buffer_type})")
    
    # ========== ÂºÄÂßãËÆ≠ÁªÉ ==========
    print("\n" + "=" * 60)
    print("  ÂºÄÂßãËÆ≠ÁªÉ")
    print("=" * 60)
    print(f"\n‚ö†Ô∏è Ê≥®ÊÑè: Â•ñÂä±Â∑≤Áº©Êîæ {args.reward_scale}x")
    print(f"\nüí° ÊèêÁ§∫: Êó•ÂøóËæìÂá∫Â∑≤‰ºòÂåñÔºåÂÖ≥ÈîÆÊåáÊ†áÂ∞ÜÊ∏ÖÊô∞ÊòæÁ§∫")
    print(f"  - ÊØè‰∏™epochÈÉΩ‰ºöÊòæÁ§∫ËØ¶ÁªÜÁöÑËÆ≠ÁªÉÊåáÊ†á")
    print(f"  - ÂºÇÂ∏∏ÂÄº‰ºöÁî® ‚ö† Á¨¶Âè∑Ê†áËÆ∞")
    print(f"  - Êó∂Èó¥ÁªüËÆ°‰ºöËá™Âä®‰º∞ÁÆóÂâ©‰ΩôËÆ≠ÁªÉÊó∂Èó¥\n")

    last_paper_epoch = {"value": None}

    def save_checkpoint_fn(epoch, env_step, gradient_step):
        if args.save_interval > 0 and epoch % args.save_interval == 0:
            torch.save(
                {
                    'model': policy.state_dict(),
                    'optim_actor': actor_optim.state_dict(),
                    'optim_critic': critic_optim.state_dict(),
                },
                os.path.join(log_path, f'checkpoint_{epoch}.pth')
            )
        if args.paper_log and args.paper_log_interval > 0 and epoch % args.paper_log_interval == 0:
            try:
                print(f"\n[paper-log] Epoch {epoch}: collecting trajectories and plots ...")
                run_paper_logging(
                    env=env,
                    policy=policy,
                    actor=diffusion,
                    guidance_fn=None,
                    args=args,
                    log_path=log_path,
                )
                last_paper_epoch["value"] = epoch
            except Exception as exc:
                print(f"[paper-log] Failed at epoch {epoch}: {exc}")
        return None

    result = offpolicy_trainer(
        policy=policy,
        train_collector=train_collector,
        test_collector=test_collector,
        max_epoch=args.epoch,
        step_per_epoch=args.step_per_epoch,
        step_per_collect=args.step_per_collect,
        episode_per_test=args.episode_per_test,
        batch_size=args.batch_size,
        update_per_step=args.update_per_step,
        test_in_train=False,
        logger=logger,
        save_best_fn=lambda policy: torch.save(
            policy.state_dict(),
            os.path.join(log_path, 'policy_best.pth')
        ),
        save_checkpoint_fn=save_checkpoint_fn,
        train_fn=train_fn,
    )
    
    # ========== ËÆ≠ÁªÉÂÆåÊàê ==========
    print("\n" + "=" * 60)
    print("  ËÆ≠ÁªÉÂÆåÊàê")
    print("=" * 60)
    pprint.pprint(result)
    
    # ‰øùÂ≠òÊúÄÁªàÊ®°Âûã
    torch.save(policy.state_dict(), os.path.join(log_path, 'policy_final.pth'))

    if args.paper_log:
        try:
            if args.paper_log_interval > 0 and last_paper_epoch["value"] == args.epoch:
                print("[paper-log] Skipped final logging (already captured at last epoch).")
            else:
                print("\n[paper-log] Collecting trajectories and plots ...")
                run_paper_logging(
                    env=env,
                    policy=policy,
                    actor=diffusion,
                    guidance_fn=None,
                    args=args,
                    log_path=log_path,
                )
                print(f"[paper-log] Saved to: {os.path.join(log_path, 'paper_data')}")
        except Exception as exc:
            print(f"[paper-log] Failed: {exc}")
    print(f"\n‚úì Ê®°ÂûãÂ∑≤‰øùÂ≠òÂà∞: {log_path}")


if __name__ == '__main__':
    main()
