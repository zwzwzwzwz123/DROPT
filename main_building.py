# ========================================
# BEAR å»ºç­‘ç¯å¢ƒä¼˜åŒ–è®­ç»ƒä¸»ç¨‹åº
# ========================================
# åŸºäº DROPT æ¡†æ¶ï¼Œåº”ç”¨æ‰©æ•£æ¨¡å‹+å¼ºåŒ–å­¦ä¹ åˆ°å»ºç­‘HVACæ§åˆ¶

import argparse
import os
import pprint
import torch
import numpy as np
from datetime import datetime
from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer
from torch.utils.tensorboard import SummaryWriter
from tianshou.utils import TensorboardLogger
from tianshou.trainer import offpolicy_trainer
import warnings

# å¯¼å…¥å»ºç­‘ç¯å¢ƒ
from env.building_env_wrapper import make_building_env
# å¯¼å…¥æ—¥å¿—æ ¼å¼åŒ–å·¥å…·
from utils.logger_formatter import EnhancedTensorboardLogger
# å¯¼å…¥é…ç½®å¸¸é‡ï¼ˆä¿®å¤ï¼šç»Ÿä¸€ç®¡ç†é…ç½®å‚æ•°ï¼Œé¿å…ç¡¬ç¼–ç ï¼‰
from env.building_config import (
    DEFAULT_REWARD_SCALE,
    DEFAULT_ENERGY_WEIGHT,
    DEFAULT_TEMP_WEIGHT,
    DEFAULT_VIOLATION_PENALTY,
    DEFAULT_TARGET_TEMP,
    DEFAULT_TEMP_TOLERANCE,
    DEFAULT_MAX_POWER,
    DEFAULT_TIME_RESOLUTION,
    DEFAULT_TRAINING_NUM,
    DEFAULT_TEST_NUM,
    DEFAULT_BUFFER_SIZE,
    DEFAULT_BATCH_SIZE,
    DEFAULT_GAMMA,
    DEFAULT_N_STEP,
    DEFAULT_DIFFUSION_STEPS,
    DEFAULT_BETA_SCHEDULE,
    DEFAULT_HIDDEN_DIM,
    DEFAULT_ACTOR_LR,
    DEFAULT_CRITIC_LR,
    DEFAULT_EXPLORATION_NOISE,
    DEFAULT_LOG_DIR,
    DEFAULT_SAVE_INTERVAL,
)

# å¯¼å…¥ DROPT æ ¸å¿ƒç»„ä»¶
from policy import DiffusionOPT
from diffusion import Diffusion
from diffusion.model import MLP, DoubleCritic

warnings.filterwarnings('ignore')


def get_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆé’ˆå¯¹å»ºç­‘ç¯å¢ƒè°ƒæ•´ï¼‰
    """
    parser = argparse.ArgumentParser(description='BEAR å»ºç­‘ç¯å¢ƒ HVAC ä¼˜åŒ–è®­ç»ƒç¨‹åº')
    
    # ========== ç¯å¢ƒå‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--building-type', type=str, default='OfficeSmall',
                        help='å»ºç­‘ç±»å‹ (OfficeSmall, Hospital, SchoolPrimaryç­‰)')
    parser.add_argument('--weather-type', type=str, default='Hot_Dry',
                        help='æ°”å€™ç±»å‹ (Hot_Dry, Hot_Humid, Cold_Humidç­‰)')
    parser.add_argument('--location', type=str, default='Tucson',
                        help='åœ°ç†ä½ç½® (Tucson, Tampa, Rochesterç­‰)')
    parser.add_argument('--target-temp', type=float, default=DEFAULT_TARGET_TEMP,
                        help=f'ç›®æ ‡æ¸©åº¦ (Â°Cï¼Œé»˜è®¤{DEFAULT_TARGET_TEMP})')
    parser.add_argument('--temp-tolerance', type=float, default=DEFAULT_TEMP_TOLERANCE,
                        help=f'æ¸©åº¦å®¹å·® (Â°Cï¼Œé»˜è®¤{DEFAULT_TEMP_TOLERANCE})')
    parser.add_argument('--max-power', type=int, default=DEFAULT_MAX_POWER,
                        help=f'HVAC æœ€å¤§åŠŸç‡ (Wï¼Œé»˜è®¤{DEFAULT_MAX_POWER})')
    parser.add_argument('--time-resolution', type=int, default=DEFAULT_TIME_RESOLUTION,
                        help=f'æ—¶é—´åˆ†è¾¨ç‡ (ç§’ï¼Œé»˜è®¤{DEFAULT_TIME_RESOLUTION}=1å°æ—¶)')
    parser.add_argument('--episode-length', type=int, default=None,
                        help='å›åˆé•¿åº¦ï¼ˆæ­¥æ•°ï¼ŒNoneè¡¨ç¤ºå®Œæ•´å¹´åº¦ï¼‰')
    parser.add_argument('--energy-weight', type=float, default=DEFAULT_ENERGY_WEIGHT,
                        help=f'èƒ½è€—æƒé‡ Î± (é»˜è®¤{DEFAULT_ENERGY_WEIGHT})')
    parser.add_argument('--temp-weight', type=float, default=DEFAULT_TEMP_WEIGHT,
                        help=f'æ¸©åº¦åå·®æƒé‡ Î² (é»˜è®¤{DEFAULT_TEMP_WEIGHT})')
    parser.add_argument('--add-violation-penalty', action='store_true', default=False,
                        help='æ˜¯å¦æ·»åŠ æ¸©åº¦è¶Šç•Œæƒ©ç½š')
    parser.add_argument('--violation-penalty', type=float, default=DEFAULT_VIOLATION_PENALTY,
                        help=f'æ¸©åº¦è¶Šç•Œæƒ©ç½šç³»æ•° Î³ (é»˜è®¤{DEFAULT_VIOLATION_PENALTY})')

    # ========== ä¸“å®¶æ§åˆ¶å™¨å‚æ•° ==========
    parser.add_argument('--expert-type', type=str, default=None,
                        choices=['mpc', 'pid', 'rule', 'bangbang', None],
                        help='ä¸“å®¶æ§åˆ¶å™¨ç±»å‹ï¼ˆç”¨äºè¡Œä¸ºå…‹éš†ï¼‰')
    parser.add_argument('--bc-coef', action='store_true', default=False,
                        help='æ˜¯å¦ä½¿ç”¨è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰æŸå¤±')
    parser.add_argument('--bc-weight', type=float, default=1.0,
                        help='è¡Œä¸ºå…‹éš†æŸå¤±æƒé‡')
    
    # ========== åŸºç¡€è®­ç»ƒå‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--exploration-noise', type=float, default=DEFAULT_EXPLORATION_NOISE,
                        help=f'æ¢ç´¢å™ªå£°æ ‡å‡†å·® (é»˜è®¤{DEFAULT_EXPLORATION_NOISE})')
    parser.add_argument('--algorithm', type=str, default='diffusion_opt',
                        help='ç®—æ³•åç§°')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­')
    parser.add_argument('--buffer-size', type=int, default=DEFAULT_BUFFER_SIZE,
                        help=f'ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å° (é»˜è®¤{DEFAULT_BUFFER_SIZE:,})')
    parser.add_argument('-e', '--epoch', type=int, default=50000,
                        help='æ€»è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--step-per-epoch', type=int, default=1,
                        help='æ¯ä¸ªè®­ç»ƒè½®æ¬¡çš„æ­¥æ•°')
    parser.add_argument('--step-per-collect', type=int, default=1,
                        help='æ¯æ¬¡æ”¶é›†çš„æ­¥æ•°')
    parser.add_argument('-b', '--batch-size', type=int, default=DEFAULT_BATCH_SIZE,
                        help=f'æ‰¹æ¬¡å¤§å° (é»˜è®¤{DEFAULT_BATCH_SIZE})')
    parser.add_argument('--wd', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡ç³»æ•°')
    parser.add_argument('--gamma', type=float, default=DEFAULT_GAMMA,
                        help=f'æŠ˜æ‰£å› å­ (é»˜è®¤{DEFAULT_GAMMA})')
    parser.add_argument('--n-step', type=int, default=DEFAULT_N_STEP,
                        help=f'Næ­¥TDå­¦ä¹  (é»˜è®¤{DEFAULT_N_STEP})')
    parser.add_argument('--training-num', type=int, default=DEFAULT_TRAINING_NUM,
                        help=f'å¹¶è¡Œè®­ç»ƒç¯å¢ƒæ•°é‡ (é»˜è®¤{DEFAULT_TRAINING_NUM})')
    parser.add_argument('--test-num', type=int, default=DEFAULT_TEST_NUM,
                        help=f'å¹¶è¡Œæµ‹è¯•ç¯å¢ƒæ•°é‡ (é»˜è®¤{DEFAULT_TEST_NUM})')

    # ========== ç½‘ç»œæ¶æ„å‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--hidden-dim', type=int, default=DEFAULT_HIDDEN_DIM,
                        help=f'MLPéšè—å±‚ç»´åº¦ (é»˜è®¤{DEFAULT_HIDDEN_DIM})')
    parser.add_argument('--actor-lr', type=float, default=DEFAULT_ACTOR_LR,
                        help=f'Actorå­¦ä¹ ç‡ (é»˜è®¤{DEFAULT_ACTOR_LR})')
    parser.add_argument('--critic-lr', type=float, default=DEFAULT_CRITIC_LR,
                        help=f'Criticå­¦ä¹ ç‡ (é»˜è®¤{DEFAULT_CRITIC_LR})')
    parser.add_argument('--reward-scale', type=float, default=DEFAULT_REWARD_SCALE,
                        help=f'å¥–åŠ±ç¼©æ”¾ç³»æ•°ï¼ˆé™ä½å¥–åŠ±å°ºåº¦ï¼Œç¨³å®šè®­ç»ƒï¼Œé»˜è®¤{DEFAULT_REWARD_SCALE}ï¼‰')

    # ========== æ‰©æ•£æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--diffusion-steps', type=int, default=DEFAULT_DIFFUSION_STEPS,
                        help=f'æ‰©æ•£æ­¥æ•° (é»˜è®¤{DEFAULT_DIFFUSION_STEPS})')
    parser.add_argument('--beta-schedule', type=str, default=DEFAULT_BETA_SCHEDULE,
                        help=f'å™ªå£°è°ƒåº¦ç±»å‹ (é»˜è®¤{DEFAULT_BETA_SCHEDULE})')

    # ========== æ—¥å¿—å’Œè®¾å¤‡å‚æ•°ï¼ˆä½¿ç”¨é…ç½®å¸¸é‡ä½œä¸ºé»˜è®¤å€¼ï¼‰ ==========
    parser.add_argument('--logdir', type=str, default=DEFAULT_LOG_DIR,
                        help=f'æ—¥å¿—ä¿å­˜ç›®å½• (é»˜è®¤{DEFAULT_LOG_DIR})')
    parser.add_argument('--log-prefix', type=str, default='default',
                        help='æ—¥å¿—æ–‡ä»¶å‰ç¼€')
    parser.add_argument('--device', type=str, default='cuda:0',
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--resume-path', type=str, default=None,
                        help='æ¢å¤è®­ç»ƒçš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--watch', action='store_true', default=False,
                        help='è§‚å¯Ÿæ¨¡å¼ï¼ˆä¸è®­ç»ƒï¼‰')
    parser.add_argument('--lr-decay', action='store_true', default=False,
                        help='æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡')
    parser.add_argument('--save-interval', type=int, default=DEFAULT_SAVE_INTERVAL,
                        help=f'æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆè½®æ¬¡ï¼Œé»˜è®¤{DEFAULT_SAVE_INTERVAL}ï¼‰')
    
    args = parser.parse_args()
    return args


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # ========== è·å–å‚æ•° ==========
    args = get_args()
    
    # è®¾ç½®è®¾å¤‡
    args.device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # ========== åˆ›å»ºæ—¥å¿—ç›®å½• ==========
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_name = f"{args.log_prefix}_{args.building_type}_{args.weather_type}_{timestamp}"
    log_path = os.path.join(args.logdir, log_name)
    os.makedirs(log_path, exist_ok=True)
    
    # åˆ›å»º TensorBoard writer å’Œå¢å¼ºçš„æ—¥å¿—è®°å½•å™¨
    writer = SummaryWriter(log_path)
    logger = EnhancedTensorboardLogger(
        writer=writer,
        total_epochs=args.epoch,
        reward_scale=args.reward_scale,
        log_interval=1,  # æ¯ä¸ªepochéƒ½è¾“å‡ºï¼ˆå¯æ”¹ä¸º10è¡¨ç¤ºæ¯10ä¸ªepochè¾“å‡ºä¸€æ¬¡ï¼‰
        verbose=True,  # True=è¯¦ç»†æ ¼å¼ï¼ŒFalse=ç´§å‡‘æ ¼å¼
        diffusion_steps=args.diffusion_steps  # æ‰©æ•£æ¨¡å‹æ­¥æ•°
    )
    
    # æ‰“å°é…ç½®
    print("\n" + "=" * 60)
    print("  BEAR å»ºç­‘ç¯å¢ƒ HVAC ä¼˜åŒ–è®­ç»ƒ")
    print("=" * 60)
    print(f"\né…ç½®å‚æ•°:")
    pprint.pprint(vars(args))
    print()
    
    # ========== åˆ›å»ºç¯å¢ƒ ==========
    print("æ­£åœ¨åˆ›å»ºç¯å¢ƒ...")
    env, train_envs, test_envs = make_building_env(
        building_type=args.building_type,
        weather_type=args.weather_type,
        location=args.location,
        target_temp=args.target_temp,
        temp_tolerance=args.temp_tolerance,
        max_power=args.max_power,
        time_resolution=args.time_resolution,
        energy_weight=args.energy_weight,
        temp_weight=args.temp_weight,
        episode_length=args.episode_length,
        add_violation_penalty=args.add_violation_penalty,
        violation_penalty=args.violation_penalty,
        reward_scale=args.reward_scale,  # å¥–åŠ±ç¼©æ”¾ï¼Œé™ä½Qå€¼å’ŒæŸå¤±çš„å°ºåº¦
        expert_type=args.expert_type if args.bc_coef else None,
        training_num=args.training_num,
        test_num=args.test_num
    )
    
    print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"  å»ºç­‘ç±»å‹: {args.building_type}")
    print(f"  æ°”å€™ç±»å‹: {args.weather_type}")
    print(f"  åœ°ç†ä½ç½®: {args.location}")
    print(f"  æˆ¿é—´æ•°é‡: {env.roomnum}")
    print(f"  çŠ¶æ€ç»´åº¦: {env.state_dim}")
    print(f"  åŠ¨ä½œç»´åº¦: {env.action_dim}")
    print(f"  å¥–åŠ±ç¼©æ”¾ç³»æ•°: {env.reward_scale}")
    if args.expert_type:
        print(f"  ä¸“å®¶æ§åˆ¶å™¨: {args.expert_type}")
    
    # ========== åˆ›å»ºç½‘ç»œ ==========
    print("\næ­£åœ¨åˆ›å»ºç¥ç»ç½‘ç»œ...")
    state_dim = env.state_dim
    action_dim = env.action_dim
    max_action = 1.0
    
    # Actor (æ‰©æ•£æ¨¡å‹)
    actor = MLP(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim,
        t_dim=16
    ).to(args.device)

    actor_optim = torch.optim.Adam(
        actor.parameters(),
        lr=args.actor_lr,
        weight_decay=args.wd
    )

    # Critic (åŒQç½‘ç»œ)
    critic = DoubleCritic(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim
    ).to(args.device)
    
    critic_optim = torch.optim.Adam(
        critic.parameters(),
        lr=args.critic_lr,
        weight_decay=args.wd
    )
    
    print(f"âœ“ ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    print(f"  Actor å‚æ•°é‡: {sum(p.numel() for p in actor.parameters()):,}")
    print(f"  Critic å‚æ•°é‡: {sum(p.numel() for p in critic.parameters()):,}")
    
    # ========== åˆ›å»ºæ‰©æ•£æ¨¡å‹ ==========
    diffusion = Diffusion(
        state_dim=state_dim,
        action_dim=action_dim,
        model=actor,
        max_action=max_action,
        beta_schedule=args.beta_schedule,
        n_timesteps=args.diffusion_steps,
    ).to(args.device)
    
    # ========== åˆ›å»ºç­–ç•¥ ==========
    print("\næ­£åœ¨åˆ›å»ºç­–ç•¥...")
    policy = DiffusionOPT(
        state_dim=state_dim,
        actor=diffusion,  # ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸º actor
        actor_optim=actor_optim,
        action_dim=action_dim,
        critic=critic,
        critic_optim=critic_optim,
        device=args.device,
        tau=0.005,
        gamma=args.gamma,
        exploration_noise=args.exploration_noise,
        bc_coef=args.bc_coef,
        action_space=env.action_space,
        estimation_step=args.n_step,
        lr_decay=args.lr_decay,
        lr_maxt=args.epoch,
        reward_normalization=False,  # Tianshouä¸æ”¯æŒNæ­¥å›æŠ¥+å¥–åŠ±å½’ä¸€åŒ–
    )
    
    print(f"âœ“ ç­–ç•¥åˆ›å»ºæˆåŠŸ")
    print(f"  ç®—æ³•: {args.algorithm}")
    print(f"  æ‰©æ•£æ­¥æ•°: {args.diffusion_steps}")
    if args.bc_coef:
        print(f"  è¡Œä¸ºå…‹éš†æƒé‡: {args.bc_weight}")
    
    # ========== åˆ›å»ºæ”¶é›†å™¨ ==========
    print("\næ­£åœ¨åˆ›å»ºæ•°æ®æ”¶é›†å™¨...")
    train_collector = Collector(
        policy,
        train_envs,
        VectorReplayBuffer(args.buffer_size, len(train_envs)),
        exploration_noise=True
    )
    
    test_collector = Collector(policy, test_envs)
    
    print(f"âœ“ æ”¶é›†å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  è®­ç»ƒç¯å¢ƒæ•°: {args.training_num}")
    print(f"  æµ‹è¯•ç¯å¢ƒæ•°: {args.test_num}")
    print(f"  ç¼“å†²åŒºå¤§å°: {args.buffer_size:,}")
    
    # ========== å¼€å§‹è®­ç»ƒ ==========
    print("\n" + "=" * 60)
    print("  å¼€å§‹è®­ç»ƒ")
    print("=" * 60)
    print(f"\nâš ï¸ æ³¨æ„: å¥–åŠ±å·²ç¼©æ”¾ {args.reward_scale}x")
    print(f"\nğŸ’¡ æç¤º: æ—¥å¿—è¾“å‡ºå·²ä¼˜åŒ–ï¼Œå…³é”®æŒ‡æ ‡å°†æ¸…æ™°æ˜¾ç¤º")
    print(f"  - æ¯ä¸ªepochéƒ½ä¼šæ˜¾ç¤ºè¯¦ç»†çš„è®­ç»ƒæŒ‡æ ‡")
    print(f"  - å¼‚å¸¸å€¼ä¼šç”¨ âš  ç¬¦å·æ ‡è®°")
    print(f"  - æ—¶é—´ç»Ÿè®¡ä¼šè‡ªåŠ¨ä¼°ç®—å‰©ä½™è®­ç»ƒæ—¶é—´\n")

    result = offpolicy_trainer(
        policy=policy,
        train_collector=train_collector,
        test_collector=test_collector,
        max_epoch=args.epoch,
        step_per_epoch=args.step_per_epoch,
        step_per_collect=args.step_per_collect,
        episode_per_test=args.test_num,
        batch_size=args.batch_size,
        update_per_step=1.0,
        test_in_train=False,
        logger=logger,
        save_best_fn=lambda policy: torch.save(
            policy.state_dict(),
            os.path.join(log_path, 'policy_best.pth')
        ),
        save_checkpoint_fn=lambda epoch, env_step, gradient_step: torch.save(
            {
                'model': policy.state_dict(),
                'optim_actor': actor_optim.state_dict(),
                'optim_critic': critic_optim.state_dict(),
            },
            os.path.join(log_path, f'checkpoint_{epoch}.pth')
        ) if epoch % args.save_interval == 0 else None,
    )
    
    # ========== è®­ç»ƒå®Œæˆ ==========
    print("\n" + "=" * 60)
    print("  è®­ç»ƒå®Œæˆ")
    print("=" * 60)
    pprint.pprint(result)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(policy.state_dict(), os.path.join(log_path, 'policy_final.pth'))
    print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {log_path}")


if __name__ == '__main__':
    main()

