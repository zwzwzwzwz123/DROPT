# 建筑环境奖励系统快速参考

**快速查阅版** - 5分钟了解奖励系统核心要点

---

## 📋 核心概念

### 奖励公式

```
最终奖励 = [(基础奖励) + (可选自定义奖励) + (越界惩罚)] × 奖励缩放因子

基础奖励 = -(能耗惩罚 + 舒适度惩罚)
         = -(||action||₂ × q_rate + ||error||₂ × error_rate)
```

### 三层架构

```
┌─────────────────────────────────────────────┐
│  第三层: 适配器 (building_env_wrapper.py)   │
│  - 越界惩罚: -100 × 越界房间数               │
│  - 奖励缩放: × 0.0001                        │
└─────────────────────────────────────────────┘
                    ↑
┌─────────────────────────────────────────────┐
│  第二层: 自定义 (reward_functions.py)       │
│  - CO2排放惩罚 (可选)                        │
│  - 温度范围惩罚 (可选)                       │
└─────────────────────────────────────────────┘
                    ↑
┌─────────────────────────────────────────────┐
│  第一层: BEAR核心 (env_building.py)         │
│  - 能耗惩罚: -||action||₂ × 2.4             │
│  - 舒适度惩罚: -||error||₂ × 0.9            │
└─────────────────────────────────────────────┘
```

---

## 🔧 关键配置参数

### 必须了解的5个参数

| 参数 | 默认值 | 位置 | 作用 |
|-----|--------|------|------|
| `reward_scale` | 0.0001 | `building_config.py:15` | 将奖励从-64000缩放到-6.4 |
| `energy_weight` | 0.1 | `building_config.py:16` | 能耗权重（实际×24=2.4） |
| `temp_weight` | 0.9 | `building_config.py:17` | 舒适度权重 |
| `violation_penalty` | 100.0 | `building_config.py:18` | 每个越界房间的惩罚 |
| `temp_tolerance` | 2.0 | `building_config.py:25` | 温度容差（22±2°C） |

### 快速调整指南

**场景1: 智能体过度节能，房间太冷/太热**
```python
# 增加舒适度权重
energy_weight=0.05, temp_weight=0.95
```

**场景2: 智能体不节能，能耗过高**
```python
# 增加能耗权重
energy_weight=0.3, temp_weight=0.7
```

**场景3: 训练不稳定，损失爆炸**
```python
# 增加奖励缩放（缩小奖励）
reward_scale=0.00001  # 从0.0001改为0.00001
```

**场景4: 温度经常越界**
```python
# 增加越界惩罚
violation_penalty=500.0, add_violation_penalty=True
```

---

## 📂 关键文件位置

```
DROPT/
├── BEAR/BEAR/Env/env_building.py           # 第187-193行: 默认奖励函数
├── BEAR/BEAR/Customize/reward_functions.py # 第15-46行: 自定义奖励示例
├── env/building_config.py                  # 第9-18行: 奖励配置常量
├── env/building_env_wrapper.py             # 第272-311行: 奖励适配函数
└── main_building.py                        # 第197-217行: 环境创建与配置
```

---

## 🎯 常见任务

### 任务1: 修改奖励权重

**方法A: 修改配置文件（推荐）**
```python
# env/building_config.py
DEFAULT_ENERGY_WEIGHT = 0.2  # 改为0.2
DEFAULT_TEMP_WEIGHT = 0.8    # 改为0.8
```

**方法B: 命令行参数**
```bash
python main_building.py --energy-weight 0.2 --temp-weight 0.8
```

**方法C: 代码中指定**
```python
env = BearEnvWrapper(
    energy_weight=0.2,
    temp_weight=0.8,
)
```

### 任务2: 启用越界惩罚

```python
env = BearEnvWrapper(
    add_violation_penalty=True,
    violation_penalty=100.0,
    temp_tolerance=2.0,
)
```

### 任务3: 使用自定义奖励函数

```python
# 1. 在 BEAR/BEAR/Customize/reward_functions.py 中定义函数
def my_reward_function(self, state, action, error, state_new):
    # ... 你的逻辑
    return reward

# 2. 在创建环境时指定
from BEAR.Customize import reward_functions
env = BuildingEnvReal(
    parameter,
    user_reward_function=reward_functions.my_reward_function,
    reward_breakdown_keys=['component1', 'component2']
)
```

### 任务4: 查看奖励分解

```python
# 在训练循环中
obs, reward, done, _, info = env.step(action)
bear_info = info['bear_info']
reward_breakdown = bear_info['reward_breakdown']

print(f"舒适度: {reward_breakdown['comfort_level']:.2f}")
print(f"能耗: {reward_breakdown['power_consumption']:.2f}")
```

---

## 🐛 调试技巧

### 检查奖励尺度

```python
# 运行10个episode，观察奖励范围
obs, _ = env.reset()
rewards = []
for _ in range(1000):
    action = env.action_space.sample()
    obs, reward, done, _, _ = env.step(action)
    rewards.append(reward)
    if done:
        obs, _ = env.reset()

print(f"奖励范围: [{min(rewards):.2f}, {max(rewards):.2f}]")
print(f"平均奖励: {np.mean(rewards):.2f}")
```

**期望结果**: 奖励应该在 -10 到 10 之间

### 检查奖励组成

```python
# 打印奖励分解
print(f"总奖励: {reward:.4f}")
for key, value in reward_breakdown.items():
    print(f"  {key}: {value:.4f}")
```

### 可视化奖励曲线

```python
import matplotlib.pyplot as plt

# 收集episode奖励
episode_rewards = []
for ep in range(100):
    obs, _ = env.reset()
    ep_reward = 0
    done = False
    while not done:
        action = policy(obs)
        obs, reward, done, _, _ = env.step(action)
        ep_reward += reward
    episode_rewards.append(ep_reward)

# 绘制
plt.plot(episode_rewards)
plt.xlabel('Episode')
plt.ylabel('Cumulative Reward')
plt.title('Training Progress')
plt.show()
```

---

## ⚠️ 常见问题

### Q1: 奖励值太大（如-64000），导致训练不稳定？

**A**: 调整 `reward_scale`
```python
reward_scale=0.0001  # 缩小10000倍
```

### Q2: 智能体学不到东西，奖励一直是负数？

**A**: 添加正向奖励
```python
# 在自定义奖励函数中
temp_reward = 10.0 * np.exp(-0.5 * (temp_error ** 2))  # 高斯型正奖励
reward += temp_reward
```

### Q3: 如何知道当前配置是否合理？

**A**: 检查以下指标
- 奖励范围: 应在 -10 到 10 之间
- Q值范围: 应在 -100 到 100 之间
- 损失值: 应逐渐下降
- 温度偏差: 应逐渐减小

### Q4: 能耗和舒适度无法平衡？

**A**: 调整权重比例
```python
# 当前: energy:temp = 2.4:0.9 ≈ 8:3 (能耗权重过大)
# 推荐: energy:temp = 0.6:0.9 ≈ 2:3 (舒适度优先)
energy_weight=0.025, temp_weight=0.9
```

---

## 📊 性能基准

### OfficeSmall (6房间) 在 Hot_Dry 气候下

| 指标 | 随机策略 | 训练后策略 | 目标 |
|-----|---------|-----------|------|
| 平均奖励 | -5.0 | -1.5 | > -2.0 |
| 温度偏差 | ±3.5°C | ±1.0°C | < ±2.0°C |
| 能耗 | 6000W | 3500W | < 4000W |
| 越界率 | 40% | 5% | < 10% |

---

## 🔗 相关文档

- **详细分析**: `docs/建筑环境奖励系统分析报告.md`
- **代码示例**: `docs/奖励系统代码示例与工具.md`
- **BEAR文档**: `BEAR/README.md`
- **优化总结**: `OPTIMIZATION_SUMMARY.md`

---

## 💡 快速命令

```bash
# 使用默认配置训练
python main_building.py

# 自定义奖励权重
python main_building.py --energy-weight 0.2 --temp-weight 0.8

# 启用越界惩罚
python main_building.py --add-violation-penalty --violation-penalty 100

# 调整奖励缩放
python main_building.py --reward-scale 0.0001

# 完整配置示例
python main_building.py \
    --building-type OfficeSmall \
    --weather-type Hot_Dry \
    --energy-weight 0.1 \
    --temp-weight 0.9 \
    --reward-scale 0.0001 \
    --add-violation-penalty \
    --violation-penalty 100.0 \
    --target-temp 22.0 \
    --temp-tolerance 2.0
```

---

**最后更新**: 2025-11-17  
**版本**: 1.0

