# 建筑环境奖励系统综合分析报告

**项目**: DROPT - 基于扩散模型的强化学习建筑控制系统  
**分析日期**: 2025-11-17  
**分析范围**: 建筑/建造场景中的奖励机制

---

## 目录

1. [执行摘要](#1-执行摘要)
2. [奖励相关代码定位](#2-奖励相关代码定位)
3. [奖励类型分类](#3-奖励类型分类)
4. [奖励计算逻辑](#4-奖励计算逻辑)
5. [奖励流程图](#5-奖励流程图)
6. [配置参数详解](#6-配置参数详解)
7. [系统依赖关系](#7-系统依赖关系)
8. [关键代码示例](#8-关键代码示例)
9. [问题与改进建议](#9-问题与改进建议)
10. [附录](#10-附录)

---

## 1. 执行摘要

本项目的建筑环境奖励系统采用**基于惩罚的负奖励机制**，主要目标是在保证室内温度舒适度的前提下最小化HVAC系统的能耗。奖励系统分为三个层次：

- **第一层（BEAR核心）**: 基础的能耗和舒适度惩罚
- **第二层（自定义）**: 可选的CO2排放和温度范围惩罚
- **第三层（适配器）**: 越界惩罚和奖励缩放

**关键发现**:
- 奖励尺度过大（原始值约-64000），需要通过 `reward_scale=0.0001` 进行缩放
- 缺乏正向激励，可能导致智能体采取过于保守的策略
- 奖励权重固定，无法适应不同场景需求
- 奖励分解追踪机制完善，便于调试和分析

---

## 2. 奖励相关代码定位

### 2.1 核心文件

| 文件路径 | 功能描述 | 关键类/函数 |
|---------|---------|-----------|
| `BEAR/BEAR/Env/env_building.py` | BEAR环境核心实现 | `BuildingEnvReal`, `default_reward_function` |
| `BEAR/BEAR/Customize/reward_functions.py` | 自定义奖励函数示例 | `my_custom_reward_function` |
| `env/building_env_wrapper.py` | DROPT适配器 | `BearEnvWrapper`, `_adapt_reward` |
| `env/building_config.py` | 奖励配置常量 | 各种 `DEFAULT_*` 常量 |
| `BEAR/BEAR/Utils/utils_building.py` | 参数生成器 | `ParameterGenerator` |
| `main_building.py` | 主执行脚本 | 命令行参数解析 |

### 2.2 代码行数统计

```
BEAR/BEAR/Env/env_building.py:187-193     # 默认奖励函数
BEAR/BEAR/Customize/reward_functions.py:15-46  # 自定义奖励函数
env/building_env_wrapper.py:272-311       # 奖励适配函数
env/building_config.py:9-18               # 奖励配置常量
```

---

## 3. 奖励类型分类

### 3.1 基础奖励（第一层）

#### 3.1.1 能耗惩罚 (Power Consumption Penalty)

- **公式**: `-||action||₂ × q_rate`
- **含义**: 对HVAC系统的能耗进行惩罚
- **权重**: `q_rate = gamma[0] × SCALING_FACTOR`
  - `gamma[0]`: 默认 0.1 (能耗权重)
  - `SCALING_FACTOR`: 固定值 24
  - 实际权重: `0.1 × 24 = 2.4`
- **单位**: 无量纲（经过归一化）
- **范围**: 取决于动作幅度，通常在 -10 到 -1000 之间

#### 3.1.2 舒适度惩罚 (Comfort Level Penalty)

- **公式**: `-||error||₂ × error_rate`
- **含义**: 对室内温度与目标温度的偏差进行惩罚
- **权重**: `error_rate = gamma[1]`，默认 0.9
- **单位**: 温度偏差的L2范数
- **范围**: 取决于温度偏差，通常在 -100 到 -10000 之间

### 3.2 自定义奖励（第二层，可选）

#### 3.2.1 温度范围惩罚 (Temperature Deviation Penalty)

- **公式**: `-Σ(max(0, T - T_upper) + max(0, T_lower - T)) × temp_rate`
- **含义**: 对超出特定温度范围（如18-22°C）的情况进行额外惩罚
- **权重**: `temp_rate`，默认 0.01
- **适用场景**: 需要严格控制温度范围的场合

#### 3.2.2 CO2排放惩罚 (CO2 Emission Penalty)

- **公式**: `-||action||₂ × co2_rate`
- **含义**: 将能耗与CO2排放关联，进行环境友好性惩罚
- **权重**: `co2_rate`，默认 0.01
- **适用场景**: 注重碳排放的绿色建筑

### 3.3 适配器层奖励（第三层）

#### 3.3.1 温度越界惩罚 (Violation Penalty)

- **公式**: `-violation_penalty × violation_count`
- **含义**: 当房间温度超出舒适区间时的固定惩罚
- **舒适区间**: `[target - tolerance, target + tolerance]`
  - 默认: `[22 - 2, 22 + 2] = [20, 24]°C`
- **权重**: `violation_penalty`，默认 100.0
- **启用条件**: `add_violation_penalty=True`

#### 3.3.2 奖励缩放 (Reward Scaling)

- **公式**: `reward × reward_scale`
- **含义**: 将奖励缩放到合适的数值范围，稳定训练
- **缩放因子**: `reward_scale`，默认 0.0001
- **效果**: 将原始奖励从 -64000 缩放到 -6.4

---

## 4. 奖励计算逻辑

### 4.1 默认奖励函数详解

**位置**: `BEAR/BEAR/Env/env_building.py:187-193`

```python
def default_reward_function(self, state, action, error, state_new):
    """
    默认奖励函数
    
    参数:
        state: 当前状态 (未使用)
        action: 动作向量，范围 [-1, 1]，维度 (roomnum,)
        error: 温度误差向量，维度 (roomnum,)
        state_new: 新状态 (未使用)
    
    返回:
        reward: 标量奖励值（负数）
    """
    reward = 0
    # 能耗惩罚
    power_penalty = LA.norm(action, 2) * self.q_rate
    # 舒适度惩罚
    comfort_penalty = LA.norm(error, 2) * self.error_rate
    # 总奖励
    reward -= power_penalty + comfort_penalty
    
    # 更新奖励分解（用于追踪）
    self._reward_breakdown['comfort_level'] -= comfort_penalty
    self._reward_breakdown['power_consumption'] -= power_penalty
    
    return reward
```

**数学表达式**:

```
reward = -(||a||₂ × q_rate + ||e||₂ × error_rate)

其中:
- a: 动作向量 (HVAC功率)
- e: 误差向量 (实际温度 - 目标温度)
- ||·||₂: L2范数 (欧几里得范数)
- q_rate: 能耗权重 = gamma[0] × 24
- error_rate: 舒适度权重 = gamma[1]
```

### 4.2 自定义奖励函数示例

**位置**: `BEAR/BEAR/Customize/reward_functions.py:15-46`

在默认奖励的基础上增加了两个额外惩罚项：

```python
# 温度范围惩罚
temp_deviation = np.sum(
    np.maximum(0, state_new - upper_temp) +  # 超过上限
    np.maximum(0, lower_temp - state_new)    # 低于下限
) * self.temp_rate

# CO2排放惩罚
co2_emission = LA.norm(action, 2) * self.co2_rate

# 总奖励
reward = -(action_contribution + error_contribution + 
           temp_deviation + co2_emission)
```

### 4.3 适配器层处理

**位置**: `env/building_env_wrapper.py:272-311`

```python
def _adapt_reward(self, bear_reward, state, info):
    reward = bear_reward
    
    # 1. 添加越界惩罚（可选）
    if self.add_violation_penalty:
        zone_temps = info.get('zone_temperature', state[:self.roomnum])
        violation_count = 0
        for temp in zone_temps:
            if temp < self.target_temp - self.temp_tolerance or \
               temp > self.target_temp + self.temp_tolerance:
                violation_count += 1
        if violation_count > 0:
            reward -= self.violation_penalty * violation_count
    
    # 2. 奖励缩放
    reward = reward * self.reward_scale
    
    return reward
```

### 4.4 完整计算流程

```
步骤1: 智能体输出动作 a ∈ [-1, 1]^roomnum
步骤2: 环境计算新状态 X_new = A_d @ X + B_d @ Y
步骤3: 计算温度误差 e = X_new - target
步骤4: 调用奖励函数
    - 默认: r_bear = -(||a||₂ × q_rate + ||e||₂ × error_rate)
    - 自定义: r_bear += -(temp_deviation + co2_emission)
步骤5: 适配器处理
    - 添加越界惩罚: r_adapt = r_bear - violation_penalty × count
    - 缩放: r_final = r_adapt × reward_scale
步骤6: 返回 r_final 给智能体
```

---

## 5. 奖励流程图

详见 Mermaid 图表（已在上方渲染）：
- **建筑环境奖励系统完整流程图**: 展示从动作到最终奖励的完整流程
- **奖励组成部分层次结构**: 展示三层奖励的组成关系
- **奖励配置参数流向图**: 展示配置参数如何从用户输入传递到各个模块

---

## 6. 配置参数详解

### 6.1 核心配置常量

**文件**: `env/building_config.py`

| 参数名 | 默认值 | 含义 | 推荐范围 |
|-------|--------|------|---------|
| `DEFAULT_REWARD_SCALE` | 0.0001 | 奖励缩放因子 | 0.0001 - 0.01 |
| `DEFAULT_ENERGY_WEIGHT` | 0.1 | 能耗权重 α | 0.01 - 1.0 |
| `DEFAULT_TEMP_WEIGHT` | 0.9 | 温度偏差权重 β | 0.5 - 1.0 |
| `DEFAULT_VIOLATION_PENALTY` | 100.0 | 越界惩罚系数 γ | 10.0 - 1000.0 |
| `DEFAULT_TARGET_TEMP` | 22.0 | 目标温度 (°C) | 18.0 - 26.0 |
| `DEFAULT_TEMP_TOLERANCE` | 2.0 | 温度容差 (°C) | 1.0 - 5.0 |

### 6.2 权重比例分析

**能耗 vs 舒适度权重比**:
```
默认配置: energy_weight : temp_weight = 0.1 : 0.9 = 1 : 9
实际权重: q_rate : error_rate = (0.1 × 24) : 0.9 = 2.4 : 0.9 ≈ 8 : 3
```

**解读**: 虽然配置中舒适度权重更高，但由于 `SCALING_FACTOR=24` 的存在，实际上能耗惩罚的影响更大。这可能导致智能体过度节能而牺牲舒适度。

### 6.3 奖励尺度示例

假设一个6房间的建筑（`OfficeSmall`）：

**场景1: 全功率制冷**
```
action = [-1, -1, -1, -1, -1, -1]  # 所有房间全功率制冷
||action||₂ = √6 ≈ 2.45
power_penalty = 2.45 × 2.4 ≈ 5.88
```

**场景2: 温度偏差**
```
error = [2, 1.5, 1, 0.5, 0.5, 0.5]  # 各房间温度偏差 (°C)
||error||₂ = √(4 + 2.25 + 1 + 0.25 + 0.25 + 0.25) ≈ 2.83
comfort_penalty = 2.83 × 0.9 ≈ 2.55
```

**总奖励（缩放前）**: `-(5.88 + 2.55) = -8.43`  
**总奖励（缩放后）**: `-8.43 × 0.0001 = -0.000843`

---

## 7. 系统依赖关系

### 7.1 与建筑物理模型的依赖

```
建筑参数 → RC模型 → 状态转移 → 温度误差 → 奖励
```

- **RC模型**: 电阻-电容热力学模型，描述建筑的热传导特性
- **状态转移矩阵**: `A_d`, `B_d`，由建筑参数和时间分辨率决定
- **温度误差**: 直接影响舒适度惩罚

### 7.2 与环境因素的依赖

```
外部因素 → 建筑状态 → 奖励
```

- **室外温度** (`OutTemp`): 影响建筑热负荷
- **太阳辐照** (`ghi`): 影响窗户得热
- **地面温度** (`GroundTemp`): 影响底层房间热损失
- **人员负荷** (`Occupancy`): 影响内部得热

### 7.3 与强化学习算法的依赖

```
奖励 → Q值估计 → 策略更新 → 动作选择
```

- **奖励尺度**: 影响Q值的数值范围，进而影响神经网络的训练稳定性
- **奖励稀疏性**: 当前奖励是密集的（每步都有），有利于学习
- **奖励延迟**: 当前奖励是即时的，没有长期延迟效应

---

## 8. 关键代码示例

### 8.1 环境初始化与奖励权重设置

```python
# main_building.py
env, train_envs, test_envs = make_building_env(
    building_type='OfficeSmall',
    weather_type='Hot_Dry',
    location='Tucson',
    target_temp=22.0,
    temp_tolerance=2.0,
    energy_weight=0.1,      # 能耗权重
    temp_weight=0.9,        # 舒适度权重
    reward_scale=0.0001,    # 奖励缩放
    add_violation_penalty=False,
    violation_penalty=100.0,
)
```

### 8.2 奖励分解追踪

```python
# 在训练循环中访问奖励分解
bear_info = info['bear_info']
reward_breakdown = bear_info.get('reward_breakdown', {})

print(f"舒适度惩罚: {reward_breakdown.get('comfort_level', 0):.2f}")
print(f"能耗惩罚: {reward_breakdown.get('power_consumption', 0):.2f}")
```

### 8.3 自定义奖励函数注册

```python
# BEAR/BEAR/Customize/reward_functions.py
def my_reward_function(self, state, action, error, state_new):
    # 自定义逻辑
    reward = ...
    # 更新分解
    self._reward_breakdown['my_component'] = ...
    return reward

# 在环境初始化时使用
env = BuildingEnvReal(
    parameter,
    user_reward_function=my_reward_function,
    reward_breakdown_keys=['my_component', 'another_component']
)
```

---

## 9. 问题与改进建议

### 9.1 已识别的问题

#### 问题1: 奖励尺度不一致

**现象**: 原始奖励值在 -10000 到 -100000 之间，远大于Q值（约-7）

**影响**:
- 神经网络训练不稳定
- 梯度爆炸风险
- 收敛速度慢

**当前解决方案**: 使用 `reward_scale=0.0001` 进行缩放

**改进建议**:
```python
# 自适应奖励缩放
def auto_scale_reward(env, num_episodes=10):
    """在训练前自动估算合适的reward_scale"""
    rewards = []
    for _ in range(num_episodes):
        obs, _ = env.reset()
        done = False
        while not done:
            action = env.action_space.sample()  # 随机动作
            obs, reward, done, _, _ = env.step(action)
            rewards.append(abs(reward))
    
    mean_reward = np.mean(rewards)
    # 目标: 将平均奖励缩放到 1-10 范围
    optimal_scale = 5.0 / mean_reward
    return optimal_scale
```

#### 问题2: 缺乏正向激励

**现象**: 所有奖励都是负数（惩罚），没有正向奖励

**影响**:
- 智能体可能采取过于保守的策略（如完全关闭HVAC）
- 缺乏对"接近最优"状态的明确奖励信号

**改进建议**: 引入高斯型温度奖励

```python
def improved_reward_function(self, state, action, error, state_new):
    """改进的奖励函数，包含正向激励"""
    # 1. 温度舒适度奖励（正向）
    temp_error = LA.norm(error, 2)
    temp_reward = 10.0 * np.exp(-0.5 * (temp_error ** 2))  # 在目标温度时最大
    
    # 2. 能耗惩罚（负向）
    energy_penalty = LA.norm(action, 2) * self.q_rate
    
    # 3. 基础存活奖励（正向）
    base_reward = 1.0
    
    # 总奖励
    reward = base_reward + temp_reward - energy_penalty
    
    # 更新分解
    self._reward_breakdown['base'] = base_reward
    self._reward_breakdown['temp_reward'] = temp_reward
    self._reward_breakdown['energy_penalty'] = -energy_penalty
    
    return reward
```

#### 问题3: 权重固定不灵活

**现象**: 能耗和舒适度权重在整个训练过程中固定

**影响**:
- 无法适应不同时间段的需求（如电价峰谷）
- 无法适应不同用户偏好

**改进建议**: 将权重作为状态的一部分

```python
# 扩展状态空间
class AdaptiveWeightWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        # 扩展状态空间，增加2个维度（能耗权重、舒适度权重）
        low = np.append(env.observation_space.low, [0.0, 0.0])
        high = np.append(env.observation_space.high, [1.0, 1.0])
        self.observation_space = spaces.Box(low, high, dtype=np.float32)
    
    def step(self, action):
        obs, reward, done, truncated, info = self.env.step(action)
        # 根据时间或其他因素动态调整权重
        current_hour = self.env.current_step % 24
        if 9 <= current_hour <= 17:  # 工作时间
            energy_weight, temp_weight = 0.3, 0.7  # 更注重舒适
        else:  # 非工作时间
            energy_weight, temp_weight = 0.7, 0.3  # 更注重节能
        
        # 将权重添加到状态
        obs = np.append(obs, [energy_weight, temp_weight])
        return obs, reward, done, truncated, info
```

#### 问题4: 越界惩罚未追踪

**现象**: `violation_penalty` 在 `_adapt_reward` 中添加，但未记录到 `reward_breakdown`

**影响**: 无法分析越界惩罚对总奖励的贡献

**改进建议**:

```python
def _adapt_reward(self, bear_reward, state, info):
    reward = bear_reward
    violation_penalty_value = 0.0
    
    if self.add_violation_penalty:
        zone_temps = info.get('zone_temperature', state[:self.roomnum])
        violation_count = 0
        for temp in zone_temps:
            if temp < self.target_temp - self.temp_tolerance or \
               temp > self.target_temp + self.temp_tolerance:
                violation_count += 1
        if violation_count > 0:
            violation_penalty_value = self.violation_penalty * violation_count
            reward -= violation_penalty_value
    
    # 记录到info中
    info['violation_penalty'] = -violation_penalty_value
    info['reward_before_scale'] = reward
    
    # 奖励缩放
    reward = reward * self.reward_scale
    info['reward_scale_factor'] = self.reward_scale
    
    return reward
```

### 9.2 性能优化建议

#### 建议1: 奖励归一化

```python
from tianshou.utils import RunningMeanStd

class RewardNormalizer:
    def __init__(self):
        self.rms = RunningMeanStd()
    
    def normalize(self, reward):
        self.rms.update(np.array([reward]))
        return (reward - self.rms.mean) / (self.rms.std + 1e-8)
```

#### 建议2: 奖励裁剪

```python
def clip_reward(reward, min_val=-10.0, max_val=10.0):
    """防止极端奖励值"""
    return np.clip(reward, min_val, max_val)
```

---

## 10. 附录

### 10.1 术语表

| 术语 | 英文 | 解释 |
|-----|------|------|
| HVAC | Heating, Ventilation, and Air Conditioning | 供暖、通风与空调系统 |
| RC模型 | Resistance-Capacitance Model | 电阻-电容热力学模型 |
| GHI | Global Horizontal Irradiance | 全局水平辐照度 |
| L2范数 | L2 Norm / Euclidean Norm | 欧几里得范数，向量元素平方和的平方根 |
| PMV/PPD | Predicted Mean Vote / Predicted Percentage of Dissatisfied | 预测平均投票/预测不满意百分比（舒适度指标） |

### 10.2 参考文献

1. BEAR 文档: `BEAR/README.md`
2. DROPT 优化总结: `OPTIMIZATION_SUMMARY.md`
3. 建筑配置文件: `env/building_config.py`

### 10.3 相关文件清单

```
DROPT/
├── BEAR/
│   ├── BEAR/
│   │   ├── Env/
│   │   │   └── env_building.py          # 核心环境
│   │   ├── Customize/
│   │   │   └── reward_functions.py      # 自定义奖励
│   │   └── Utils/
│   │       └── utils_building.py        # 参数生成器
├── env/
│   ├── building_config.py               # 配置常量
│   ├── building_env_wrapper.py          # 适配器
│   └── building_expert_controller.py    # 专家控制器
├── main_building.py                     # 主脚本
└── docs/
    └── 建筑环境奖励系统分析报告.md      # 本文档
```

---

**报告结束**

