# 奖励系统代码示例与实用工具

本文档提供建筑环境奖励系统的实用代码示例、调试工具和最佳实践。

---

## 目录

1. [奖励函数示例](#1-奖励函数示例)
2. [调试与可视化工具](#2-调试与可视化工具)
3. [奖励分析脚本](#3-奖励分析脚本)
4. [最佳实践](#4-最佳实践)

---

## 1. 奖励函数示例

### 1.1 基础奖励函数（当前实现）

```python
# BEAR/BEAR/Env/env_building.py
def default_reward_function(self, state, action, error, state_new):
    """
    默认奖励函数：平衡能耗和舒适度
    
    参数:
        state: 当前状态 [roomnum + 1 + roomnum + 1 + roomnum]
        action: 动作 [roomnum], 范围 [-1, 1]
        error: 温度误差 [roomnum]
        state_new: 新状态 [roomnum]
    
    返回:
        reward: 标量奖励值
    """
    reward = 0
    # 能耗惩罚: ||action||_2 * q_rate
    power_penalty = LA.norm(action, 2) * self.q_rate
    # 舒适度惩罚: ||error||_2 * error_rate
    comfort_penalty = LA.norm(error, 2) * self.error_rate
    # 总奖励
    reward -= power_penalty + comfort_penalty
    
    # 更新奖励分解
    self._reward_breakdown['comfort_level'] -= comfort_penalty
    self._reward_breakdown['power_consumption'] -= power_penalty
    
    return reward
```

### 1.2 改进版奖励函数（推荐）

```python
# 在 BEAR/BEAR/Customize/reward_functions.py 中添加
import numpy as np
from numpy import linalg as LA

def improved_reward_function_v2(self, state, action, error, state_new):
    """
    改进的奖励函数 v2
    
    改进点:
    1. 添加正向温度舒适度奖励（高斯型）
    2. 降低能耗惩罚权重
    3. 添加基础存活奖励
    4. 添加温度越界惩罚
    
    预期奖励范围: -20 ~ +15 (单步)
    """
    reward = 0
    
    # ========== 1. 基础存活奖励 (正向) ==========
    base_reward = 1.0
    reward += base_reward
    
    # ========== 2. 温度舒适度奖励 (正向, 高斯型) ==========
    # 在目标温度时奖励最大，偏离时指数衰减
    temp_error_norm = LA.norm(error, 2)
    temp_reward = 10.0 * np.exp(-0.5 * (temp_error_norm ** 2))
    reward += temp_reward
    
    # ========== 3. 温度惩罚 (负向, 降低权重) ==========
    # 原始 error_rate 可能过大，降低10倍
    temp_penalty = 0.1 * self.error_rate * (temp_error_norm ** 2)
    reward -= temp_penalty
    
    # ========== 4. 能耗惩罚 (负向, 归一化) ==========
    # 归一化到 [0, 1] 范围
    action_norm = LA.norm(action, 2)
    max_action_norm = np.sqrt(len(action))  # 最大可能的L2范数
    normalized_energy = action_norm / max_action_norm
    energy_penalty = 5.0 * normalized_energy
    reward -= energy_penalty
    
    # ========== 5. 温度越界惩罚 (负向) ==========
    # 检查每个房间是否越界
    violation_penalty = 0.0
    target_temp = 22.0  # 或使用 self.target
    tolerance = 2.0
    for temp in state_new:
        if temp < target_temp - tolerance or temp > target_temp + tolerance:
            violation_penalty += 10.0  # 每个越界房间惩罚10
    reward -= violation_penalty
    
    # ========== 6. 更新奖励分解 ==========
    self._reward_breakdown['base_reward'] = base_reward
    self._reward_breakdown['temp_reward'] = temp_reward
    self._reward_breakdown['temp_penalty'] = -temp_penalty
    self._reward_breakdown['energy_penalty'] = -energy_penalty
    self._reward_breakdown['violation_penalty'] = -violation_penalty
    
    return reward
```

### 1.3 多目标奖励函数

```python
def multi_objective_reward_function(self, state, action, error, state_new):
    """
    多目标奖励函数：考虑能耗、舒适度、CO2排放和成本
    
    适用场景：需要平衡多个优化目标的复杂建筑
    """
    reward = 0
    
    # 权重配置（可从外部传入）
    w_comfort = 0.4
    w_energy = 0.3
    w_co2 = 0.2
    w_cost = 0.1
    
    # 1. 舒适度得分 (0-100)
    temp_error_norm = LA.norm(error, 2)
    comfort_score = 100.0 * np.exp(-0.1 * temp_error_norm)
    
    # 2. 能耗得分 (0-100, 越低越好)
    action_norm = LA.norm(action, 2)
    energy_score = 100.0 * (1.0 - action_norm / np.sqrt(len(action)))
    
    # 3. CO2排放得分 (假设与能耗成正比)
    co2_emission = action_norm * 0.5  # kg CO2
    co2_score = 100.0 * (1.0 - co2_emission / 10.0)  # 归一化
    
    # 4. 成本得分 (考虑峰谷电价)
    current_hour = self.epochs % 24
    if 9 <= current_hour <= 17:  # 峰时电价
        electricity_price = 1.0
    else:  # 谷时电价
        electricity_price = 0.5
    cost = action_norm * electricity_price
    cost_score = 100.0 * (1.0 - cost / 5.0)
    
    # 加权求和
    reward = (w_comfort * comfort_score + 
              w_energy * energy_score + 
              w_co2 * co2_score + 
              w_cost * cost_score)
    
    # 更新奖励分解
    self._reward_breakdown['comfort_score'] = comfort_score
    self._reward_breakdown['energy_score'] = energy_score
    self._reward_breakdown['co2_score'] = co2_score
    self._reward_breakdown['cost_score'] = cost_score
    
    return reward
```

### 1.4 自适应权重奖励函数

```python
def adaptive_weight_reward_function(self, state, action, error, state_new):
    """
    自适应权重奖励函数：根据时间和状态动态调整权重
    
    适用场景：需要根据不同时间段或状态调整优化目标
    """
    reward = 0
    
    # 获取当前时间（小时）
    current_hour = self.epochs % 24
    
    # 根据时间段调整权重
    if 0 <= current_hour < 6:  # 深夜：极度节能
        energy_weight = 0.9
        comfort_weight = 0.1
    elif 6 <= current_hour < 9:  # 早晨：准备阶段
        energy_weight = 0.5
        comfort_weight = 0.5
    elif 9 <= current_hour < 18:  # 工作时间：舒适优先
        energy_weight = 0.2
        comfort_weight = 0.8
    elif 18 <= current_hour < 22:  # 傍晚：平衡
        energy_weight = 0.4
        comfort_weight = 0.6
    else:  # 夜晚：节能
        energy_weight = 0.7
        comfort_weight = 0.3
    
    # 根据室外温度调整权重（极端天气下更注重舒适）
    outdoor_temp = state[self.roomnum]  # 室外温度
    if outdoor_temp < 0 or outdoor_temp > 35:  # 极端天气
        comfort_weight *= 1.5
        energy_weight *= 0.5
    
    # 归一化权重
    total_weight = energy_weight + comfort_weight
    energy_weight /= total_weight
    comfort_weight /= total_weight
    
    # 计算奖励
    energy_penalty = LA.norm(action, 2) * self.q_rate * energy_weight
    comfort_penalty = LA.norm(error, 2) * self.error_rate * comfort_weight
    reward = -(energy_penalty + comfort_penalty)
    
    # 更新奖励分解
    self._reward_breakdown['energy_penalty'] = -energy_penalty
    self._reward_breakdown['comfort_penalty'] = -comfort_penalty
    self._reward_breakdown['energy_weight'] = energy_weight
    self._reward_breakdown['comfort_weight'] = comfort_weight
    
    return reward
```

---

## 2. 调试与可视化工具

### 2.1 奖励追踪器

```python
# utils/reward_tracker.py
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

class RewardTracker:
    """追踪和分析奖励的各个组成部分"""
    
    def __init__(self):
        self.episode_rewards = []
        self.step_rewards = []
        self.reward_components = defaultdict(list)
        self.current_episode_reward = 0
        self.current_episode_components = defaultdict(float)
    
    def add_step(self, reward, reward_breakdown):
        """添加一个步骤的奖励"""
        self.step_rewards.append(reward)
        self.current_episode_reward += reward
        
        for key, value in reward_breakdown.items():
            self.current_episode_components[key] += value
    
    def end_episode(self):
        """结束当前episode，保存统计"""
        self.episode_rewards.append(self.current_episode_reward)
        for key, value in self.current_episode_components.items():
            self.reward_components[key].append(value)
        
        # 重置
        self.current_episode_reward = 0
        self.current_episode_components = defaultdict(float)
    
    def plot_episode_rewards(self, save_path=None):
        """绘制episode奖励曲线"""
        plt.figure(figsize=(12, 6))
        plt.plot(self.episode_rewards, label='Total Reward')
        plt.xlabel('Episode')
        plt.ylabel('Cumulative Reward')
        plt.title('Episode Rewards Over Time')
        plt.legend()
        plt.grid(True)
        if save_path:
            plt.savefig(save_path)
        plt.show()
    
    def plot_reward_components(self, save_path=None):
        """绘制奖励组成部分"""
        plt.figure(figsize=(14, 8))
        for key, values in self.reward_components.items():
            plt.plot(values, label=key, alpha=0.7)
        plt.xlabel('Episode')
        plt.ylabel('Cumulative Component Value')
        plt.title('Reward Components Over Time')
        plt.legend()
        plt.grid(True)
        if save_path:
            plt.savefig(save_path)
        plt.show()
    
    def print_statistics(self, last_n_episodes=100):
        """打印最近N个episode的统计信息"""
        recent_rewards = self.episode_rewards[-last_n_episodes:]
        print(f"\n===== 最近 {len(recent_rewards)} 个 Episode 统计 =====")
        print(f"平均奖励: {np.mean(recent_rewards):.2f}")
        print(f"标准差: {np.std(recent_rewards):.2f}")
        print(f"最大奖励: {np.max(recent_rewards):.2f}")
        print(f"最小奖励: {np.min(recent_rewards):.2f}")
        
        print("\n奖励组成部分平均值:")
        for key, values in self.reward_components.items():
            recent_values = values[-last_n_episodes:]
            print(f"  {key}: {np.mean(recent_values):.2f}")
```

### 2.2 奖励可视化脚本

```python
# scripts/visualize_rewards.py
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_reward_distribution(rewards, title="Reward Distribution"):
    """可视化奖励分布"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. 直方图
    axes[0, 0].hist(rewards, bins=50, edgecolor='black', alpha=0.7)
    axes[0, 0].set_xlabel('Reward')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title(f'{title} - Histogram')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. 箱线图
    axes[0, 1].boxplot(rewards, vert=True)
    axes[0, 1].set_ylabel('Reward')
    axes[0, 1].set_title(f'{title} - Boxplot')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. 时间序列
    axes[1, 0].plot(rewards, alpha=0.6)
    axes[1, 0].set_xlabel('Step')
    axes[1, 0].set_ylabel('Reward')
    axes[1, 0].set_title(f'{title} - Time Series')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. 核密度估计
    sns.kdeplot(rewards, ax=axes[1, 1], fill=True)
    axes[1, 1].set_xlabel('Reward')
    axes[1, 1].set_ylabel('Density')
    axes[1, 1].set_title(f'{title} - KDE')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def compare_reward_functions(rewards_dict):
    """比较不同奖励函数的效果"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # 1. 箱线图比较
    data = [rewards for rewards in rewards_dict.values()]
    labels = list(rewards_dict.keys())
    axes[0].boxplot(data, labels=labels)
    axes[0].set_ylabel('Reward')
    axes[0].set_title('Reward Function Comparison - Boxplot')
    axes[0].grid(True, alpha=0.3)
    
    # 2. 累积分布函数
    for name, rewards in rewards_dict.items():
        sorted_rewards = np.sort(rewards)
        cdf = np.arange(1, len(sorted_rewards) + 1) / len(sorted_rewards)
        axes[1].plot(sorted_rewards, cdf, label=name, alpha=0.7)
    axes[1].set_xlabel('Reward')
    axes[1].set_ylabel('CDF')
    axes[1].set_title('Reward Function Comparison - CDF')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
```

---

## 3. 奖励分析脚本

### 3.1 奖励尺度分析

```python
# scripts/analyze_reward_scale.py
import numpy as np
from env.building_env_wrapper import BearEnvWrapper

def analyze_reward_scale(env, num_episodes=10):
    """
    分析奖励尺度，推荐合适的 reward_scale
    
    参数:
        env: 环境实例
        num_episodes: 分析的episode数量
    
    返回:
        recommended_scale: 推荐的reward_scale值
    """
    print("正在分析奖励尺度...")
    
    rewards = []
    for ep in range(num_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0
        while not done:
            action = env.action_space.sample()  # 随机动作
            obs, reward, done, _, info = env.step(action)
            rewards.append(abs(reward))
            episode_reward += reward
        print(f"Episode {ep+1}: 总奖励 = {episode_reward:.2f}")
    
    # 统计
    mean_reward = np.mean(rewards)
    std_reward = np.std(rewards)
    max_reward = np.max(rewards)
    min_reward = np.min(rewards)
    
    print(f"\n===== 奖励统计 =====")
    print(f"平均奖励（绝对值）: {mean_reward:.2f}")
    print(f"标准差: {std_reward:.2f}")
    print(f"最大值: {max_reward:.2f}")
    print(f"最小值: {min_reward:.2f}")
    
    # 推荐缩放因子
    # 目标：将平均奖励缩放到 1-10 范围
    target_reward = 5.0
    recommended_scale = target_reward / mean_reward
    
    print(f"\n===== 推荐配置 =====")
    print(f"推荐 reward_scale: {recommended_scale:.6f}")
    print(f"缩放后平均奖励: {mean_reward * recommended_scale:.2f}")
    
    return recommended_scale

# 使用示例
if __name__ == "__main__":
    from env.building_env_wrapper import make_building_env
    
    env, _, _ = make_building_env(
        building_type='OfficeSmall',
        weather_type='Hot_Dry',
        location='Tucson',
        reward_scale=1.0,  # 先不缩放，分析原始奖励
    )
    
    recommended_scale = analyze_reward_scale(env, num_episodes=10)
```

### 3.2 奖励组成分析

```python
# scripts/analyze_reward_components.py
import numpy as np
import pandas as pd

def analyze_reward_components(env, num_steps=1000):
    """
    分析奖励各组成部分的贡献
    
    参数:
        env: 环境实例
        num_steps: 分析的步数
    
    返回:
        df: 包含各组成部分统计的DataFrame
    """
    print("正在分析奖励组成部分...")
    
    components_history = []
    
    obs, _ = env.reset()
    for step in range(num_steps):
        action = env.action_space.sample()
        obs, reward, done, _, info = env.step(action)
        
        # 提取奖励分解
        bear_info = info.get('bear_info', {})
        reward_breakdown = bear_info.get('reward_breakdown', {})
        components_history.append(reward_breakdown.copy())
        
        if done:
            obs, _ = env.reset()
    
    # 转换为DataFrame
    df = pd.DataFrame(components_history)
    
    # 统计
    print("\n===== 奖励组成部分统计 =====")
    print(df.describe())
    
    # 计算各组成部分的平均贡献比例
    print("\n===== 平均贡献比例 =====")
    mean_values = df.mean()
    total_abs = mean_values.abs().sum()
    for key, value in mean_values.items():
        percentage = abs(value) / total_abs * 100
        print(f"{key}: {percentage:.2f}%")
    
    return df

# 使用示例
if __name__ == "__main__":
    from env.building_env_wrapper import make_building_env
    
    env, _, _ = make_building_env(
        building_type='OfficeSmall',
        weather_type='Hot_Dry',
        location='Tucson',
    )
    
    df = analyze_reward_components(env, num_steps=1000)
    
    # 可视化
    import matplotlib.pyplot as plt
    df.plot(kind='box', figsize=(12, 6))
    plt.title('Reward Components Distribution')
    plt.ylabel('Value')
    plt.grid(True)
    plt.show()
```

---

## 4. 最佳实践

### 4.1 奖励函数设计原则

1. **平衡性**: 确保各个目标（能耗、舒适度等）的权重合理
2. **可解释性**: 奖励函数应该易于理解和调试
3. **稳定性**: 避免奖励值波动过大
4. **引导性**: 奖励应该引导智能体朝正确方向学习

### 4.2 调试检查清单

- [ ] 奖励尺度是否合理（建议在 -10 到 10 之间）
- [ ] 奖励是否有正向信号（不全是负数）
- [ ] 各组成部分的贡献比例是否合理
- [ ] 奖励是否随着策略改进而提高
- [ ] 奖励分解是否正确记录

### 4.3 常见问题与解决方案

**问题1: 奖励过大导致训练不稳定**
```python
# 解决方案：使用reward_scale缩放
env = BearEnvWrapper(..., reward_scale=0.0001)
```

**问题2: 智能体过度节能，牺牲舒适度**
```python
# 解决方案：增加舒适度权重或添加正向奖励
reward_gamma = (0.05, 0.95)  # 降低能耗权重，提高舒适度权重
```

**问题3: 奖励信号稀疏，学习困难**
```python
# 解决方案：添加中间奖励（shaping）
def shaped_reward_function(self, state, action, error, state_new):
    base_reward = self.default_reward_function(state, action, error, state_new)
    # 添加接近目标的奖励
    proximity_bonus = 1.0 / (1.0 + LA.norm(error, 2))
    return base_reward + proximity_bonus
```

---

**文档结束**

