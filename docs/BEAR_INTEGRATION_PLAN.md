# BEAR å»ºç­‘æ¨¡æ‹Ÿç¯å¢ƒé›†æˆæ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•
1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [BEAR ä»£ç åˆ†æ](#bear-ä»£ç åˆ†æ)
3. [DROPT ç¯å¢ƒæ¥å£åˆ†æ](#dropt-ç¯å¢ƒæ¥å£åˆ†æ)
4. [é›†æˆæ¶æ„è®¾è®¡](#é›†æˆæ¶æ„è®¾è®¡)
5. [å®ç°æ­¥éª¤](#å®ç°æ­¥éª¤)
6. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 BEAR é¡¹ç›®ç®€ä»‹
**BEAR** (Building Environment for Control And Reinforcement Learning) æ˜¯ä¸€ä¸ªåŸºäºç‰©ç†åŸç†çš„å»ºç­‘ç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼Œä¸“ä¸ºæ§åˆ¶å’Œå¼ºåŒ–å­¦ä¹ è®¾è®¡ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- âœ… **16ç§å»ºç­‘ç±»å‹**ï¼šåŠå…¬æ¥¼ã€åŒ»é™¢ã€é…’åº—ã€å­¦æ ¡ã€ä»“åº“ç­‰
- âœ… **19ä¸ªåœ°ç†ä½ç½®**ï¼šè¦†ç›–å…¨çƒä¸åŒæ°”å€™åŒº
- âœ… **ç‰©ç†å»ºæ¨¡**ï¼šåŸºäºRCçƒ­åŠ›å­¦æ¨¡å‹ï¼ˆç”µé˜»-ç”µå®¹ç½‘ç»œï¼‰
- âœ… **çœŸå®å¤©æ°”æ•°æ®**ï¼šEPWæ ¼å¼æ°”è±¡æ–‡ä»¶ï¼ˆ8760å°æ—¶/å¹´ï¼‰
- âœ… **OpenAI Gymæ¥å£**ï¼šæ ‡å‡†RLç¯å¢ƒæ¥å£
- âœ… **å¯å®šåˆ¶å¥–åŠ±å‡½æ•°**ï¼šæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰å¥–åŠ±
- âœ… **MPCæ§åˆ¶å™¨**ï¼šå†…ç½®æ¨¡å‹é¢„æµ‹æ§åˆ¶åŸºçº¿

**GitHub**: https://github.com/chz056/BEAR

### 1.2 DROPT é¡¹ç›®ç®€ä»‹
**DROPT** æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå½“å‰åº”ç”¨äºæ•°æ®ä¸­å¿ƒç©ºè°ƒä¼˜åŒ–ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- âœ… **æ‰©æ•£æ¨¡å‹Actor**ï¼šä½¿ç”¨DDPMç”ŸæˆåŠ¨ä½œ
- âœ… **åŒQç½‘ç»œCritic**ï¼šå‡å°‘ä»·å€¼è¿‡ä¼°è®¡
- âœ… **è¡Œä¸ºå…‹éš†æ”¯æŒ**ï¼šå¯åˆ©ç”¨ä¸“å®¶æ•°æ®åŠ é€Ÿè®­ç»ƒ
- âœ… **Tianshouæ¡†æ¶**ï¼šé«˜æ•ˆçš„RLè®­ç»ƒæµç¨‹
- âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ˜“äºæ‰©å±•åˆ°æ–°ç¯å¢ƒ

---

## 2. BEAR ä»£ç åˆ†æ

### 2.1 æ ¸å¿ƒæ–‡ä»¶ç»“æ„
```
bear/BEAR/
â”œâ”€â”€ Env/
â”‚   â””â”€â”€ env_building.py          # å»ºç­‘ç¯å¢ƒä¸»ç±» (433è¡Œ)
â”œâ”€â”€ Controller/
â”‚   â””â”€â”€ MPC_Controller.py        # MPCæ§åˆ¶å™¨ (172è¡Œ)
â”œâ”€â”€ Utils/
â”‚   â””â”€â”€ utils_building.py        # å·¥å…·å‡½æ•° (830è¡Œ)
â”œâ”€â”€ Customize/
â”‚   â””â”€â”€ reward_functions.py      # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
â”œâ”€â”€ Data/                        # å»ºç­‘å’Œå¤©æ°”æ•°æ®
â”‚   â”œâ”€â”€ *.table.htm              # 16ç§å»ºç­‘çš„å‡ ä½•æ•°æ®
â”‚   â””â”€â”€ *.epw                    # 19ä¸ªåŸå¸‚çš„å¤©æ°”æ•°æ®
â””â”€â”€ examples/
    â””â”€â”€ quickstart.py            # å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
```

### 2.2 ç¯å¢ƒæ¥å£åˆ†æ

#### 2.2.1 åˆå§‹åŒ–å‚æ•°
```python
from BEAR.Utils.utils_building import ParameterGenerator
from BEAR.Env.env_building import BuildingEnvReal

# ç”Ÿæˆç¯å¢ƒå‚æ•°
Parameter = ParameterGenerator(
    Building='OfficeSmall',      # å»ºç­‘ç±»å‹
    Weather='Hot_Dry',           # æ°”å€™ç±»å‹
    Location='Tucson',           # åœ°ç†ä½ç½®
    max_power=8000,              # HVACæœ€å¤§åŠŸç‡ (W)
    time_reso=3600,              # æ—¶é—´åˆ†è¾¨ç‡ (ç§’)
    reward_gamma=(0.001, 0.999), # [èƒ½è€—æƒé‡, æ¸©åº¦æƒé‡]
    target=22,                   # ç›®æ ‡æ¸©åº¦ (Â°C)
    temp_range=(-40, 40),        # æ¸©åº¦èŒƒå›´
    spacetype='continuous'       # è¿ç»­åŠ¨ä½œç©ºé—´
)

# åˆ›å»ºç¯å¢ƒ
env = BuildingEnvReal(Parameter)
```

#### 2.2.2 çŠ¶æ€ç©ºé—´ (Observation Space)
```python
# çŠ¶æ€ç»´åº¦ï¼š3*roomnum + 3
# ç»„æˆï¼š
# - å„æˆ¿é—´æ¸©åº¦ (roomnum)
# - å®¤å¤–æ¸©åº¦ (1)
# - å…¨å±€æ°´å¹³è¾ç…§åº¦ GHI (roomnum)
# - åœ°é¢æ¸©åº¦ (1)
# - äººå‘˜çƒ­è´Ÿè· (roomnum)

# ç¤ºä¾‹ï¼š6ä¸ªæˆ¿é—´çš„åŠå…¬æ¥¼
# çŠ¶æ€ç»´åº¦ = 6 + 1 + 6 + 1 + 6 = 20
```

#### 2.2.3 åŠ¨ä½œç©ºé—´ (Action Space)
```python
# åŠ¨ä½œç»´åº¦ï¼šroomnum
# æ¯ä¸ªæˆ¿é—´çš„HVACåŠŸç‡ï¼š[-1, 1]
# - è´Ÿå€¼ï¼šåˆ¶å†· (cooling)
# - æ­£å€¼ï¼šåˆ¶çƒ­ (heating)
# - å½’ä¸€åŒ–åˆ° [-1, 1]ï¼Œå®é™…åŠŸç‡ = action * max_power

# ç¤ºä¾‹ï¼š6ä¸ªæˆ¿é—´
# åŠ¨ä½œç»´åº¦ = 6
# action = [-0.5, -0.3, 0.0, -0.2, -0.4, -0.1]
# å®é™…åŠŸç‡ = action * 8000W
```

#### 2.2.4 å¥–åŠ±å‡½æ•°
```python
# é»˜è®¤å¥–åŠ±å‡½æ•°
def default_reward_function(self, state, action, error, state_new):
    reward = 0
    # èƒ½è€—æƒ©ç½š
    reward -= LA.norm(action, 2) * self.q_rate
    # æ¸©åº¦åå·®æƒ©ç½š
    reward -= LA.norm(error, 2) * self.error_rate
    return reward

# error = (å½“å‰æ¸©åº¦ - ç›®æ ‡æ¸©åº¦) * AC_map
# AC_map: æ ‡è®°å“ªäº›æˆ¿é—´æœ‰ç©ºè°ƒ
```

#### 2.2.5 ç¯å¢ƒåŠ¨æ€
```python
# åŸºäºRCçƒ­åŠ›å­¦æ¨¡å‹
# çŠ¶æ€æ›´æ–°æ–¹ç¨‹ï¼š
# X_{t+1} = A_d @ X_t + B_d @ Y_t

# å…¶ä¸­ï¼š
# - A_d: ç¦»æ•£åŒ–ç³»ç»ŸçŸ©é˜µ (roomnum x roomnum)
# - B_d: è¾“å…¥çŸ©é˜µ (roomnum x (4+roomnum+1))
# - Y_t: è¾“å…¥å‘é‡ [äººå‘˜çƒ­è´Ÿè·, åœ°é¢æ¸©åº¦, å®¤å¤–æ¸©åº¦, HVACåŠŸç‡, GHI]
```

### 2.3 å…³é”®ç±»å’Œæ–¹æ³•

#### BuildingEnvReal ç±»
```python
class BuildingEnvReal(gym.Env):
    """å»ºç­‘ç¯å¢ƒç±»"""
    
    def __init__(self, Parameter: Dict[str, Any], 
                 user_reward_function=None,
                 reward_breakdown_keys=None):
        """åˆå§‹åŒ–ç¯å¢ƒ"""
        # è§£æå‚æ•°
        self.OutTemp = Parameter['OutTemp']      # å®¤å¤–æ¸©åº¦åºåˆ—
        self.roomnum = Parameter['roomnum']      # æˆ¿é—´æ•°é‡
        self.target = Parameter['target']        # ç›®æ ‡æ¸©åº¦
        self.gamma = Parameter['gamma']          # å¥–åŠ±æƒé‡
        self.ghi = Parameter['ghi']              # å¤ªé˜³è¾ç…§åº¦
        self.Occupancy = Parameter['Occupancy']  # äººå‘˜å ç”¨ç‡
        # ... æ›´å¤šå‚æ•°
        
        # å®šä¹‰åŠ¨ä½œå’ŒçŠ¶æ€ç©ºé—´
        self.action_space = gym.spaces.Box(...)
        self.observation_space = gym.spaces.Box(...)
        
        # è®¡ç®—ç³»ç»ŸçŸ©é˜µ
        self.A_d = expm(Amatrix * self.timestep)
        self.B_d = LA.inv(Amatrix) @ (self.A_d - I) @ Bmatrix
    
    def reset(self, *, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        self.epochs = 0
        T_initial = self.target  # åˆå§‹æ¸©åº¦
        # æ„é€ åˆå§‹çŠ¶æ€
        self.state = np.concatenate([
            T_initial,                    # æˆ¿é—´æ¸©åº¦
            self.OutTemp[0],              # å®¤å¤–æ¸©åº¦
            self.ghi[0],                  # GHI
            self.GroundTemp[0],           # åœ°é¢æ¸©åº¦
            self.Occupower/1000           # äººå‘˜çƒ­è´Ÿè·
        ])
        return self.state, {}
    
    def step(self, action: np.ndarray):
        """æ‰§è¡Œä¸€æ­¥"""
        # çŠ¶æ€æ›´æ–°
        X_new = self.A_d @ X + self.B_d @ Y
        
        # è®¡ç®—å¥–åŠ±
        error = X_new * self.acmap - self.target * self.acmap
        reward = self.reward_function(self.state, action, error, X_new)
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = (self.epochs >= len(self.OutTemp) - 1)
        
        return self.state, reward, done, done, info
```

#### ParameterGenerator å‡½æ•°
```python
def ParameterGenerator(
    Building: str,              # å»ºç­‘ç±»å‹æˆ–æ–‡ä»¶è·¯å¾„
    Weather: str,               # æ°”å€™ç±»å‹æˆ–EPWæ–‡ä»¶è·¯å¾„
    Location: str,              # åœ°ç†ä½ç½®
    U_Wall: List[float],        # å¢™ä½“çƒ­ä¼ å¯¼ç³»æ•°
    max_power: int = 8000,      # HVACæœ€å¤§åŠŸç‡
    time_reso: int = 3600,      # æ—¶é—´åˆ†è¾¨ç‡
    reward_gamma: Tuple = (0.001, 0.999),  # å¥–åŠ±æƒé‡
    target: float = 22,         # ç›®æ ‡æ¸©åº¦
    temp_range: Tuple = (-40, 40),  # æ¸©åº¦èŒƒå›´
    spacetype: str = 'continuous',  # åŠ¨ä½œç©ºé—´ç±»å‹
    root: str = 'BEAR/Data/'    # æ•°æ®æ ¹ç›®å½•
) -> Dict[str, Any]:
    """ç”Ÿæˆç¯å¢ƒå‚æ•°å­—å…¸"""
    
    # 1. è§£æå»ºç­‘ç±»å‹
    Building_dic = {
        'OfficeSmall': ('ASHRAE901_OfficeSmall_STD2019_Tucson.table.htm', [...]),
        'Hospital': (...),
        # ... 16ç§å»ºç­‘
    }
    
    # 2. è§£æå¤©æ°”ç±»å‹
    weather_dic = {
        'Hot_Dry': 'USA_AZ_Tucson-Davis-Monthan.AFB.722745_TMY3.epw',
        'Cold_Humid': 'USA_MN_Rochester.Intl.AP.726440_TMY3.epw',
        # ... 16ç§æ°”å€™
    }
    
    # 3. è¯»å–å»ºç­‘å‡ ä½•ä¿¡æ¯
    Layerall, roomnum, buildall = Getroominfor(filename)
    
    # 4. è¯»å–å¤©æ°”æ•°æ®
    data = pvlib.iotools.read_epw(weatherfile)
    outtempdatanew = interpolate_temperature(data, time_reso)
    solardatanew = interpolate_ghi(data, time_reso)
    
    # 5. è®¡ç®—RCç½‘ç»œå‚æ•°
    dicRoom, Rtable, Ctable, Windowtable = Nfind_neighbor(...)
    
    # 6. è¿”å›å‚æ•°å­—å…¸
    return {
        'OutTemp': outtempdatanew,
        'roomnum': roomnum,
        'connectmap': connectmap,
        'RCtable': RCtable,
        'target': target,
        'gamma': reward_gamma,
        'ghi': solardatanew,
        'GroundTemp': groundtemp,
        'Occupancy': occupancy_schedule,
        'ACmap': AC_map,
        'max_power': max_power,
        'nonlinear': nonlinear_term,
        'temp_range': temp_range,
        'spacetype': spacetype,
        'time_resolution': time_reso
    }
```

---

## 3. DROPT ç¯å¢ƒæ¥å£åˆ†æ

### 3.1 å½“å‰ç¯å¢ƒç»“æ„ (DataCenterEnv)

```python
class DataCenterEnv(gym.Env):
    """æ•°æ®ä¸­å¿ƒç©ºè°ƒä¼˜åŒ–ç¯å¢ƒ"""
    
    def __init__(
        self,
        num_crac_units: int = 4,
        target_temp: float = 24.0,
        temp_tolerance: float = 2.0,
        time_step: float = 5.0,
        episode_length: int = 288,
        energy_weight: float = 1.0,
        temp_weight: float = 10.0,
        violation_penalty: float = 100.0,
        use_real_weather: bool = False,
        weather_file: str = None,
        workload_file: str = None,
    ):
        # çŠ¶æ€ç©ºé—´ï¼š[T_in, T_out, H_in, IT_load, T_supply_1, ..., T_supply_n, reward_last]
        self.state_dim = 4 + num_crac_units + 1
        
        # åŠ¨ä½œç©ºé—´ï¼š[T_set_1, fan_speed_1, ..., T_set_n, fan_speed_n]
        self.action_dim = num_crac_units * 2
        
        # å­æ¨¡å—
        self.thermal_model = ThermalModel(...)
        self.expert_controller = ExpertController(...)
    
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        # åˆå§‹åŒ–ç‰©ç†çŠ¶æ€
        self.T_in = self.target_temp + random()
        self.T_out = random_outdoor_temp()
        # ...
        return self._get_state()
    
    def step(self, action: np.ndarray):
        """æ‰§è¡Œä¸€æ­¥"""
        # åŠ¨ä½œåå½’ä¸€åŒ–
        T_set, fan_speed = self._denormalize_action(action)
        
        # è·å–ä¸“å®¶åŠ¨ä½œ
        expert_action = self.expert_controller.get_action(...)
        
        # æ›´æ–°ç¯å¢ƒåŠ¨æ€
        next_T_in, next_H_in, next_T_supply, energy = self.thermal_model.step(...)
        
        # è®¡ç®—å¥–åŠ±
        reward, info = self._compute_reward(...)
        
        return next_state, reward, done, info
```

### 3.2 ç¯å¢ƒåˆ›å»ºæ¥å£

```python
def make_datacenter_env(training_num: int = 1, test_num: int = 1, **kwargs):
    """åˆ›å»ºæ•°æ®ä¸­å¿ƒç¯å¢ƒ"""
    from tianshou.env import DummyVectorEnv
    
    env = DataCenterEnv(**kwargs)
    
    train_envs = DummyVectorEnv([
        lambda: DataCenterEnv(**kwargs) for _ in range(training_num)
    ])
    
    test_envs = DummyVectorEnv([
        lambda: DataCenterEnv(**kwargs) for _ in range(test_num)
    ])
    
    return env, train_envs, test_envs
```

---

## 4. é›†æˆæ¶æ„è®¾è®¡

### 4.1 è®¾è®¡ç›®æ ‡

1. **æœ€å°ä¾µå…¥æ€§**ï¼šä¸ä¿®æ”¹BEARåŸå§‹ä»£ç 
2. **æ¥å£å…¼å®¹æ€§**ï¼šç¬¦åˆDROPTçš„ç¯å¢ƒæ¥å£è§„èŒƒ
3. **åŠŸèƒ½å®Œæ•´æ€§**ï¼šä¿ç•™BEARçš„æ‰€æœ‰ç‰¹æ€§
4. **æ˜“ç”¨æ€§**ï¼šç®€åŒ–ç¯å¢ƒåˆ›å»ºæµç¨‹
5. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒè‡ªå®šä¹‰é…ç½®

### 4.2 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DROPT Training Pipeline                   â”‚
â”‚                  (main_building.py)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BearEnvWrapper (é€‚é…å™¨å±‚)                       â”‚
â”‚  - çŠ¶æ€ç©ºé—´æ˜ å°„                                              â”‚
â”‚  - åŠ¨ä½œç©ºé—´æ˜ å°„                                              â”‚
â”‚  - å¥–åŠ±å‡½æ•°é€‚é…                                              â”‚
â”‚  - ä¸“å®¶æ§åˆ¶å™¨é›†æˆ                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BuildingEnvReal (BEARåŸå§‹ç¯å¢ƒ)                  â”‚
â”‚  - RCçƒ­åŠ›å­¦æ¨¡å‹                                              â”‚
â”‚  - çœŸå®å¤©æ°”æ•°æ®                                              â”‚
â”‚  - å»ºç­‘å‡ ä½•ä¿¡æ¯                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 4.3.1 BearEnvWrapper (é€‚é…å™¨ç±»)
```python
class BearEnvWrapper(gym.Env):
    """BEARç¯å¢ƒé€‚é…å™¨ï¼Œä½¿å…¶å…¼å®¹DROPTæ¥å£"""
    
    def __init__(
        self,
        building_type: str = 'OfficeSmall',
        weather_type: str = 'Hot_Dry',
        location: str = 'Tucson',
        target_temp: float = 22.0,
        temp_tolerance: float = 2.0,
        max_power: int = 8000,
        time_resolution: int = 3600,
        energy_weight: float = 0.001,
        temp_weight: float = 0.999,
        episode_length: int = None,
        expert_type: str = 'mpc',
        **kwargs
    ):
        """åˆå§‹åŒ–é€‚é…å™¨"""
        # 1. ç”ŸæˆBEARå‚æ•°
        self.bear_params = ParameterGenerator(
            Building=building_type,
            Weather=weather_type,
            Location=location,
            target=target_temp,
            reward_gamma=(energy_weight, temp_weight),
            max_power=max_power,
            time_reso=time_resolution,
            **kwargs
        )
        
        # 2. åˆ›å»ºBEARç¯å¢ƒ
        self.bear_env = BuildingEnvReal(self.bear_params)
        
        # 3. é€‚é…çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
        self.observation_space = self._adapt_observation_space()
        self.action_space = self._adapt_action_space()
        
        # 4. åˆ›å»ºä¸“å®¶æ§åˆ¶å™¨
        self.expert_controller = self._create_expert_controller(expert_type)
        
        # 5. è®¾ç½®å›åˆé•¿åº¦
        self.episode_length = episode_length or len(self.bear_params['OutTemp'])
        self.current_step = 0
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        state, info = self.bear_env.reset()
        self.current_step = 0
        return self._adapt_state(state), info
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥"""
        # 1. é€‚é…åŠ¨ä½œ
        bear_action = self._adapt_action(action)
        
        # 2. æ‰§è¡ŒBEARç¯å¢ƒ
        state, reward, done, truncated, info = self.bear_env.step(bear_action)
        
        # 3. è·å–ä¸“å®¶åŠ¨ä½œ
        expert_action = self.expert_controller.get_action(state, self.bear_env)
        info['expert_action'] = expert_action
        
        # 4. é€‚é…çŠ¶æ€å’Œå¥–åŠ±
        adapted_state = self._adapt_state(state)
        adapted_reward = self._adapt_reward(reward, state, info)
        
        # 5. æ£€æŸ¥å›åˆç»“æŸ
        self.current_step += 1
        if self.episode_length and self.current_step >= self.episode_length:
            done = True
        
        return adapted_state, adapted_reward, done, info
```

#### 4.3.2 çŠ¶æ€ç©ºé—´æ˜ å°„
```python
def _adapt_observation_space(self):
    """é€‚é…çŠ¶æ€ç©ºé—´"""
    # BEARçŠ¶æ€ï¼š[æˆ¿é—´æ¸©åº¦(n), å®¤å¤–æ¸©åº¦(1), GHI(n), åœ°é¢æ¸©åº¦(1), äººå‘˜çƒ­è´Ÿè·(n)]
    # DROPTæœŸæœ›ï¼šæ ‡å‡†åŒ–çš„Boxç©ºé—´
    
    roomnum = self.bear_params['roomnum']
    state_dim = 3 * roomnum + 3
    
    # å®šä¹‰çŠ¶æ€èŒƒå›´
    temp_min, temp_max = self.bear_params['temp_range']
    
    low = np.array([temp_min] * (roomnum + 1) +  # æ¸©åº¦
                   [0] * roomnum +                 # GHI
                   [temp_min] +                    # åœ°é¢æ¸©åº¦
                   [0] * roomnum)                  # äººå‘˜çƒ­è´Ÿè·
    
    high = np.array([temp_max] * (roomnum + 1) +
                    [1000] * roomnum +
                    [temp_max] +
                    [1000] * roomnum)
    
    return gym.spaces.Box(low=low, high=high, dtype=np.float32)

def _adapt_state(self, bear_state):
    """é€‚é…çŠ¶æ€å‘é‡"""
    # BEARçŠ¶æ€å·²ç»æ˜¯æ­£ç¡®æ ¼å¼ï¼Œå¯èƒ½éœ€è¦å½’ä¸€åŒ–
    return bear_state.astype(np.float32)
```

#### 4.3.3 åŠ¨ä½œç©ºé—´æ˜ å°„
```python
def _adapt_action_space(self):
    """é€‚é…åŠ¨ä½œç©ºé—´"""
    # BEARåŠ¨ä½œï¼šæ¯ä¸ªæˆ¿é—´çš„HVACåŠŸç‡ [-1, 1]
    # DROPTæœŸæœ›ï¼šå½’ä¸€åŒ–çš„Boxç©ºé—´ [-1, 1]

    roomnum = self.bear_params['roomnum']

    return gym.spaces.Box(
        low=-1.0,
        high=1.0,
        shape=(roomnum,),
        dtype=np.float32
    )

def _adapt_action(self, dropt_action):
    """é€‚é…åŠ¨ä½œå‘é‡"""
    # DROPTåŠ¨ä½œå·²ç»æ˜¯[-1, 1]ï¼Œç›´æ¥ä¼ é€’ç»™BEAR
    return dropt_action
```

#### 4.3.4 å¥–åŠ±å‡½æ•°é€‚é…
```python
def _adapt_reward(self, bear_reward, state, info):
    """é€‚é…å¥–åŠ±å‡½æ•°"""
    # BEARå¥–åŠ±ï¼š-èƒ½è€—æƒ©ç½š - æ¸©åº¦åå·®æƒ©ç½š
    # å¯ä»¥ä¿æŒä¸å˜ï¼Œæˆ–æ·»åŠ é¢å¤–çš„æƒ©ç½šé¡¹

    # é€‰é¡¹1ï¼šç›´æ¥ä½¿ç”¨BEARå¥–åŠ±
    return bear_reward

    # é€‰é¡¹2ï¼šæ·»åŠ æ¸©åº¦è¶Šç•Œæƒ©ç½šï¼ˆç±»ä¼¼DataCenterEnvï¼‰
    zone_temps = info['zone_temperature']
    target = self.bear_params['target']
    tolerance = self.temp_tolerance

    violation_penalty = 0.0
    for temp in zone_temps:
        if temp < target - tolerance or temp > target + tolerance:
            violation_penalty += 100.0

    return bear_reward - violation_penalty
```

#### 4.3.5 ä¸“å®¶æ§åˆ¶å™¨é›†æˆ
```python
def _create_expert_controller(self, expert_type):
    """åˆ›å»ºä¸“å®¶æ§åˆ¶å™¨"""
    if expert_type == 'mpc':
        # ä½¿ç”¨BEARå†…ç½®çš„MPCæ§åˆ¶å™¨
        from bear.BEAR.Controller.MPC_Controller import MPCAgent
        return BearMPCWrapper(self.bear_env, self.bear_params)

    elif expert_type == 'rule_based':
        # åˆ›å»ºåŸºäºè§„åˆ™çš„æ§åˆ¶å™¨
        return BearRuleBasedController(self.bear_params)

    elif expert_type == 'pid':
        # åˆ›å»ºPIDæ§åˆ¶å™¨ï¼ˆéœ€è¦å®ç°ï¼‰
        return BearPIDController(self.bear_params)

    else:
        raise ValueError(f"Unknown expert type: {expert_type}")

class BearMPCWrapper:
    """BEAR MPCæ§åˆ¶å™¨åŒ…è£…å™¨"""

    def __init__(self, bear_env, bear_params):
        from bear.BEAR.Controller.MPC_Controller import MPCAgent
        self.mpc = MPCAgent(
            environment=bear_env,
            gamma=bear_params['gamma'],
            planning_steps=1
        )

    def get_action(self, state, env):
        """è·å–ä¸“å®¶åŠ¨ä½œ"""
        action, _ = self.mpc.predict(env)
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        return action
```

### 4.4 ç¯å¢ƒåˆ›å»ºæ¥å£

```python
def make_building_env(
    building_type: str = 'OfficeSmall',
    weather_type: str = 'Hot_Dry',
    location: str = 'Tucson',
    training_num: int = 1,
    test_num: int = 1,
    **kwargs
):
    """åˆ›å»ºå»ºç­‘ç¯å¢ƒï¼ˆå…¼å®¹DROPTæ¥å£ï¼‰"""
    from tianshou.env import DummyVectorEnv

    # åˆ›å»ºå•ä¸ªç¯å¢ƒå®ä¾‹
    env = BearEnvWrapper(
        building_type=building_type,
        weather_type=weather_type,
        location=location,
        **kwargs
    )

    # åˆ›å»ºè®­ç»ƒç¯å¢ƒå‘é‡
    train_envs = DummyVectorEnv([
        lambda: BearEnvWrapper(
            building_type=building_type,
            weather_type=weather_type,
            location=location,
            **kwargs
        ) for _ in range(training_num)
    ])

    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒå‘é‡
    test_envs = DummyVectorEnv([
        lambda: BearEnvWrapper(
            building_type=building_type,
            weather_type=weather_type,
            location=location,
            **kwargs
        ) for _ in range(test_num)
    ])

    return env, train_envs, test_envs
```

---

## 5. å®ç°æ­¥éª¤

### 5.1 æ–‡ä»¶åˆ›å»ºæ¸…å•

éœ€è¦åˆ›å»ºä»¥ä¸‹æ–°æ–‡ä»¶ï¼š

1. **`env/building_env_wrapper.py`** (çº¦400è¡Œ)
   - `BearEnvWrapper` ç±»
   - çŠ¶æ€/åŠ¨ä½œ/å¥–åŠ±é€‚é…æ–¹æ³•
   - ä¸“å®¶æ§åˆ¶å™¨åŒ…è£…å™¨

2. **`env/building_expert_controller.py`** (çº¦300è¡Œ)
   - `BearMPCWrapper` ç±»
   - `BearPIDController` ç±»
   - `BearRuleBasedController` ç±»

3. **`env/building_config.py`** (çº¦200è¡Œ)
   - é¢„å®šä¹‰å»ºç­‘é…ç½®
   - è®­ç»ƒè¶…å‚æ•°æ¨è

4. **`main_building.py`** (çº¦300è¡Œ)
   - å»ºç­‘ç¯å¢ƒè®­ç»ƒä¸»ç¨‹åº
   - å‚æ•°è§£æ
   - è®­ç»ƒæµç¨‹

5. **`scripts/test_building_env.py`** (çº¦200è¡Œ)
   - ç¯å¢ƒæµ‹è¯•è„šæœ¬
   - åŠŸèƒ½éªŒè¯

6. **`docs/BEAR_INTEGRATION_GUIDE.md`** (çº¦150è¡Œ)
   - ä½¿ç”¨æŒ‡å—
   - ç¤ºä¾‹ä»£ç 

### 5.2 ä¿®æ”¹ç°æœ‰æ–‡ä»¶

éœ€è¦ä¿®æ”¹ä»¥ä¸‹æ–‡ä»¶ï¼š

1. **`env/__init__.py`**
   - æ·»åŠ  `from .building_env_wrapper import BearEnvWrapper, make_building_env`

2. **`requirements.txt`** æˆ–åˆ›å»º **`bear_requirements.txt`**
   - æ·»åŠ BEARä¾èµ–ï¼š`pvlib`, `cvxpy`, `scikit-learn`

### 5.3 è¯¦ç»†å®ç°æ­¥éª¤

#### æ­¥éª¤1ï¼šå®‰è£…BEARä¾èµ–
```bash
# å®‰è£…BEARæ‰€éœ€çš„é¢å¤–ä¾èµ–
pip install pvlib scikit-learn cvxpy
```

#### æ­¥éª¤2ï¼šåˆ›å»ºé€‚é…å™¨ç±»
åˆ›å»º `env/building_env_wrapper.py`ï¼Œå®ç°ï¼š
- `BearEnvWrapper` ä¸»ç±»
- çŠ¶æ€ç©ºé—´é€‚é…
- åŠ¨ä½œç©ºé—´é€‚é…
- å¥–åŠ±å‡½æ•°é€‚é…
- ä¸“å®¶æ§åˆ¶å™¨æ¥å£

#### æ­¥éª¤3ï¼šåˆ›å»ºä¸“å®¶æ§åˆ¶å™¨
åˆ›å»º `env/building_expert_controller.py`ï¼Œå®ç°ï¼š
- MPCæ§åˆ¶å™¨åŒ…è£…å™¨
- PIDæ§åˆ¶å™¨ï¼ˆå‚è€ƒ `expert_controller.py`ï¼‰
- åŸºäºè§„åˆ™çš„æ§åˆ¶å™¨

#### æ­¥éª¤4ï¼šåˆ›å»ºé…ç½®æ–‡ä»¶
åˆ›å»º `env/building_config.py`ï¼Œå®šä¹‰ï¼š
- å¸¸ç”¨å»ºç­‘ç±»å‹é…ç½®
- è®­ç»ƒè¶…å‚æ•°æ¨è
- ç¯å¢ƒå‚æ•°æ¨¡æ¿

#### æ­¥éª¤5ï¼šåˆ›å»ºè®­ç»ƒè„šæœ¬
åˆ›å»º `main_building.py`ï¼Œå®ç°ï¼š
- å‚æ•°è§£æï¼ˆæ‰©å±•è‡ª `main_datacenter.py`ï¼‰
- ç¯å¢ƒåˆ›å»º
- ç½‘ç»œåˆå§‹åŒ–
- è®­ç»ƒå¾ªç¯

#### æ­¥éª¤6ï¼šæµ‹è¯•å’ŒéªŒè¯
åˆ›å»º `scripts/test_building_env.py`ï¼Œæµ‹è¯•ï¼š
- ç¯å¢ƒåˆ›å»º
- çŠ¶æ€/åŠ¨ä½œç©ºé—´
- ä¸“å®¶æ§åˆ¶å™¨
- è®­ç»ƒæµç¨‹

---

## 6. ä½¿ç”¨ç¤ºä¾‹

### 6.1 åŸºæœ¬ä½¿ç”¨

```python
from env.building_env_wrapper import make_building_env

# åˆ›å»ºå°å‹åŠå…¬æ¥¼ç¯å¢ƒ
env, train_envs, test_envs = make_building_env(
    building_type='OfficeSmall',
    weather_type='Hot_Dry',
    location='Tucson',
    target_temp=22.0,
    temp_tolerance=2.0,
    max_power=8000,
    time_resolution=3600,  # 1å°æ—¶
    energy_weight=0.001,
    temp_weight=0.999,
    training_num=4,
    test_num=2
)

# æµ‹è¯•ç¯å¢ƒ
state, info = env.reset()
print(f"çŠ¶æ€ç»´åº¦: {state.shape}")
print(f"åŠ¨ä½œç»´åº¦: {env.action_space.shape}")

for step in range(10):
    action = env.action_space.sample()
    next_state, reward, done, info = env.step(action)
    print(f"Step {step}: Reward={reward:.2f}, Done={done}")
    if done:
        break
```

### 6.2 è®­ç»ƒç¤ºä¾‹

```bash
# å¿«é€Ÿè®­ç»ƒï¼ˆè¡Œä¸ºå…‹éš†æ¨¡å¼ï¼‰
python main_building.py \
    --building-type OfficeSmall \
    --weather-type Hot_Dry \
    --location Tucson \
    --bc-coef \
    --expert-type mpc \
    --epoch 50000 \
    --batch-size 256 \
    --n-timesteps 5 \
    --device cuda:0

# é«˜æ€§èƒ½è®­ç»ƒï¼ˆç­–ç•¥æ¢¯åº¦æ¨¡å¼ï¼‰
python main_building.py \
    --building-type Hospital \
    --weather-type Cold_Humid \
    --location Rochester \
    --epoch 200000 \
    --batch-size 512 \
    --n-timesteps 8 \
    --gamma 0.99 \
    --device cuda:0
```

### 6.3 å¤šå»ºç­‘ç±»å‹å¯¹æ¯”

```python
# æµ‹è¯•ä¸åŒå»ºç­‘ç±»å‹
building_types = ['OfficeSmall', 'Hospital', 'SchoolPrimary', 'Warehouse']

for building in building_types:
    env, _, _ = make_building_env(
        building_type=building,
        weather_type='Hot_Dry',
        location='Tucson'
    )

    print(f"\n{building}:")
    print(f"  æˆ¿é—´æ•°: {env.bear_params['roomnum']}")
    print(f"  çŠ¶æ€ç»´åº¦: {env.observation_space.shape}")
    print(f"  åŠ¨ä½œç»´åº¦: {env.action_space.shape}")
```

### 6.4 è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

```python
def custom_reward_function(self, state, action, error, state_new):
    """è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°"""
    reward = 0

    # èƒ½è€—æƒ©ç½š
    energy_penalty = LA.norm(action, 2) * self.q_rate
    reward -= energy_penalty

    # æ¸©åº¦åå·®æƒ©ç½š
    temp_penalty = LA.norm(error, 2) * self.error_rate
    reward -= temp_penalty

    # èˆ’é€‚åº¦å¥–åŠ±ï¼ˆæ¸©åº¦åœ¨ç›®æ ‡èŒƒå›´å†…ï¼‰
    comfort_bonus = 0
    for temp in state_new:
        if 20 <= temp <= 24:
            comfort_bonus += 1.0
    reward += comfort_bonus

    # è®°å½•å¥–åŠ±åˆ†è§£
    self._reward_breakdown['energy'] = -energy_penalty
    self._reward_breakdown['temperature'] = -temp_penalty
    self._reward_breakdown['comfort'] = comfort_bonus

    return reward

# ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
from bear.BEAR.Env.env_building import BuildingEnvReal
from bear.BEAR.Utils.utils_building import ParameterGenerator

Parameter = ParameterGenerator('OfficeSmall', 'Hot_Dry', 'Tucson')
env = BuildingEnvReal(
    Parameter,
    user_reward_function=custom_reward_function,
    reward_breakdown_keys=['energy', 'temperature', 'comfort']
)
```

---

## 7. å…³é”®æŠ€æœ¯ç»†èŠ‚

### 7.1 çŠ¶æ€ç©ºé—´è®¾è®¡

**BEARåŸå§‹çŠ¶æ€**ï¼š
- æˆ¿é—´æ¸©åº¦ (nç»´)
- å®¤å¤–æ¸©åº¦ (1ç»´)
- å…¨å±€æ°´å¹³è¾ç…§åº¦ GHI (nç»´)
- åœ°é¢æ¸©åº¦ (1ç»´)
- äººå‘˜çƒ­è´Ÿè· (nç»´)

**é€‚é…ç­–ç•¥**ï¼š
- ä¿æŒåŸå§‹çŠ¶æ€ç»“æ„
- æ·»åŠ å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
- æ·»åŠ å†å²ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰

### 7.2 åŠ¨ä½œç©ºé—´è®¾è®¡

**BEARåŸå§‹åŠ¨ä½œ**ï¼š
- æ¯ä¸ªæˆ¿é—´çš„HVACåŠŸç‡ï¼š[-1, 1]
- è´Ÿå€¼=åˆ¶å†·ï¼Œæ­£å€¼=åˆ¶çƒ­

**é€‚é…ç­–ç•¥**ï¼š
- ç›´æ¥ä½¿ç”¨BEARçš„åŠ¨ä½œç©ºé—´
- ä¸DROPTçš„å½’ä¸€åŒ–åŠ¨ä½œç©ºé—´å®Œå…¨å…¼å®¹

### 7.3 å¥–åŠ±å‡½æ•°è®¾è®¡

**BEARé»˜è®¤å¥–åŠ±**ï¼š
```
reward = -Î± * ||action||â‚‚ - Î² * ||error||â‚‚
```

**å¯é€‰å¢å¼º**ï¼š
1. æ·»åŠ æ¸©åº¦è¶Šç•Œæƒ©ç½š
2. æ·»åŠ èˆ’é€‚åº¦å¥–åŠ±
3. æ·»åŠ èƒ½æ•ˆæ¯”å¥–åŠ±
4. æ·»åŠ å³°å€¼åŠŸç‡æƒ©ç½š

### 7.4 ä¸“å®¶æ§åˆ¶å™¨è®¾è®¡

**MPCæ§åˆ¶å™¨**ï¼š
- ä½¿ç”¨BEARå†…ç½®çš„ `MPCAgent`
- åŸºäºå‡¸ä¼˜åŒ–æ±‚è§£æœ€ä¼˜æ§åˆ¶åºåˆ—
- éœ€è¦ `cvxpy` åº“

**PIDæ§åˆ¶å™¨**ï¼š
- å‚è€ƒ `env/expert_controller.py` ä¸­çš„å®ç°
- ä¸ºæ¯ä¸ªæˆ¿é—´ç‹¬ç«‹è®¾è®¡PIDæ§åˆ¶å™¨
- è€ƒè™‘æˆ¿é—´é—´çš„è€¦åˆ

**åŸºäºè§„åˆ™çš„æ§åˆ¶å™¨**ï¼š
- ç®€å•çš„if-elseè§„åˆ™
- é€‚åˆä½œä¸ºbaseline

---

## 8. é¢„æœŸæ•ˆæœ

### 8.1 ç¯å¢ƒç‰¹æ€§

| ç‰¹æ€§ | BEARç¯å¢ƒ | DataCenterç¯å¢ƒ |
|------|----------|----------------|
| çŠ¶æ€ç»´åº¦ | 3n+3 (n=æˆ¿é—´æ•°) | 4+m+1 (m=CRACæ•°) |
| åŠ¨ä½œç»´åº¦ | n | 2m |
| æ—¶é—´åˆ†è¾¨ç‡ | å¯é…ç½® (é»˜è®¤1å°æ—¶) | 5åˆ†é’Ÿ |
| å›åˆé•¿åº¦ | 8760æ­¥ (1å¹´) | 288æ­¥ (24å°æ—¶) |
| ç‰©ç†æ¨¡å‹ | RCçƒ­åŠ›å­¦æ¨¡å‹ | ç®€åŒ–çƒ­åŠ›å­¦æ¨¡å‹ |
| çœŸå®æ•°æ® | EPWå¤©æ°”æ–‡ä»¶ | å¯é€‰ |

### 8.2 è®­ç»ƒæ€§èƒ½

**é¢„æœŸè®­ç»ƒæ—¶é—´**ï¼ˆGPUï¼‰ï¼š
- å¿«é€Ÿæ¼”ç¤ºï¼ˆBCæ¨¡å¼ï¼Œ1000 epochsï¼‰ï¼š~10åˆ†é’Ÿ
- æ ‡å‡†è®­ç»ƒï¼ˆBCæ¨¡å¼ï¼Œ50000 epochsï¼‰ï¼š~2å°æ—¶
- é«˜æ€§èƒ½è®­ç»ƒï¼ˆPGæ¨¡å¼ï¼Œ200000 epochsï¼‰ï¼š~8å°æ—¶

**é¢„æœŸæ€§èƒ½æå‡**ï¼š
- ç›¸æ¯”éšæœºç­–ç•¥ï¼šèŠ‚èƒ½ 30-50%
- ç›¸æ¯”MPCåŸºçº¿ï¼šèŠ‚èƒ½ 5-15%
- æ¸©åº¦æ§åˆ¶ç²¾åº¦ï¼šÂ±0.5Â°C

### 8.3 åº”ç”¨åœºæ™¯

1. **åŠå…¬æ¥¼èƒ½æºç®¡ç†**
   - å»ºç­‘ç±»å‹ï¼šOfficeSmall/Medium/Large
   - ä¼˜åŒ–ç›®æ ‡ï¼šèŠ‚èƒ½ + èˆ’é€‚åº¦

2. **åŒ»é™¢æ¸©åº¦æ§åˆ¶**
   - å»ºç­‘ç±»å‹ï¼šHospital
   - ä¼˜åŒ–ç›®æ ‡ï¼šç²¾ç¡®æ¸©åº¦æ§åˆ¶

3. **å­¦æ ¡HVACè°ƒåº¦**
   - å»ºç­‘ç±»å‹ï¼šSchoolPrimary/Secondary
   - ä¼˜åŒ–ç›®æ ‡ï¼šè€ƒè™‘å ç”¨ç‡çš„åŠ¨æ€è°ƒåº¦

4. **ä»“åº“æ¸©åº¦ç®¡ç†**
   - å»ºç­‘ç±»å‹ï¼šWarehouse
   - ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å°åŒ–èƒ½è€—

---

## 9. åç»­æ‰©å±•

### 9.1 çŸ­æœŸæ‰©å±•ï¼ˆ1-2å‘¨ï¼‰

1. **å¤šç›®æ ‡ä¼˜åŒ–**
   - èƒ½è€— vs èˆ’é€‚åº¦çš„Paretoå‰æ²¿
   - å¯è§†åŒ–æƒè¡¡æ›²çº¿

2. **è¿ç§»å­¦ä¹ **
   - åœ¨ä¸€ä¸ªå»ºç­‘ä¸Šè®­ç»ƒï¼Œè¿ç§»åˆ°å¦ä¸€ä¸ªå»ºç­‘
   - è·¨æ°”å€™åŒºåŸŸçš„è¿ç§»

3. **é²æ£’æ€§å¢å¼º**
   - æ·»åŠ æ¨¡å‹ä¸ç¡®å®šæ€§
   - æ·»åŠ ä¼ æ„Ÿå™¨å™ªå£°

### 9.2 ä¸­æœŸæ‰©å±•ï¼ˆ1-2æœˆï¼‰

1. **æ•°æ®é©±åŠ¨å»ºæ¨¡**
   - ä½¿ç”¨BEARçš„ `train()` æ–¹æ³•
   - ä»çœŸå®æ•°æ®å­¦ä¹ ç³»ç»ŸçŸ©é˜µ

2. **åˆ†å¸ƒå¼æ§åˆ¶**
   - å¤šå»ºç­‘ååŒä¼˜åŒ–
   - åŒºåŸŸèƒ½æºç®¡ç†

3. **å®æ—¶æ§åˆ¶**
   - é™ä½æ—¶é—´åˆ†è¾¨ç‡ï¼ˆ5åˆ†é’Ÿï¼‰
   - åœ¨çº¿å­¦ä¹ å’Œé€‚åº”

### 9.3 é•¿æœŸæ‰©å±•ï¼ˆ3-6æœˆï¼‰

1. **çœŸå®å»ºç­‘éƒ¨ç½²**
   - ä¸BMSç³»ç»Ÿé›†æˆ
   - å®é™…å»ºç­‘æµ‹è¯•

2. **ç»æµä¼˜åŒ–**
   - è€ƒè™‘ç”µä»·
   - éœ€æ±‚å“åº”

3. **å¯å†ç”Ÿèƒ½æºé›†æˆ**
   - å¤ªé˜³èƒ½å‘ç”µ
   - å‚¨èƒ½ç³»ç»Ÿ

---

## 10. æ€»ç»“

### 10.1 é›†æˆä¼˜åŠ¿

âœ… **ä¸°å¯Œçš„å»ºç­‘ç±»å‹**ï¼š16ç§å»ºç­‘ Ã— 19ä¸ªåœ°ç†ä½ç½® = 304ç§ç»„åˆ
âœ… **çœŸå®ç‰©ç†æ¨¡å‹**ï¼šåŸºäºRCç½‘ç»œçš„çƒ­åŠ›å­¦æ¨¡æ‹Ÿ
âœ… **çœŸå®å¤©æ°”æ•°æ®**ï¼šEPWæ ¼å¼ï¼Œ8760å°æ—¶/å¹´
âœ… **æˆç†Ÿçš„åŸºçº¿**ï¼šå†…ç½®MPCæ§åˆ¶å™¨
âœ… **æœ€å°ä¾µå…¥æ€§**ï¼šé€šè¿‡é€‚é…å™¨å±‚é›†æˆï¼Œä¸ä¿®æ”¹åŸå§‹ä»£ç 
âœ… **å®Œå…¨å…¼å®¹**ï¼šç¬¦åˆDROPTçš„ç¯å¢ƒæ¥å£è§„èŒƒ

### 10.2 å®æ–½å»ºè®®

1. **å…ˆå®ç°åŸºç¡€åŠŸèƒ½**ï¼š
   - åˆ›å»º `BearEnvWrapper` ç±»
   - å®ç°çŠ¶æ€/åŠ¨ä½œ/å¥–åŠ±é€‚é…
   - æµ‹è¯•åŸºæœ¬åŠŸèƒ½

2. **å†æ·»åŠ ä¸“å®¶æ§åˆ¶å™¨**ï¼š
   - åŒ…è£…BEARçš„MPCæ§åˆ¶å™¨
   - å®ç°PIDæ§åˆ¶å™¨
   - æµ‹è¯•è¡Œä¸ºå…‹éš†è®­ç»ƒ

3. **æœ€åä¼˜åŒ–å’Œæ‰©å±•**ï¼š
   - æ€§èƒ½è°ƒä¼˜
   - æ·»åŠ å¯è§†åŒ–
   - ç¼–å†™æ–‡æ¡£

### 10.3 é¢„æœŸæˆæœ

å®Œæˆé›†æˆåï¼Œä½ å°†æ‹¥æœ‰ï¼š
- âœ… ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„å»ºç­‘ç¯å¢ƒæ¨¡æ‹Ÿå™¨
- âœ… æ”¯æŒ16ç§å»ºç­‘ç±»å‹å’Œ19ä¸ªåœ°ç†ä½ç½®
- âœ… ä¸DROPTæ¡†æ¶æ— ç¼é›†æˆ
- âœ… æ”¯æŒè¡Œä¸ºå…‹éš†å’Œç­–ç•¥æ¢¯åº¦è®­ç»ƒ
- âœ… å†…ç½®ä¸“å®¶æ§åˆ¶å™¨ï¼ˆMPC/PID/è§„åˆ™ï¼‰
- âœ… å®Œæ•´çš„æµ‹è¯•å’Œæ–‡æ¡£

---

## é™„å½•

### A. BEARå»ºç­‘ç±»å‹åˆ—è¡¨

| å»ºç­‘ç±»å‹ | æè¿° | å…¸å‹æˆ¿é—´æ•° |
|---------|------|-----------|
| ApartmentHighRise | é«˜å±‚å…¬å¯“ | 20-50 |
| ApartmentMidRise | ä¸­å±‚å…¬å¯“ | 10-30 |
| Hospital | åŒ»é™¢ | 30-80 |
| HotelLarge | å¤§å‹é…’åº— | 40-100 |
| HotelSmall | å°å‹é…’åº— | 10-30 |
| OfficeLarge | å¤§å‹åŠå…¬æ¥¼ | 30-80 |
| OfficeMedium | ä¸­å‹åŠå…¬æ¥¼ | 10-30 |
| OfficeSmall | å°å‹åŠå…¬æ¥¼ | 5-15 |
| OutPatientHealthCare | é—¨è¯ŠåŒ»ç–— | 10-30 |
| RestaurantFastFood | å¿«é¤åº— | 3-8 |
| RestaurantSitDown | æ­£é¤é¤å… | 5-15 |
| RetailStandalone | ç‹¬ç«‹é›¶å”®åº— | 5-15 |
| RetailStripmall | è´­ç‰©ä¸­å¿ƒ | 10-30 |
| SchoolPrimary | å°å­¦ | 15-40 |
| SchoolSecondary | ä¸­å­¦ | 20-60 |
| Warehouse | ä»“åº“ | 3-10 |

### B. BEARæ°”å€™ç±»å‹åˆ—è¡¨

| æ°”å€™ç±»å‹ | æè¿° | ä»£è¡¨åŸå¸‚ |
|---------|------|---------|
| Very_Hot_Humid | æçƒ­æ¹¿æ¶¦ | Honolulu |
| Hot_Humid | çƒ­æ¹¿æ¶¦ | Tampa |
| Hot_Dry | çƒ­å¹²ç‡¥ | Tucson |
| Warm_Humid | æ¸©æš–æ¹¿æ¶¦ | Atlanta |
| Warm_Dry | æ¸©æš–å¹²ç‡¥ | El Paso |
| Warm_Marine | æ¸©æš–æµ·æ´‹æ€§ | San Diego |
| Mixed_Humid | æ··åˆæ¹¿æ¶¦ | New York |
| Mixed_Dry | æ··åˆå¹²ç‡¥ | Albuquerque |
| Mixed_Marine | æ··åˆæµ·æ´‹æ€§ | Seattle |
| Cool_Humid | å‡‰çˆ½æ¹¿æ¶¦ | Buffalo |
| Cool_Dry | å‡‰çˆ½å¹²ç‡¥ | Denver |
| Cool_Marine | å‡‰çˆ½æµ·æ´‹æ€§ | Port Angeles |
| Cold_Humid | å¯’å†·æ¹¿æ¶¦ | Rochester |
| Cold_Dry | å¯’å†·å¹²ç‡¥ | Great Falls |
| Very_Cold | æå¯’ | International Falls |
| Subarctic/Arctic | äºšåŒ—æ/åŒ—æ | Fairbanks |

### C. å‚è€ƒèµ„æ–™

1. **BEARè®ºæ–‡**ï¼š
   - Zhang, C., Shi, Y., & Chen, Y. (2023). BEAR: Physics-Principled Building Environment for Control and Reinforcement Learning. ACM e-Energy 2023.

2. **DROPTç›¸å…³**ï¼š
   - æ‰©æ•£æ¨¡å‹ï¼šHo et al. (2020). Denoising Diffusion Probabilistic Models.
   - Tianshouæ¡†æ¶ï¼šhttps://github.com/thu-ml/tianshou

3. **å»ºç­‘èƒ½æºç®¡ç†**ï¼š
   - ASHRAEæ ‡å‡†ï¼šhttps://www.ashrae.org/
   - EnergyPlusæ–‡æ¡£ï¼šhttps://energyplus.net/

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11-07
**ä½œè€…**: DROPT Team


