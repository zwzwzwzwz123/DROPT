# æ•°æ®ä¸­å¿ƒç©ºè°ƒä¼˜åŒ–ç³»ç»Ÿæ¶æ„

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ç³»ç»Ÿçš„æ¶æ„è®¾è®¡å’Œæ•°æ®æµã€‚

---

## ğŸ“ ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         è®­ç»ƒä¸»ç¨‹åº (main_datacenter.py)                â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ å‚æ•°è§£æ     â”‚â†’â”‚ ç¯å¢ƒåˆ›å»º      â”‚â†’â”‚ ç½‘ç»œåˆå§‹åŒ–   â”‚â†’â”‚ è®­ç»ƒå¾ªç¯  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â†“                               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ç¯å¢ƒæ¨¡å— (env/)      â”‚       â”‚  ç­–ç•¥æ¨¡å— (policy/)   â”‚
    â”‚                       â”‚       â”‚                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚ DataCenterEnv   â”‚ â”‚       â”‚  â”‚ DiffusionOPT   â”‚ â”‚
    â”‚  â”‚                 â”‚ â”‚       â”‚  â”‚                â”‚ â”‚
    â”‚  â”‚ - reset()       â”‚ â”‚       â”‚  â”‚ - forward()    â”‚ â”‚
    â”‚  â”‚ - step()        â”‚ â”‚       â”‚  â”‚ - learn()      â”‚ â”‚
    â”‚  â”‚ - reward()      â”‚ â”‚       â”‚  â”‚ - update()     â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚           â”‚          â”‚       â”‚           â”‚         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚ ThermalModel    â”‚ â”‚       â”‚  â”‚ Diffusion      â”‚ â”‚
    â”‚  â”‚                 â”‚ â”‚       â”‚  â”‚                â”‚ â”‚
    â”‚  â”‚ - step()        â”‚ â”‚       â”‚  â”‚ - p_sample()   â”‚ â”‚
    â”‚  â”‚ - crac_model()  â”‚ â”‚       â”‚  â”‚ - q_sample()   â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚  â”‚ - loss()       â”‚ â”‚
    â”‚                      â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚                     â”‚
    â”‚  â”‚ ExpertControllerâ”‚ â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚                 â”‚ â”‚       â”‚  â”‚ DoubleCritic   â”‚ â”‚
    â”‚  â”‚ - PID           â”‚ â”‚       â”‚  â”‚                â”‚ â”‚
    â”‚  â”‚ - MPC           â”‚ â”‚       â”‚  â”‚ - Q1, Q2       â”‚ â”‚
    â”‚  â”‚ - RuleBased     â”‚ â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ è®­ç»ƒæµç¨‹å›¾

```
å¼€å§‹è®­ç»ƒ
    â”‚
    â”œâ”€â†’ [1] åˆå§‹åŒ–ç¯å¢ƒ
    â”‚       â”œâ”€ åˆ›å»ºDataCenterEnv
    â”‚       â”œâ”€ åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ (training_numä¸ª)
    â”‚       â””â”€ åˆ›å»ºæµ‹è¯•ç¯å¢ƒ (test_numä¸ª)
    â”‚
    â”œâ”€â†’ [2] åˆå§‹åŒ–ç½‘ç»œ
    â”‚       â”œâ”€ Actor: Diffusionæ¨¡å‹ (MLP + DDPM)
    â”‚       â”œâ”€ Critic: DoubleCritic (Q1, Q2)
    â”‚       â””â”€ Target Networks (è½¯æ›´æ–°)
    â”‚
    â”œâ”€â†’ [3] åˆ›å»ºç­–ç•¥
    â”‚       â””â”€ DiffusionOPT(actor, critic, ...)
    â”‚
    â”œâ”€â†’ [4] åˆ›å»ºCollectorå’ŒBuffer
    â”‚       â”œâ”€ TrainCollector (ä¸ç¯å¢ƒäº¤äº’)
    â”‚       â”œâ”€ TestCollector (è¯„ä¼°æ€§èƒ½)
    â”‚       â””â”€ ReplayBuffer (å­˜å‚¨ç»éªŒ)
    â”‚
    â””â”€â†’ [5] è®­ç»ƒå¾ªç¯ (offpolicy_trainer)
            â”‚
            â”œâ”€â†’ [5.1] æ”¶é›†ç»éªŒ
            â”‚       â”œâ”€ æ‰§è¡Œn_stepæ­¥
            â”‚       â”œâ”€ å­˜å…¥ReplayBuffer
            â”‚       â””â”€ è®°å½•ç»Ÿè®¡ä¿¡æ¯
            â”‚
            â”œâ”€â†’ [5.2] ç­–ç•¥æ›´æ–°
            â”‚       â”œâ”€ ä»Bufferé‡‡æ ·batch
            â”‚       â”œâ”€ æ›´æ–°Critic (TD learning)
            â”‚       â”œâ”€ æ›´æ–°Actor (BCæˆ–PG)
            â”‚       â””â”€ è½¯æ›´æ–°Target Networks
            â”‚
            â”œâ”€â†’ [5.3] è¯„ä¼°
            â”‚       â”œâ”€ åœ¨æµ‹è¯•ç¯å¢ƒè¿è¡Œ
            â”‚       â”œâ”€ è®¡ç®—å¹³å‡å¥–åŠ±
            â”‚       â””â”€ ä¿å­˜æœ€ä½³æ¨¡å‹
            â”‚
            â””â”€â†’ [5.4] æ—¥å¿—è®°å½•
                    â”œâ”€ TensorBoard
                    â”œâ”€ ç»ˆç«¯è¾“å‡º
                    â””â”€ æ£€æŸ¥ç‚¹ä¿å­˜
```

---

## ğŸŒŠ ç¯å¢ƒäº¤äº’æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å•æ­¥äº¤äº’æµç¨‹                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

çŠ¶æ€ s_t
    â”‚
    â”œâ”€â†’ [1] ç­–ç•¥ç”ŸæˆåŠ¨ä½œ
    â”‚       â”‚
    â”‚       â”œâ”€ è¾“å…¥: state (å½’ä¸€åŒ–)
    â”‚       â”‚
    â”‚       â”œâ”€ Diffusioné‡‡æ ·:
    â”‚       â”‚   â”œâ”€ åˆå§‹åŒ–: a_T ~ N(0, I)
    â”‚       â”‚   â”œâ”€ é€æ­¥å»å™ª: a_{t-1} = Î¼(a_t, s, t) + Ïƒ*Îµ
    â”‚       â”‚   â””â”€ è¾“å‡º: a_0 (å½’ä¸€åŒ–åŠ¨ä½œ)
    â”‚       â”‚
    â”‚       â””â”€ æ¢ç´¢å™ªå£° (è®­ç»ƒæ—¶10%æ¦‚ç‡)
    â”‚
    â”œâ”€â†’ [2] ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œ
    â”‚       â”‚
    â”‚       â”œâ”€ åå½’ä¸€åŒ–åŠ¨ä½œ:
    â”‚       â”‚   â”œâ”€ T_set: [-1,1] â†’ [18,28]Â°C
    â”‚       â”‚   â””â”€ fan_speed: [-1,1] â†’ [0.3,1.0]
    â”‚       â”‚
    â”‚       â”œâ”€ è·å–ä¸“å®¶åŠ¨ä½œ (BCæ¨¡å¼):
    â”‚       â”‚   â””â”€ expert.get_action(T_in, T_out, H_in, IT_load)
    â”‚       â”‚
    â”‚       â”œâ”€ çƒ­åŠ›å­¦ä»¿çœŸ:
    â”‚       â”‚   â”œâ”€ è®¡ç®—CRACåˆ¶å†·é‡
    â”‚       â”‚   â”œâ”€ æ›´æ–°æ¸©åº¦: T_in(t+1)
    â”‚       â”‚   â”œâ”€ æ›´æ–°æ¹¿åº¦: H_in(t+1)
    â”‚       â”‚   â””â”€ è®¡ç®—èƒ½è€—: E
    â”‚       â”‚
    â”‚       â”œâ”€ æ›´æ–°å¤–éƒ¨æ‰°åŠ¨:
    â”‚       â”‚   â”œâ”€ å®¤å¤–æ¸©åº¦: T_out(t+1)
    â”‚       â”‚   â””â”€ ITè´Ÿè½½: IT_load(t+1)
    â”‚       â”‚
    â”‚       â””â”€ è®¡ç®—å¥–åŠ±:
    â”‚           â”œâ”€ èƒ½è€—æƒ©ç½š: -Î±*E
    â”‚           â”œâ”€ æ¸©åº¦åå·®: -Î²*(T_in - T_target)Â²
    â”‚           â””â”€ è¶Šç•Œæƒ©ç½š: -Î³*I_violation
    â”‚
    â””â”€â†’ [3] è¿”å›ç»“æœ
            â”‚
            â”œâ”€ next_state: [T_in, T_out, H_in, IT_load, T_supply, reward]
            â”œâ”€ reward: R
            â”œâ”€ done: episodeç»“æŸæ ‡å¿—
            â””â”€ info: {T_in, energy, temp_violation, expert_action, ...}
```

---

## ğŸ§  ç­–ç•¥æ›´æ–°æµç¨‹

### Criticæ›´æ–°ï¼ˆTD Learningï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Criticæ›´æ–°æµç¨‹                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä»Bufferé‡‡æ ·: (s, a, r, s', done)
    â”‚
    â”œâ”€â†’ [1] è®¡ç®—ç›®æ ‡Qå€¼
    â”‚       â”‚
    â”‚       â”œâ”€ ç”Ÿæˆä¸‹ä¸€åŠ¨ä½œ: a' = Ï€(s')
    â”‚       â”‚
    â”‚       â”œâ”€ è®¡ç®—ç›®æ ‡Q:
    â”‚       â”‚   â”œâ”€ Q1_target = Q1'(s', a')
    â”‚       â”‚   â”œâ”€ Q2_target = Q2'(s', a')
    â”‚       â”‚   â””â”€ Q_target = min(Q1_target, Q2_target)
    â”‚       â”‚
    â”‚       â””â”€ TDç›®æ ‡: y = r + Î³ * Q_target * (1 - done)
    â”‚
    â”œâ”€â†’ [2] è®¡ç®—å½“å‰Qå€¼
    â”‚       â”‚
    â”‚       â”œâ”€ Q1_current = Q1(s, a)
    â”‚       â””â”€ Q2_current = Q2(s, a)
    â”‚
    â”œâ”€â†’ [3] è®¡ç®—æŸå¤±
    â”‚       â”‚
    â”‚       â””â”€ loss = MSE(Q1_current, y) + MSE(Q2_current, y)
    â”‚
    â””â”€â†’ [4] æ¢¯åº¦æ›´æ–°
            â”‚
            â”œâ”€ loss.backward()
            â”œâ”€ optimizer.step()
            â””â”€ è½¯æ›´æ–°Target: Î¸' â† Ï„*Î¸ + (1-Ï„)*Î¸'
```

### Actoræ›´æ–°ï¼ˆBCæˆ–PGï¼‰

#### è¡Œä¸ºå…‹éš†æ¨¡å¼ (BC)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Actoræ›´æ–° - BCæ¨¡å¼                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä»Bufferé‡‡æ ·: (s, a, r, s', done, expert_action)
    â”‚
    â”œâ”€â†’ [1] ç”ŸæˆåŠ¨ä½œ
    â”‚       â”‚
    â”‚       â””â”€ a_pred = Ï€(s)  # é€šè¿‡æ‰©æ•£æ¨¡å‹é‡‡æ ·
    â”‚
    â”œâ”€â†’ [2] è®¡ç®—BCæŸå¤±
    â”‚       â”‚
    â”‚       â””â”€ loss_bc = MSE(a_pred, expert_action)
    â”‚
    â””â”€â†’ [3] æ¢¯åº¦æ›´æ–°
            â”‚
            â”œâ”€ loss_bc.backward()
            â””â”€ optimizer.step()
```

#### ç­–ç•¥æ¢¯åº¦æ¨¡å¼ (PG)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Actoræ›´æ–° - PGæ¨¡å¼                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä»Bufferé‡‡æ ·: (s, a, r, s', done)
    â”‚
    â”œâ”€â†’ [1] ç”ŸæˆåŠ¨ä½œ
    â”‚       â”‚
    â”‚       â””â”€ a_new = Ï€(s)  # é€šè¿‡æ‰©æ•£æ¨¡å‹é‡‡æ ·
    â”‚
    â”œâ”€â†’ [2] è®¡ç®—Qå€¼
    â”‚       â”‚
    â”‚       â””â”€ Q = min(Q1(s, a_new), Q2(s, a_new))
    â”‚
    â”œâ”€â†’ [3] è®¡ç®—ç­–ç•¥æŸå¤±
    â”‚       â”‚
    â”‚       â””â”€ loss_policy = -Q.mean()  # æœ€å¤§åŒ–Qå€¼
    â”‚
    â””â”€â†’ [4] æ¢¯åº¦æ›´æ–°
            â”‚
            â”œâ”€ loss_policy.backward()
            â””â”€ optimizer.step()
```

---

## ğŸ¯ æ‰©æ•£æ¨¡å‹è¯¦è§£

### å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼ˆè®­ç»ƒæ—¶ï¼‰

```
åŸå§‹åŠ¨ä½œ a_0
    â”‚
    â”œâ”€â†’ æ·»åŠ å™ªå£°: a_t = âˆš(Î±Ì…_t) * a_0 + âˆš(1-Î±Ì…_t) * Îµ
    â”‚
    â””â”€â†’ é¢„æµ‹å™ªå£°: Îµ_pred = Îµ_Î¸(a_t, s, t)
            â”‚
            â””â”€â†’ æŸå¤±: L = MSE(Îµ_pred, Îµ)
```

### åå‘é‡‡æ ·è¿‡ç¨‹ï¼ˆæ¨ç†æ—¶ï¼‰

```
éšæœºå™ªå£° a_T ~ N(0, I)
    â”‚
    â”œâ”€â†’ æ­¥éª¤ T â†’ T-1:
    â”‚   â”œâ”€ é¢„æµ‹å™ªå£°: Îµ = Îµ_Î¸(a_T, s, T)
    â”‚   â”œâ”€ è®¡ç®—å‡å€¼: Î¼ = (a_T - Î²_T*Îµ) / âˆš(Î±_T)
    â”‚   â””â”€ é‡‡æ ·: a_{T-1} = Î¼ + Ïƒ_T * z
    â”‚
    â”œâ”€â†’ æ­¥éª¤ T-1 â†’ T-2:
    â”‚   â””â”€ ... (é‡å¤)
    â”‚
    â”œâ”€â†’ ...
    â”‚
    â””â”€â†’ æ­¥éª¤ 1 â†’ 0:
        â””â”€â†’ è¾“å‡º: a_0 (æœ€ç»ˆåŠ¨ä½œ)
```

---

## ğŸ“Š æ•°æ®æµå›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ•°æ®æµå‘                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¤–éƒ¨æ•°æ®æº
    â”‚
    â”œâ”€â†’ weather_data.csv â”€â”€â”
    â”‚                      â”‚
    â””â”€â†’ workload_trace.csv â”˜
                â”‚
                â†“
        DataCenterEnv
                â”‚
                â”œâ”€â†’ çŠ¶æ€ s_t â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Policy â”€â”€â†’ åŠ¨ä½œ a_t
                â”‚                          â†‘
                â”‚                          â”‚
                â†“                          â”‚
        ThermalModel                       â”‚
                â”‚                          â”‚
                â”œâ”€â†’ æ¸©åº¦æ¼”åŒ–                â”‚
                â”œâ”€â†’ èƒ½è€—è®¡ç®—                â”‚
                â””â”€â†’ å¥–åŠ±è®¡ç®— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
                  ReplayBuffer
                        â”‚
                        â”œâ”€â†’ é‡‡æ · â”€â”€â†’ Criticæ›´æ–°
                        â”‚
                        â””â”€â†’ é‡‡æ · â”€â”€â†’ Actoræ›´æ–°
                                â”‚
                                â†“
                          TensorBoardæ—¥å¿—
```

---

## ğŸ”§ å…³é”®ç»„ä»¶æ¥å£

### DataCenterEnv

```python
class DataCenterEnv(gym.Env):
    """æ•°æ®ä¸­å¿ƒç¯å¢ƒ"""
    
    def __init__(self, num_crac_units, target_temp, ...):
        """åˆå§‹åŒ–ç¯å¢ƒå‚æ•°"""
        
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹çŠ¶æ€"""
        
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
        """æ‰§è¡Œä¸€æ­¥ï¼Œè¿”å›(next_state, reward, done, info)"""
        
    def _compute_reward(self) -> float:
        """è®¡ç®—å¥–åŠ±"""
        
    def _denormalize_action(self, action: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """åå½’ä¸€åŒ–åŠ¨ä½œ"""
```

### ThermalModel

```python
class ThermalModel:
    """çƒ­åŠ›å­¦æ¨¡å‹"""
    
    def __init__(self, num_crac, thermal_mass, ...):
        """åˆå§‹åŒ–ç‰©ç†å‚æ•°"""
        
    def step(self, T_in, T_out, H_in, IT_load, T_set, fan_speed) -> Tuple:
        """ä»¿çœŸä¸€æ­¥ï¼Œè¿”å›(T_in_next, H_in_next, T_supply, energy)"""
        
    def _crac_model(self, T_in, T_out, T_set, fan_speed) -> Tuple:
        """CRACå•å…ƒæ¨¡å‹ï¼Œè¿”å›(Q_cooling, T_supply, energy)"""
```

### ExpertController

```python
class PIDController:
    """PIDæ§åˆ¶å™¨"""
    
    def __init__(self, num_crac, target_temp, Kp, Ki, Kd):
        """åˆå§‹åŒ–PIDå‚æ•°"""
        
    def get_action(self, T_in, T_out, H_in, IT_load) -> np.ndarray:
        """ç”Ÿæˆæ§åˆ¶åŠ¨ä½œï¼ˆå½’ä¸€åŒ–åˆ°[-1,1]ï¼‰"""
        
    def reset(self):
        """é‡ç½®ç§¯åˆ†é¡¹"""
```

### DiffusionOPT

```python
class DiffusionOPT(BasePolicy):
    """æ‰©æ•£æ¨¡å‹ç­–ç•¥"""
    
    def __init__(self, actor, critic, ...):
        """åˆå§‹åŒ–Actorå’ŒCritic"""
        
    def forward(self, batch, state=None, **kwargs) -> Batch:
        """ç”ŸæˆåŠ¨ä½œ"""
        
    def learn(self, batch: Batch, **kwargs) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œï¼Œè¿”å›æŸå¤±å­—å…¸"""
        
    def update(self, sample_size: int, buffer, **kwargs) -> Dict[str, Any]:
        """è®­ç»ƒå¾ªç¯è°ƒç”¨çš„æ›´æ–°å‡½æ•°"""
```

---

## ğŸ“ˆ æ€§èƒ½ç›‘æ§æŒ‡æ ‡

### è®­ç»ƒé˜¶æ®µç›‘æ§

```
TensorBoardæŒ‡æ ‡:
â”œâ”€ train/
â”‚  â”œâ”€ loss/actor          # ActoræŸå¤±
â”‚  â”œâ”€ loss/critic         # CriticæŸå¤±
â”‚  â”œâ”€ loss/bc             # BCæŸå¤±ï¼ˆBCæ¨¡å¼ï¼‰
â”‚  â”œâ”€ reward              # å¹³å‡å¥–åŠ±
â”‚  â”œâ”€ length              # å›åˆé•¿åº¦
â”‚  â””â”€ gradient_norm       # æ¢¯åº¦èŒƒæ•°
â”‚
â””â”€ test/
   â”œâ”€ reward              # æµ‹è¯•å¥–åŠ±
   â”œâ”€ energy              # å¹³å‡èƒ½è€—
   â”œâ”€ temp_violation      # æ¸©åº¦è¶Šç•Œç‡
   â””â”€ temp_std            # æ¸©åº¦æ ‡å‡†å·®
```

### æ¨ç†é˜¶æ®µç›‘æ§

```
å®æ—¶æŒ‡æ ‡:
â”œâ”€ æ§åˆ¶æ€§èƒ½
â”‚  â”œâ”€ æ¸©åº¦åå·® (Â°C)
â”‚  â”œâ”€ æ¸©åº¦è¶Šç•Œç‡ (%)
â”‚  â””â”€ æ¸©åº¦æ ‡å‡†å·® (Â°C)
â”‚
â”œâ”€ èƒ½è€—æŒ‡æ ‡
â”‚  â”œâ”€ ç¬æ—¶åŠŸç‡ (kW)
â”‚  â”œâ”€ ç´¯è®¡èƒ½è€— (kWh)
â”‚  â””â”€ PUE (Power Usage Effectiveness)
â”‚
â””â”€ ç³»ç»ŸæŒ‡æ ‡
   â”œâ”€ æ¨ç†å»¶è¿Ÿ (ms)
   â”œâ”€ åŠ¨ä½œå¹³æ»‘åº¦
   â””â”€ CRACè´Ÿè½½å‡è¡¡
```

---

## ğŸ¨ å¯è§†åŒ–ç¤ºä¾‹

### è®­ç»ƒæ›²çº¿

```
Reward vs. Epoch
    â”‚
 0  â”‚                                    â•±â”€â”€â”€â”€â”€
    â”‚                              â•±â”€â”€â”€â”€â•¯
-50 â”‚                        â•±â”€â”€â”€â”€â•¯
    â”‚                  â•±â”€â”€â”€â”€â•¯
-100â”‚            â•±â”€â”€â”€â”€â•¯
    â”‚      â•±â”€â”€â”€â”€â•¯
-150â”‚ â”€â”€â”€â”€â•¯
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
      0    50k   100k  150k  200k  Epoch
```

### æ¸©åº¦æ§åˆ¶æ•ˆæœ

```
Temperature (Â°C)
    â”‚
 26 â”‚  â•±â•²    â•±â•²                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚ â•±  â•²  â•±  â•²                  â•±            
 24 â”‚â•¯    â•²â•±    â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  (Target)
    â”‚                                           
 22 â”‚                                           
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
      0    6    12   18   24   Time (hour)
      
      â† PIDæ§åˆ¶ â†’  â† DRLæ§åˆ¶ â†’
```

---

## ğŸ” è°ƒè¯•è§†å›¾

### çŠ¶æ€ç©ºé—´å¯è§†åŒ–

```
State Vector (9-dim for 4 CRACs):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [0] T_in        = 24.5Â°C               â”‚
â”‚ [1] T_out       = 30.2Â°C               â”‚
â”‚ [2] H_in        = 52.3%                â”‚
â”‚ [3] IT_load     = 250.0kW              â”‚
â”‚ [4] T_supply_1  = 18.5Â°C               â”‚
â”‚ [5] T_supply_2  = 18.7Â°C               â”‚
â”‚ [6] T_supply_3  = 18.6Â°C               â”‚
â”‚ [7] T_supply_4  = 18.8Â°C               â”‚
â”‚ [8] reward_last = -45.2                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åŠ¨ä½œç©ºé—´å¯è§†åŒ–

```
Action Vector (8-dim for 4 CRACs):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CRAC 1: T_set=20.0Â°C, fan=0.75         â”‚
â”‚ CRAC 2: T_set=20.5Â°C, fan=0.70         â”‚
â”‚ CRAC 3: T_set=19.8Â°C, fan=0.78         â”‚
â”‚ CRAC 4: T_set=20.2Â°C, fan=0.72         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**æœ¬æ¶æ„æ–‡æ¡£æä¾›äº†ç³»ç»Ÿçš„å®Œæ•´è§†å›¾ï¼Œå¸®åŠ©ç†è§£å„ç»„ä»¶å¦‚ä½•ååŒå·¥ä½œã€‚**

