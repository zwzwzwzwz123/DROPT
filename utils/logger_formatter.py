# ========================================
# è®­ç»ƒæ—¥å¿—æ ¼å¼åŒ–å·¥å…·
# ========================================
# æä¾›ç¾åŒ–çš„ç»ˆç«¯æ—¥å¿—è¾“å‡ºï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´æ¸…æ™°æ˜“è¯»

import time
from typing import Dict, Any, Optional
from datetime import datetime, timedelta


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—æ ¼å¼åŒ–å™¨"""

    def __init__(self, total_epochs: int, reward_scale: float = 1.0, diffusion_steps: int = None):
        """
        åˆå§‹åŒ–æ—¥å¿—æ ¼å¼åŒ–å™¨

        å‚æ•°:
        - total_epochs: æ€»è®­ç»ƒè½®æ¬¡
        - reward_scale: å¥–åŠ±ç¼©æ”¾ç³»æ•°
        - diffusion_steps: æ‰©æ•£æ¨¡å‹æ­¥æ•°ï¼ˆå¯é€‰ï¼‰
        """
        self.total_epochs = total_epochs  # æ€»è½®æ¬¡
        self.reward_scale = reward_scale  # å¥–åŠ±ç¼©æ”¾ç³»æ•°
        self.diffusion_steps = diffusion_steps  # æ‰©æ•£æ­¥æ•°
        self.start_time = time.time()  # è®­ç»ƒå¼€å§‹æ—¶é—´
        self.last_epoch_time = time.time()  # ä¸Šä¸€è½®æ¬¡æ—¶é—´
        self.epoch_times = []  # æ¯è½®è€—æ—¶è®°å½•

        # ç”¨äºæ£€æµ‹å¼‚å¸¸å€¼çš„é˜ˆå€¼
        self.thresholds = {
            'actor_loss_high': 20.0,  # ActoræŸå¤±è¿‡é«˜é˜ˆå€¼
            'critic_loss_high': 300.0,  # CriticæŸå¤±è¿‡é«˜é˜ˆå€¼
            'grad_norm_high': 1000.0,  # æ¢¯åº¦èŒƒæ•°è¿‡é«˜é˜ˆå€¼
        }
    
    def format_time(self, seconds: float) -> str:
        """
        æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
        
        å‚æ•°:
        - seconds: ç§’æ•°
        
        è¿”å›:
        - æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
        """
        if seconds < 60:
            return f"{int(seconds)}ç§’"
        elif seconds < 3600:
            minutes = int(seconds / 60)
            secs = int(seconds % 60)
            return f"{minutes}åˆ†{secs}ç§’"
        else:
            hours = int(seconds / 3600)
            minutes = int((seconds % 3600) / 60)
            return f"{hours}å°æ—¶{minutes}åˆ†"
    
    def get_indicator(self, value: float, threshold: float, lower_is_better: bool = True) -> str:
        """
        è·å–æŒ‡æ ‡çŠ¶æ€æŒ‡ç¤ºç¬¦
        
        å‚æ•°:
        - value: å½“å‰å€¼
        - threshold: é˜ˆå€¼
        - lower_is_better: æ˜¯å¦è¶Šä½è¶Šå¥½
        
        è¿”å›:
        - çŠ¶æ€æŒ‡ç¤ºç¬¦ï¼ˆâœ“ æ­£å¸¸ï¼Œâš  è­¦å‘Šï¼‰
        """
        if lower_is_better:
            return "âœ“" if value < threshold else "âš "
        else:
            return "âœ“" if value > threshold else "âš "
    
    def log_epoch(
        self,
        epoch: int,
        train_result: Dict[str, Any],
        test_result: Optional[Dict[str, Any]] = None
    ):
        """
        è®°å½•å¹¶æ ¼å¼åŒ–è¾“å‡ºä¸€ä¸ªepochçš„è®­ç»ƒä¿¡æ¯
        
        å‚æ•°:
        - epoch: å½“å‰è½®æ¬¡
        - train_result: è®­ç»ƒç»“æœå­—å…¸
        - test_result: æµ‹è¯•ç»“æœå­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        current_time = time.time()
        epoch_time = current_time - self.last_epoch_time
        self.last_epoch_time = current_time
        self.epoch_times.append(epoch_time)
        
        # ä¿ç•™æœ€è¿‘100ä¸ªepochçš„æ—¶é—´ç”¨äºä¼°ç®—
        if len(self.epoch_times) > 100:
            self.epoch_times.pop(0)
        
        avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)
        elapsed_time = current_time - self.start_time
        remaining_epochs = self.total_epochs - epoch
        estimated_remaining = avg_epoch_time * remaining_epochs
        
        # æå–å…³é”®æŒ‡æ ‡
        actor_loss = train_result.get('loss/actor', 0.0)
        critic_loss = train_result.get('loss/critic', 0.0)
        actor_grad = train_result.get('grad_norm/actor', 0.0)
        critic_grad = train_result.get('grad_norm/critic', 0.0)

        # å¥–åŠ±å€¼ï¼ˆå°è¯•å¤šä¸ªå¯èƒ½çš„é”®åï¼‰
        train_reward = train_result.get('train/reward',
                                       train_result.get('rew',
                                       train_result.get('rews', 0.0)))
        test_reward = 0.0
        if test_result:
            test_reward = test_result.get('test/reward',
                                         test_result.get('rew',
                                         test_result.get('rews', 0.0)))

        q_mean = train_result.get('q_value/q_mean', 0.0)
        td_error = train_result.get('q_value/td_error', 0.0)

        # æ¢ç´¢ç‡
        exploration_noise = train_result.get('exploration/noise', 0.0)
        
        # æ‰“å°æ ¼å¼åŒ–çš„æ—¥å¿—
        print("\n" + "=" * 80)
        epoch_info = f"  Epoch {epoch}/{self.total_epochs}  [{epoch/self.total_epochs*100:.1f}%]"
        if self.diffusion_steps:
            epoch_info += f"  | æ‰©æ•£æ­¥æ•°: {self.diffusion_steps}"
        print(epoch_info)
        print("=" * 80)
        
        # æŸå¤±æŒ‡æ ‡
        print("\nğŸ“Š æŸå¤±æŒ‡æ ‡:")
        actor_indicator = self.get_indicator(actor_loss, self.thresholds['actor_loss_high'])
        critic_indicator = self.get_indicator(critic_loss, self.thresholds['critic_loss_high'])
        print(f"  {actor_indicator} ActoræŸå¤±:     {actor_loss:>10.3f}")
        print(f"  {critic_indicator} CriticæŸå¤±:    {critic_loss:>10.3f}")
        
        # æ¢¯åº¦ä¿¡æ¯
        print("\nğŸ“ˆ æ¢¯åº¦èŒƒæ•°:")
        actor_grad_indicator = self.get_indicator(actor_grad, self.thresholds['grad_norm_high'])
        critic_grad_indicator = self.get_indicator(critic_grad, self.thresholds['grad_norm_high'])
        print(f"  {actor_grad_indicator} Actoræ¢¯åº¦:     {actor_grad:>10.3f}")
        print(f"  {critic_grad_indicator} Criticæ¢¯åº¦:    {critic_grad:>10.3f}")
        
        # æ€§èƒ½æŒ‡æ ‡
        print("\nğŸ¯ æ€§èƒ½æŒ‡æ ‡:")
        print(f"  è®­ç»ƒå¥–åŠ±:       {train_reward:>10.2f}  (ç¼©æ”¾å)")
        if test_result:
            print(f"  æµ‹è¯•å¥–åŠ±:       {test_reward:>10.2f}  (ç¼©æ”¾å)")
        print(f"  çœŸå®è®­ç»ƒå¥–åŠ±:   {train_reward/self.reward_scale:>10.2f}")
        if test_result:
            print(f"  çœŸå®æµ‹è¯•å¥–åŠ±:   {test_reward/self.reward_scale:>10.2f}")
        
        # Qå€¼ç»Ÿè®¡
        print("\nğŸ’ Qå€¼ç»Ÿè®¡:")
        print(f"  Qå‡å€¼:          {q_mean:>10.3f}")
        print(f"  TDè¯¯å·®:         {td_error:>10.3f}")
        
        # æ¢ç´¢ä¿¡æ¯
        if exploration_noise > 0:
            print("\nğŸ” æ¢ç´¢ä¿¡æ¯:")
            print(f"  æ¢ç´¢å™ªå£°:       {exploration_noise:>10.3f}")
        
        # æ—¶é—´ç»Ÿè®¡
        print("\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
        print(f"  æœ¬è½®è€—æ—¶:       {self.format_time(epoch_time)}")
        print(f"  å·²ç”¨æ—¶é—´:       {self.format_time(elapsed_time)}")
        print(f"  é¢„è®¡å‰©ä½™:       {self.format_time(estimated_remaining)}")
        print(f"  å¹³å‡æ¯è½®:       {self.format_time(avg_epoch_time)}")
        
        # å¼‚å¸¸è­¦å‘Š
        warnings = []
        if actor_loss > self.thresholds['actor_loss_high']:
            warnings.append(f"ActoræŸå¤±è¿‡é«˜ ({actor_loss:.2f} > {self.thresholds['actor_loss_high']})")
        if critic_loss > self.thresholds['critic_loss_high']:
            warnings.append(f"CriticæŸå¤±è¿‡é«˜ ({critic_loss:.2f} > {self.thresholds['critic_loss_high']})")
        if critic_grad > self.thresholds['grad_norm_high']:
            warnings.append(f"Criticæ¢¯åº¦è¿‡å¤§ ({critic_grad:.2f} > {self.thresholds['grad_norm_high']})")
        
        if warnings:
            print("\nâš ï¸  è­¦å‘Š:")
            for warning in warnings:
                print(f"  - {warning}")
        
        print("\n" + "=" * 80)
    
    def log_compact(
        self,
        epoch: int,
        train_result: Dict[str, Any],
        test_result: Optional[Dict[str, Any]] = None
    ):
        """
        ç´§å‡‘æ ¼å¼çš„æ—¥å¿—è¾“å‡ºï¼ˆé€‚åˆé¢‘ç¹è¾“å‡ºï¼‰
        
        å‚æ•°:
        - epoch: å½“å‰è½®æ¬¡
        - train_result: è®­ç»ƒç»“æœå­—å…¸
        - test_result: æµ‹è¯•ç»“æœå­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        actor_loss = train_result.get('loss/actor', 0.0)
        critic_loss = train_result.get('loss/critic', 0.0)

        # å¥–åŠ±å€¼ï¼ˆå°è¯•å¤šä¸ªå¯èƒ½çš„é”®åï¼‰
        train_reward = train_result.get('train/reward',
                                       train_result.get('rew',
                                       train_result.get('rews', 0.0)))
        test_reward = 0.0
        if test_result:
            test_reward = test_result.get('test/reward',
                                         test_result.get('rew',
                                         test_result.get('rews', 0.0)))
        
        # è®¡ç®—è¿›åº¦
        progress = epoch / self.total_epochs * 100
        elapsed = time.time() - self.start_time

        # å•è¡Œè¾“å‡º
        compact_line = (f"Epoch {epoch:>5}/{self.total_epochs} [{progress:>5.1f}%] | "
                       f"Actor: {actor_loss:>7.2f} | Critic: {critic_loss:>7.2f} | "
                       f"Train: {train_reward:>8.2f} | Test: {test_reward:>8.2f} | "
                       f"Time: {self.format_time(elapsed)}")
        if self.diffusion_steps:
            compact_line += f" | Diff: {self.diffusion_steps}æ­¥"
        print(compact_line)
    
    def log_summary(self, final_result: Dict[str, Any]):
        """
        è®­ç»ƒç»“æŸåçš„æ€»ç»“æ—¥å¿—
        
        å‚æ•°:
        - final_result: æœ€ç»ˆè®­ç»ƒç»“æœ
        """
        total_time = time.time() - self.start_time
        
        print("\n" + "=" * 80)
        print("  ğŸ‰ è®­ç»ƒå®Œæˆæ€»ç»“")
        print("=" * 80)
        
        print(f"\næ€»è®­ç»ƒæ—¶é—´: {self.format_time(total_time)}")
        print(f"æ€»è½®æ¬¡: {self.total_epochs}")
        print(f"å¹³å‡æ¯è½®: {self.format_time(total_time / self.total_epochs)}")
        
        if 'best_reward' in final_result:
            best_reward = final_result['best_reward']
            print(f"\næœ€ä½³æµ‹è¯•å¥–åŠ±: {best_reward:.2f} (ç¼©æ”¾å)")
            print(f"çœŸå®æœ€ä½³å¥–åŠ±: {best_reward / self.reward_scale:.2f}")
        
        print("\n" + "=" * 80)


def create_epoch_logger(total_epochs: int, reward_scale: float = 1.0, verbose: bool = True):
    """
    åˆ›å»ºepochæ—¥å¿—è®°å½•å™¨ï¼ˆç”¨äºTianshou trainerçš„å›è°ƒï¼‰

    å‚æ•°:
    - total_epochs: æ€»è®­ç»ƒè½®æ¬¡
    - reward_scale: å¥–åŠ±ç¼©æ”¾ç³»æ•°
    - verbose: æ˜¯å¦è¯¦ç»†è¾“å‡ºï¼ˆTrue=è¯¦ç»†æ ¼å¼ï¼ŒFalse=ç´§å‡‘æ ¼å¼ï¼‰

    è¿”å›:
    - æ—¥å¿—å›è°ƒå‡½æ•°
    """
    logger = TrainingLogger(total_epochs, reward_scale)

    def log_fn(epoch: int, env_step: int, gradient_step: int,
               train_result: Dict[str, Any], test_result: Optional[Dict[str, Any]] = None):
        """
        Tianshou trainerçš„æ—¥å¿—å›è°ƒå‡½æ•°

        å‚æ•°:
        - epoch: å½“å‰è½®æ¬¡
        - env_step: ç¯å¢ƒæ­¥æ•°
        - gradient_step: æ¢¯åº¦æ›´æ–°æ­¥æ•°
        - train_result: è®­ç»ƒç»“æœ
        - test_result: æµ‹è¯•ç»“æœ
        """
        if verbose:
            logger.log_epoch(epoch, train_result, test_result)
        else:
            logger.log_compact(epoch, train_result, test_result)

    return log_fn, logger


class EnhancedTensorboardLogger:
    """
    å¢å¼ºçš„TensorBoardæ—¥å¿—è®°å½•å™¨

    åœ¨åŸæœ‰TensorBoardè®°å½•çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ ç¾åŒ–çš„ç»ˆç«¯è¾“å‡º
    ç»§æ‰¿å¹¶æ‰©å±•TensorboardLoggerçš„æ‰€æœ‰åŠŸèƒ½
    """

    def __init__(self, writer, total_epochs: int, reward_scale: float = 1.0,
                 log_interval: int = 1, verbose: bool = True, diffusion_steps: int = None):
        """
        åˆå§‹åŒ–å¢å¼ºæ—¥å¿—è®°å½•å™¨

        å‚æ•°:
        - writer: TensorBoard SummaryWriter
        - total_epochs: æ€»è®­ç»ƒè½®æ¬¡
        - reward_scale: å¥–åŠ±ç¼©æ”¾ç³»æ•°
        - log_interval: æ—¥å¿—è¾“å‡ºé—´éš”ï¼ˆæ¯Nä¸ªepochè¾“å‡ºä¸€æ¬¡ï¼‰
        - verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        - diffusion_steps: æ‰©æ•£æ¨¡å‹æ­¥æ•°ï¼ˆå¯é€‰ï¼‰
        """
        from tianshou.utils import TensorboardLogger

        self.tb_logger = TensorboardLogger(writer)  # åŸå§‹TensorBoard logger
        self.training_logger = TrainingLogger(total_epochs, reward_scale, diffusion_steps)  # ç»ˆç«¯æ—¥å¿—æ ¼å¼åŒ–å™¨
        self.log_interval = log_interval  # æ—¥å¿—è¾“å‡ºé—´éš”
        self.verbose = verbose  # æ˜¯å¦è¯¦ç»†è¾“å‡º
        self.writer = writer  # TensorBoard writer

        # åˆå§‹åŒ–ç»“æœç¼“å­˜
        self._last_train_result = {}
        self._last_test_result = {}
        self._last_update_result = {}
        self._current_epoch = 0
        self._last_output_epoch = -1  # è®°å½•ä¸Šæ¬¡è¾“å‡ºçš„epochï¼Œé¿å…é‡å¤è¾“å‡º
        self._has_update_data = False  # æ ‡è®°æ˜¯å¦æœ‰æ›´æ–°æ•°æ®

    def write(self, step_type: str, step: int, data: Dict[str, Any]):
        """
        å†™å…¥æ—¥å¿—ï¼ˆå…¼å®¹Tianshouçš„Loggeræ¥å£ï¼‰

        å‚æ•°:
        - step_type: æ­¥éª¤ç±»å‹ï¼ˆ'train', 'test', 'update'ç­‰ï¼‰
        - step: æ­¥éª¤ç¼–å·
        - data: æ•°æ®å­—å…¸
        """
        # å†™å…¥TensorBoard
        self.tb_logger.write(step_type, step, data)

    def save_data(
        self,
        epoch: int,
        env_step: int,
        gradient_step: int,
        save_checkpoint_fn=None,
    ):
        """ä¿å­˜æ•°æ®ï¼ˆå…¼å®¹Tianshouçš„Loggeræ¥å£ï¼‰"""
        self.tb_logger.save_data(epoch, env_step, gradient_step, save_checkpoint_fn)

    def restore_data(self):
        """æ¢å¤æ•°æ®ï¼ˆå…¼å®¹Tianshouçš„Loggeræ¥å£ï¼‰"""
        return self.tb_logger.restore_data()

    def log_test_data(self, collect_result: Dict[str, Any], step: int):
        """
        è®°å½•æµ‹è¯•æ•°æ®ï¼ˆå…¼å®¹Tianshouçš„Loggeræ¥å£ï¼‰

        å‚æ•°:
        - collect_result: æ”¶é›†ç»“æœå­—å…¸
        - step: å½“å‰æ­¥æ•°
        """
        # è°ƒç”¨åŸå§‹TensorBoard logger
        self.tb_logger.log_test_data(collect_result, step)

        # ä¿å­˜æµ‹è¯•ç»“æœ
        self._last_test_result = collect_result
        # æ›´æ–°å½“å‰epochï¼ˆé™¤ä»¥4ä¿®æ­£ï¼‰
        self._current_epoch = step // 4

        # è¾“å‡ºåˆ°ç»ˆç«¯ï¼ˆæµ‹è¯•åè¾“å‡ºï¼‰
        self._output_to_terminal()

    def log_train_data(self, collect_result: Dict[str, Any], step: int):
        """
        è®°å½•è®­ç»ƒæ•°æ®ï¼ˆå…¼å®¹Tianshouçš„Loggeræ¥å£ï¼‰

        å‚æ•°:
        - collect_result: æ”¶é›†ç»“æœå­—å…¸
        - step: å½“å‰æ­¥æ•°
        """
        # è°ƒç”¨åŸå§‹TensorBoard logger
        self.tb_logger.log_train_data(collect_result, step)

        # ä¿å­˜è®­ç»ƒç»“æœ
        self._last_train_result = collect_result

        # æ›´æ–°å½“å‰epochï¼ˆé™¤ä»¥4ä¿®æ­£ï¼‰
        self._current_epoch = step // 4

    def log_update_data(self, update_result: Dict[str, Any], step: int):
        """
        è®°å½•æ›´æ–°æ•°æ®ï¼ˆå…¼å®¹Tianshouçš„Loggeræ¥å£ï¼‰

        å‚æ•°:
        - update_result: æ›´æ–°ç»“æœå­—å…¸
        - step: å½“å‰æ­¥æ•°ï¼ˆæ³¨æ„ï¼šè¿™æ˜¯gradient_stepï¼Œä¸æ˜¯epochï¼ï¼‰
        """
        # è°ƒç”¨åŸå§‹TensorBoard logger
        self.tb_logger.log_update_data(update_result, step)

        # ä¿å­˜æ›´æ–°ç»“æœï¼ˆåˆå¹¶åˆ°è®­ç»ƒç»“æœä¸­ï¼‰
        self._last_train_result.update(update_result)

        # æ³¨æ„ï¼šstepæ˜¯gradient_stepï¼Œä¸æ˜¯epoch
        # æˆ‘ä»¬éœ€è¦ä»train_dataæˆ–test_dataä¸­è·å–çœŸæ­£çš„epoch
        # è¿™é‡Œæš‚æ—¶ä¿å­˜stepï¼Œä½†ä¸æ›´æ–°_current_epoch
        # self._current_epoch = step  # â† è¿™æ˜¯é”™è¯¯çš„ï¼

        # æ ‡è®°æœ‰æ›´æ–°æ•°æ®ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åº”è¯¥è¾“å‡ºï¼‰
        self._has_update_data = True

    def _output_to_terminal(self):
        """è¾“å‡ºåˆ°ç»ˆç«¯ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¾“å‡ºé—´éš”
        if self._current_epoch % self.log_interval != 0:
            return

        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒæ•°æ®ï¼ˆé¿å…åœ¨åˆå§‹æµ‹è¯•æ—¶è¾“å‡ºï¼‰
        if not self._last_train_result:
            return

        # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°æ•°æ®ï¼ˆåªæœ‰åœ¨æœ‰æ›´æ–°æ•°æ®æ—¶æ‰è¾“å‡ºï¼‰
        if not self._has_update_data:
            return

        # æ£€æŸ¥æ˜¯å¦å·²ç»è¾“å‡ºè¿‡è¿™ä¸ªepochï¼ˆé¿å…é‡å¤è¾“å‡ºï¼‰
        if self._current_epoch == self._last_output_epoch:
            return

        # è®°å½•æœ¬æ¬¡è¾“å‡ºçš„epoch
        self._last_output_epoch = self._current_epoch

        # é‡ç½®æ›´æ–°æ•°æ®æ ‡è®°
        self._has_update_data = False

        # åˆå¹¶è®­ç»ƒå’Œæ›´æ–°ç»“æœ
        train_result = self._last_train_result.copy()
        test_result = self._last_test_result.copy() if self._last_test_result else None

        # è¾“å‡ºåˆ°ç»ˆç«¯
        if self.verbose:
            self.training_logger.log_epoch(self._current_epoch, train_result, test_result)
        else:
            self.training_logger.log_compact(self._current_epoch, train_result, test_result)

